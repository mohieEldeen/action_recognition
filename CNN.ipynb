{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import *\n",
    "from  models.VGG16Model import CNNModel\n",
    "from models.trainCNN import trainCNN\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning the type of device that we will work on\n",
    "device =  torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device =  torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the names of the frames and thier classes\n",
    "train_frames = pd.read_csv('frames_train_1.csv') \n",
    "test_frames = pd.read_csv('frames_test_1.csv') \n",
    "\n",
    "train_videos = pd.read_csv('video_train_1.csv') \n",
    "test_videos = pd.read_csv('video_test_1_classes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_4.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi_5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name  class\n",
       "0  v_ApplyEyeMakeup_g08_c01.avi_1.jpg      1\n",
       "1  v_ApplyEyeMakeup_g08_c01.avi_2.jpg      1\n",
       "2  v_ApplyEyeMakeup_g08_c01.avi_3.jpg      1\n",
       "3  v_ApplyEyeMakeup_g08_c01.avi_4.jpg      1\n",
       "4  v_ApplyEyeMakeup_g08_c01.avi_5.jpg      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi_1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi_2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi_3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi_4.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi_5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name  class\n",
       "0  v_ApplyEyeMakeup_g01_c01.avi_1.jpg      1\n",
       "1  v_ApplyEyeMakeup_g01_c01.avi_2.jpg      1\n",
       "2  v_ApplyEyeMakeup_g01_c01.avi_3.jpg      1\n",
       "3  v_ApplyEyeMakeup_g01_c01.avi_4.jpg      1\n",
       "4  v_ApplyEyeMakeup_g01_c01.avi_5.jpg      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_frames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>class</th>\n",
       "      <th>num_frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c01.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c02.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c03.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c04.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g08_c05.avi</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  class  num_frames\n",
       "0  v_ApplyEyeMakeup_g08_c01.avi      1          24\n",
       "1  v_ApplyEyeMakeup_g08_c02.avi      1          23\n",
       "2  v_ApplyEyeMakeup_g08_c03.avi      1          29\n",
       "3  v_ApplyEyeMakeup_g08_c04.avi      1          44\n",
       "4  v_ApplyEyeMakeup_g08_c05.avi      1          55"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>num_frames</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c01.avi</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c02.avi</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c03.avi</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c04.avi</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>v_ApplyEyeMakeup_g01_c05.avi</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  num_frames  class\n",
       "0  v_ApplyEyeMakeup_g01_c01.avi          32      1\n",
       "1  v_ApplyEyeMakeup_g01_c02.avi          24      1\n",
       "2  v_ApplyEyeMakeup_g01_c03.avi          51      1\n",
       "3  v_ApplyEyeMakeup_g01_c04.avi          47      1\n",
       "4  v_ApplyEyeMakeup_g01_c05.avi          59      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *let's work on the CNN model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    \"\"\"Resize the image to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size of the image. should be like (new_h , new_w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        \n",
    "        assert len(output_size) == 2 , \"the output size must be of lenght 2 >>> (new_h , new_w)\"\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        \n",
    "        new_h , new_w = self.output_size\n",
    "\n",
    "        image = cv2.resize(src = image, dsize = (new_w,new_h))\n",
    "\n",
    "\n",
    "        return image\n",
    "    \n",
    "    \n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the image pixel values to be between zero and one.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple): Desired output size of the image. should be like (new_h , new_w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "\n",
    "        return image / 255\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Reshaping the ndarrays then Converting them to Tensors to be ready to fed to a FFNN.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        \n",
    "        #Reshaping numpy image: H x W x C to torch Tensor : C x H x W\n",
    "        image = image.transpose((2,0,1))\n",
    "        \n",
    "        return torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation/train ratio\n",
    "validation_ratio = 0.1\n",
    "\n",
    "Image_output_size = (224,224) #output size of the images\n",
    "\n",
    "train_batch_size = 32\n",
    "val_batch_size = 8\n",
    "\n",
    "#splitting the train_videos into train and validation frames\n",
    "#the output from the function below is a data frame that holds the names of the *frames* and thier labels for train and validation\n",
    "tr_frames , val_frames = trainValSpliteFromVideoNames(validation_ratio , train_videos) \n",
    "\n",
    "#the transform steps that we will execute on the images\n",
    "transform  =torchvision.transforms.Compose([\n",
    "    Resize(Image_output_size), #resizing the images\n",
    "    ToTensor(), #transforming the images to Tensor form on the available device (GPU or CPU)\n",
    "    Scale()   #scaling the images pixel values to be in range zero to one\n",
    "    \n",
    "])\n",
    "\n",
    "#tr_frames.sample(n=train_batch_size).reset_index()\n",
    "\n",
    "#intializing the image generator\n",
    "train_gen = UCF101DatasetFrames(frame_name =tr_frames.sample(n=train_batch_size).reset_index(), img_dir ='Train_Frames_1/', shuffle = True , transform = transform)\n",
    "\n",
    "#intializing the validation image generator\n",
    "val_gen = UCF101DatasetFrames(frame_name =val_frames, img_dir ='Train_Frames_1/', shuffle = True , transform = transform)\n",
    "\n",
    "#intializing the train batches loader !!!\n",
    "train_loader = DataLoader(train_gen , batch_size= train_batch_size , shuffle = True)\n",
    "\n",
    "#intializing the val batches loader !!!\n",
    "val_loader = DataLoader(val_gen , batch_size= val_batch_size , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features = 101 #the number of labels\n",
    "lr = 0.0001 #learning rate\n",
    "\n",
    "\n",
    "\n",
    "model = CNNModel(output_features).to(device)   #intializing the model\n",
    "lossFunc = nn.NLLLoss().to(device) #the loss funstion to train on (Negative Likelihood loss since it's a classification task and the logits are log softmax)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr ) #the optimization algorithm we use during training\n",
    "\n",
    "epochs = 2 #num of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0] train_loss: 4.61975 train_acc: 0.00000 val_loss: 4.61853 val_acc: 0.00000\n",
      "[1, 10] train_loss: 4.46195 train_acc: 3.43750 val_loss: 4.59624 val_acc: 5.00000\n",
      "[1, 20] train_loss: 4.26764 train_acc: 9.06250 val_loss: 4.51077 val_acc: 0.00000\n",
      "[1, 30] train_loss: 4.16729 train_acc: 10.31250 val_loss: 4.27486 val_acc: 6.25000\n",
      "[1, 40] train_loss: 4.19870 train_acc: 10.62500 val_loss: 4.01095 val_acc: 11.25000\n",
      "[1, 50] train_loss: 4.00547 train_acc: 15.31250 val_loss: 3.87452 val_acc: 17.50000\n",
      "[1, 60] train_loss: 3.76036 train_acc: 16.87500 val_loss: 3.76541 val_acc: 17.50000\n",
      "[1, 70] train_loss: 3.87263 train_acc: 15.31250 val_loss: 4.05990 val_acc: 13.75000\n",
      "[1, 80] train_loss: 3.67828 train_acc: 19.37500 val_loss: 3.68433 val_acc: 10.00000\n",
      "[1, 90] train_loss: 3.61056 train_acc: 19.06250 val_loss: 3.82396 val_acc: 13.75000\n",
      "[1, 100] train_loss: 3.65373 train_acc: 19.68750 val_loss: 3.39128 val_acc: 16.25000\n",
      "[1, 110] train_loss: 3.68597 train_acc: 17.81250 val_loss: 3.59077 val_acc: 17.50000\n",
      "[1, 120] train_loss: 3.56224 train_acc: 19.68750 val_loss: 3.17722 val_acc: 26.25000\n",
      "[1, 130] train_loss: 3.29640 train_acc: 26.25000 val_loss: 3.54618 val_acc: 18.75000\n",
      "[1, 140] train_loss: 3.23252 train_acc: 26.25000 val_loss: 3.16842 val_acc: 26.25000\n",
      "[1, 150] train_loss: 3.21712 train_acc: 25.31250 val_loss: 3.48001 val_acc: 22.50000\n",
      "[1, 160] train_loss: 3.45484 train_acc: 23.12500 val_loss: 3.26501 val_acc: 22.50000\n",
      "[1, 170] train_loss: 3.08842 train_acc: 27.81250 val_loss: 2.99146 val_acc: 31.25000\n",
      "[1, 180] train_loss: 3.24083 train_acc: 24.37500 val_loss: 3.01494 val_acc: 32.50000\n",
      "[1, 190] train_loss: 3.26232 train_acc: 26.87500 val_loss: 3.08944 val_acc: 23.75000\n",
      "[1, 200] train_loss: 3.09395 train_acc: 28.43750 val_loss: 3.06050 val_acc: 28.75000\n",
      "[1, 210] train_loss: 3.26099 train_acc: 24.37500 val_loss: 3.18864 val_acc: 23.75000\n",
      "[1, 220] train_loss: 3.05347 train_acc: 29.06250 val_loss: 2.84265 val_acc: 36.25000\n",
      "[1, 230] train_loss: 2.89857 train_acc: 31.25000 val_loss: 3.00856 val_acc: 28.75000\n",
      "[1, 240] train_loss: 2.89702 train_acc: 29.68750 val_loss: 2.84795 val_acc: 37.50000\n",
      "[1, 250] train_loss: 2.68191 train_acc: 39.68750 val_loss: 2.71758 val_acc: 32.50000\n",
      "[1, 260] train_loss: 2.77872 train_acc: 37.81250 val_loss: 2.95847 val_acc: 35.00000\n",
      "[1, 270] train_loss: 2.77384 train_acc: 32.81250 val_loss: 2.74338 val_acc: 33.75000\n",
      "[1, 280] train_loss: 2.62999 train_acc: 38.43750 val_loss: 2.54045 val_acc: 31.25000\n",
      "[1, 290] train_loss: 2.47160 train_acc: 45.62500 val_loss: 2.50835 val_acc: 41.25000\n",
      "[1, 300] train_loss: 2.64003 train_acc: 34.68750 val_loss: 2.46543 val_acc: 43.75000\n",
      "[1, 310] train_loss: 2.58572 train_acc: 40.00000 val_loss: 2.51093 val_acc: 33.75000\n",
      "[1, 320] train_loss: 2.27464 train_acc: 45.00000 val_loss: 2.49886 val_acc: 40.00000\n",
      "[1, 330] train_loss: 2.38387 train_acc: 44.06250 val_loss: 2.23213 val_acc: 53.75000\n",
      "[1, 340] train_loss: 2.11911 train_acc: 49.06250 val_loss: 2.52630 val_acc: 36.25000\n",
      "[1, 350] train_loss: 2.49051 train_acc: 41.56250 val_loss: 2.31885 val_acc: 45.00000\n",
      "[1, 360] train_loss: 2.34653 train_acc: 45.93750 val_loss: 2.27918 val_acc: 45.00000\n",
      "[1, 370] train_loss: 2.42176 train_acc: 44.68750 val_loss: 2.71585 val_acc: 37.50000\n",
      "[1, 380] train_loss: 2.40489 train_acc: 44.06250 val_loss: 2.51575 val_acc: 43.75000\n",
      "[1, 390] train_loss: 2.28670 train_acc: 45.62500 val_loss: 2.32954 val_acc: 48.75000\n",
      "[1, 400] train_loss: 2.14485 train_acc: 47.18750 val_loss: 2.19138 val_acc: 41.25000\n",
      "[1, 410] train_loss: 1.99058 train_acc: 52.18750 val_loss: 2.07762 val_acc: 50.00000\n",
      "[1, 420] train_loss: 2.11243 train_acc: 54.68750 val_loss: 2.37509 val_acc: 41.25000\n",
      "[1, 430] train_loss: 2.11823 train_acc: 49.68750 val_loss: 2.18122 val_acc: 51.25000\n",
      "[1, 440] train_loss: 2.10605 train_acc: 50.00000 val_loss: 1.78946 val_acc: 55.00000\n",
      "[1, 450] train_loss: 1.90391 train_acc: 54.68750 val_loss: 2.32697 val_acc: 47.50000\n",
      "[1, 460] train_loss: 2.05616 train_acc: 50.31250 val_loss: 1.97402 val_acc: 48.75000\n",
      "[1, 470] train_loss: 2.00125 train_acc: 51.25000 val_loss: 1.86380 val_acc: 51.25000\n",
      "[1, 480] train_loss: 1.86218 train_acc: 54.68750 val_loss: 2.28260 val_acc: 51.25000\n",
      "[1, 490] train_loss: 2.00819 train_acc: 52.50000 val_loss: 2.22508 val_acc: 45.00000\n",
      "[1, 500] train_loss: 1.92972 train_acc: 50.62500 val_loss: 2.19213 val_acc: 42.50000\n",
      "[1, 510] train_loss: 1.88131 train_acc: 55.62500 val_loss: 1.94981 val_acc: 51.25000\n",
      "[1, 520] train_loss: 1.98869 train_acc: 52.81250 val_loss: 1.78635 val_acc: 58.75000\n",
      "[1, 530] train_loss: 1.81666 train_acc: 57.18750 val_loss: 1.73337 val_acc: 53.75000\n",
      "[1, 540] train_loss: 1.79226 train_acc: 55.62500 val_loss: 1.80852 val_acc: 61.25000\n",
      "[1, 550] train_loss: 1.72642 train_acc: 59.68750 val_loss: 1.84223 val_acc: 51.25000\n",
      "[1, 560] train_loss: 1.81044 train_acc: 54.68750 val_loss: 1.79843 val_acc: 51.25000\n",
      "[1, 570] train_loss: 1.73578 train_acc: 58.43750 val_loss: 1.55641 val_acc: 65.00000\n",
      "[1, 580] train_loss: 1.80777 train_acc: 57.18750 val_loss: 1.69434 val_acc: 56.25000\n",
      "[1, 590] train_loss: 1.73469 train_acc: 58.12500 val_loss: 1.56956 val_acc: 57.50000\n",
      "[1, 600] train_loss: 1.77148 train_acc: 57.50000 val_loss: 1.91589 val_acc: 51.25000\n",
      "[1, 610] train_loss: 1.77329 train_acc: 58.43750 val_loss: 1.73277 val_acc: 60.00000\n",
      "[1, 620] train_loss: 1.83087 train_acc: 53.43750 val_loss: 1.64707 val_acc: 62.50000\n",
      "[1, 630] train_loss: 1.57886 train_acc: 60.00000 val_loss: 2.12850 val_acc: 51.25000\n",
      "[1, 640] train_loss: 1.47420 train_acc: 63.43750 val_loss: 1.61051 val_acc: 63.75000\n",
      "[1, 650] train_loss: 1.54821 train_acc: 61.56250 val_loss: 1.61704 val_acc: 61.25000\n",
      "[1, 660] train_loss: 1.76532 train_acc: 56.56250 val_loss: 1.67923 val_acc: 56.25000\n",
      "[1, 670] train_loss: 1.53360 train_acc: 60.00000 val_loss: 1.69954 val_acc: 62.50000\n",
      "[1, 680] train_loss: 1.64821 train_acc: 61.56250 val_loss: 1.61404 val_acc: 61.25000\n",
      "[1, 690] train_loss: 1.61780 train_acc: 61.25000 val_loss: 1.61712 val_acc: 56.25000\n",
      "[1, 700] train_loss: 1.59248 train_acc: 60.31250 val_loss: 1.60311 val_acc: 62.50000\n",
      "[1, 710] train_loss: 1.43121 train_acc: 65.62500 val_loss: 1.53801 val_acc: 67.50000\n",
      "[1, 720] train_loss: 1.39322 train_acc: 68.43750 val_loss: 1.21243 val_acc: 70.00000\n",
      "[1, 730] train_loss: 1.29597 train_acc: 68.12500 val_loss: 1.55055 val_acc: 61.25000\n",
      "[1, 740] train_loss: 1.57420 train_acc: 61.87500 val_loss: 1.28158 val_acc: 66.25000\n",
      "[1, 750] train_loss: 1.43808 train_acc: 65.31250 val_loss: 1.55615 val_acc: 62.50000\n",
      "[1, 760] train_loss: 1.25206 train_acc: 70.00000 val_loss: 1.13927 val_acc: 73.75000\n",
      "[1, 770] train_loss: 1.45136 train_acc: 63.75000 val_loss: 1.35046 val_acc: 66.25000\n",
      "[1, 780] train_loss: 1.34419 train_acc: 68.12500 val_loss: 1.20004 val_acc: 67.50000\n",
      "[1, 790] train_loss: 1.30698 train_acc: 68.12500 val_loss: 1.48612 val_acc: 65.00000\n",
      "[1, 800] train_loss: 1.44949 train_acc: 67.81250 val_loss: 1.19494 val_acc: 73.75000\n",
      "[1, 810] train_loss: 1.30562 train_acc: 68.43750 val_loss: 1.10687 val_acc: 70.00000\n",
      "[1, 820] train_loss: 1.38620 train_acc: 63.12500 val_loss: 1.61119 val_acc: 62.50000\n",
      "[1, 830] train_loss: 1.41918 train_acc: 65.00000 val_loss: 1.12038 val_acc: 70.00000\n",
      "[1, 840] train_loss: 1.18343 train_acc: 68.75000 val_loss: 1.67918 val_acc: 56.25000\n",
      "[1, 850] train_loss: 1.37782 train_acc: 60.62500 val_loss: 0.98091 val_acc: 81.25000\n",
      "[1, 860] train_loss: 1.29504 train_acc: 68.12500 val_loss: 1.24879 val_acc: 68.75000\n",
      "[1, 870] train_loss: 1.33450 train_acc: 66.25000 val_loss: 1.82901 val_acc: 56.25000\n",
      "[1, 880] train_loss: 1.37147 train_acc: 66.87500 val_loss: 1.27965 val_acc: 70.00000\n",
      "[1, 890] train_loss: 1.21022 train_acc: 70.00000 val_loss: 1.52540 val_acc: 65.00000\n",
      "[1, 900] train_loss: 1.19070 train_acc: 70.93750 val_loss: 1.21481 val_acc: 67.50000\n",
      "[1, 910] train_loss: 1.21217 train_acc: 69.06250 val_loss: 1.46364 val_acc: 66.25000\n",
      "[1, 920] train_loss: 1.24738 train_acc: 69.06250 val_loss: 1.11752 val_acc: 76.25000\n",
      "[1, 930] train_loss: 1.18061 train_acc: 71.25000 val_loss: 1.43707 val_acc: 66.25000\n",
      "[1, 940] train_loss: 1.15297 train_acc: 70.31250 val_loss: 1.55743 val_acc: 63.75000\n",
      "[1, 950] train_loss: 1.49384 train_acc: 66.25000 val_loss: 1.32052 val_acc: 68.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 960] train_loss: 1.03510 train_acc: 74.68750 val_loss: 1.27989 val_acc: 68.75000\n",
      "[1, 970] train_loss: 1.16717 train_acc: 71.56250 val_loss: 1.07568 val_acc: 71.25000\n",
      "[1, 980] train_loss: 1.23302 train_acc: 68.43750 val_loss: 1.62163 val_acc: 62.50000\n",
      "[1, 990] train_loss: 1.09180 train_acc: 75.31250 val_loss: 1.30690 val_acc: 67.50000\n",
      "[1, 1000] train_loss: 0.92719 train_acc: 77.18750 val_loss: 1.07268 val_acc: 71.25000\n",
      "[1, 1010] train_loss: 1.09874 train_acc: 74.06250 val_loss: 1.06413 val_acc: 73.75000\n",
      "[1, 1020] train_loss: 0.98521 train_acc: 74.37500 val_loss: 0.88925 val_acc: 75.00000\n",
      "[1, 1030] train_loss: 1.09500 train_acc: 71.25000 val_loss: 1.27246 val_acc: 63.75000\n",
      "[1, 1040] train_loss: 1.07921 train_acc: 73.12500 val_loss: 1.55815 val_acc: 63.75000\n",
      "[1, 1050] train_loss: 1.20562 train_acc: 69.68750 val_loss: 1.09064 val_acc: 75.00000\n",
      "[1, 1060] train_loss: 1.21588 train_acc: 68.43750 val_loss: 1.11375 val_acc: 76.25000\n",
      "[1, 1070] train_loss: 1.01964 train_acc: 76.56250 val_loss: 1.31565 val_acc: 71.25000\n",
      "[1, 1080] train_loss: 0.97122 train_acc: 76.56250 val_loss: 1.38478 val_acc: 63.75000\n",
      "[1, 1090] train_loss: 0.92251 train_acc: 77.18750 val_loss: 1.06485 val_acc: 72.50000\n",
      "[1, 1100] train_loss: 1.08770 train_acc: 71.56250 val_loss: 0.98109 val_acc: 72.50000\n",
      "[1, 1110] train_loss: 1.06187 train_acc: 73.43750 val_loss: 0.87864 val_acc: 81.25000\n",
      "[1, 1120] train_loss: 1.08970 train_acc: 72.18750 val_loss: 1.18154 val_acc: 75.00000\n",
      "[1, 1130] train_loss: 0.90354 train_acc: 74.68750 val_loss: 1.39133 val_acc: 70.00000\n",
      "[1, 1140] train_loss: 1.03300 train_acc: 72.18750 val_loss: 1.17431 val_acc: 70.00000\n",
      "[1, 1150] train_loss: 0.94704 train_acc: 76.25000 val_loss: 0.85793 val_acc: 78.75000\n",
      "[1, 1160] train_loss: 0.96384 train_acc: 72.81250 val_loss: 1.22708 val_acc: 72.50000\n",
      "[1, 1170] train_loss: 0.94215 train_acc: 79.06250 val_loss: 0.96435 val_acc: 72.50000\n",
      "[1, 1180] train_loss: 0.94914 train_acc: 77.18750 val_loss: 0.72839 val_acc: 82.50000\n",
      "[1, 1190] train_loss: 0.97229 train_acc: 75.62500 val_loss: 0.93366 val_acc: 73.75000\n",
      "[1, 1200] train_loss: 1.03024 train_acc: 74.06250 val_loss: 0.97315 val_acc: 73.75000\n",
      "[1, 1210] train_loss: 0.81519 train_acc: 81.25000 val_loss: 0.85786 val_acc: 81.25000\n",
      "[1, 1220] train_loss: 0.88029 train_acc: 78.12500 val_loss: 0.95264 val_acc: 78.75000\n",
      "[1, 1230] train_loss: 0.91141 train_acc: 76.87500 val_loss: 1.16632 val_acc: 75.00000\n",
      "[1, 1240] train_loss: 0.96757 train_acc: 77.81250 val_loss: 0.87428 val_acc: 80.00000\n",
      "[1, 1250] train_loss: 0.89634 train_acc: 75.62500 val_loss: 0.91273 val_acc: 75.00000\n",
      "[1, 1260] train_loss: 0.87670 train_acc: 78.43750 val_loss: 1.21447 val_acc: 70.00000\n",
      "[1, 1270] train_loss: 0.88931 train_acc: 77.81250 val_loss: 1.16846 val_acc: 67.50000\n",
      "[1, 1280] train_loss: 0.97080 train_acc: 74.68750 val_loss: 1.20313 val_acc: 67.50000\n",
      "[1, 1290] train_loss: 1.05510 train_acc: 74.37500 val_loss: 1.01673 val_acc: 70.00000\n",
      "[1, 1300] train_loss: 0.84883 train_acc: 79.06250 val_loss: 1.03123 val_acc: 72.50000\n",
      "[1, 1310] train_loss: 0.88620 train_acc: 77.50000 val_loss: 0.88430 val_acc: 81.25000\n",
      "[1, 1320] train_loss: 0.87964 train_acc: 77.50000 val_loss: 0.80238 val_acc: 80.00000\n",
      "[1, 1330] train_loss: 0.85605 train_acc: 78.12500 val_loss: 1.12972 val_acc: 70.00000\n",
      "[1, 1340] train_loss: 0.94767 train_acc: 74.68750 val_loss: 0.84597 val_acc: 73.75000\n",
      "[1, 1350] train_loss: 0.82716 train_acc: 79.06250 val_loss: 0.76338 val_acc: 82.50000\n",
      "[1, 1360] train_loss: 0.89068 train_acc: 77.50000 val_loss: 0.80724 val_acc: 83.75000\n",
      "[1, 1370] train_loss: 0.90880 train_acc: 77.81250 val_loss: 1.03320 val_acc: 71.25000\n",
      "[1, 1380] train_loss: 0.80562 train_acc: 79.68750 val_loss: 1.03222 val_acc: 76.25000\n",
      "[1, 1390] train_loss: 0.84818 train_acc: 77.18750 val_loss: 0.95705 val_acc: 71.25000\n",
      "[1, 1400] train_loss: 0.78387 train_acc: 79.68750 val_loss: 0.60016 val_acc: 83.75000\n",
      "[1, 1410] train_loss: 0.74398 train_acc: 81.87500 val_loss: 1.27082 val_acc: 71.25000\n",
      "[1, 1420] train_loss: 0.74007 train_acc: 80.62500 val_loss: 0.70798 val_acc: 83.75000\n",
      "[1, 1430] train_loss: 0.67171 train_acc: 83.12500 val_loss: 0.91315 val_acc: 71.25000\n",
      "[1, 1440] train_loss: 0.83809 train_acc: 75.93750 val_loss: 0.95499 val_acc: 77.50000\n",
      "[1, 1450] train_loss: 0.76869 train_acc: 79.68750 val_loss: 0.92585 val_acc: 76.25000\n",
      "[1, 1460] train_loss: 0.78939 train_acc: 79.37500 val_loss: 1.05502 val_acc: 78.75000\n",
      "[1, 1470] train_loss: 0.72290 train_acc: 81.25000 val_loss: 1.07879 val_acc: 71.25000\n",
      "[1, 1480] train_loss: 0.84559 train_acc: 77.18750 val_loss: 0.80921 val_acc: 75.00000\n",
      "[1, 1490] train_loss: 0.78556 train_acc: 80.62500 val_loss: 1.16558 val_acc: 75.00000\n",
      "[1, 1500] train_loss: 0.77469 train_acc: 78.75000 val_loss: 1.37660 val_acc: 70.00000\n",
      "[1, 1510] train_loss: 0.73583 train_acc: 81.25000 val_loss: 0.89074 val_acc: 81.25000\n",
      "[1, 1520] train_loss: 0.78428 train_acc: 81.87500 val_loss: 0.57063 val_acc: 85.00000\n",
      "[1, 1530] train_loss: 0.73176 train_acc: 79.68750 val_loss: 0.71747 val_acc: 85.00000\n",
      "[1, 1540] train_loss: 0.79576 train_acc: 79.06250 val_loss: 0.94781 val_acc: 75.00000\n",
      "[1, 1550] train_loss: 0.71472 train_acc: 79.37500 val_loss: 0.86651 val_acc: 81.25000\n",
      "[1, 1560] train_loss: 0.72422 train_acc: 81.25000 val_loss: 0.71799 val_acc: 85.00000\n",
      "[1, 1570] train_loss: 0.63411 train_acc: 82.50000 val_loss: 1.12299 val_acc: 77.50000\n",
      "[1, 1580] train_loss: 0.77267 train_acc: 81.87500 val_loss: 1.05877 val_acc: 71.25000\n",
      "[1, 1590] train_loss: 0.56934 train_acc: 87.18750 val_loss: 0.81012 val_acc: 81.25000\n",
      "[1, 1600] train_loss: 0.73297 train_acc: 81.87500 val_loss: 0.85763 val_acc: 77.50000\n",
      "[1, 1610] train_loss: 0.73559 train_acc: 83.43750 val_loss: 0.66743 val_acc: 85.00000\n",
      "[1, 1620] train_loss: 0.54538 train_acc: 86.56250 val_loss: 0.77911 val_acc: 81.25000\n",
      "[1, 1630] train_loss: 0.60347 train_acc: 84.06250 val_loss: 0.99245 val_acc: 76.25000\n",
      "[1, 1640] train_loss: 0.74655 train_acc: 83.75000 val_loss: 0.85795 val_acc: 81.25000\n",
      "[1, 1650] train_loss: 0.69617 train_acc: 85.00000 val_loss: 0.77526 val_acc: 82.50000\n",
      "[1, 1660] train_loss: 0.68259 train_acc: 80.93750 val_loss: 1.08163 val_acc: 73.75000\n",
      "[1, 1670] train_loss: 0.74462 train_acc: 80.62500 val_loss: 1.14491 val_acc: 76.25000\n",
      "[1, 1680] train_loss: 0.57354 train_acc: 85.00000 val_loss: 0.85929 val_acc: 76.25000\n",
      "[1, 1690] train_loss: 0.57156 train_acc: 84.06250 val_loss: 0.85255 val_acc: 76.25000\n",
      "[1, 1700] train_loss: 0.68423 train_acc: 81.87500 val_loss: 0.79680 val_acc: 78.75000\n",
      "[1, 1710] train_loss: 0.68410 train_acc: 82.81250 val_loss: 1.06542 val_acc: 73.75000\n",
      "[1, 1720] train_loss: 0.80795 train_acc: 77.81250 val_loss: 0.69938 val_acc: 82.50000\n",
      "[1, 1730] train_loss: 0.64983 train_acc: 83.12500 val_loss: 0.86054 val_acc: 82.50000\n",
      "[1, 1740] train_loss: 0.54696 train_acc: 85.62500 val_loss: 0.94161 val_acc: 78.75000\n",
      "[1, 1750] train_loss: 0.76238 train_acc: 79.37500 val_loss: 0.80439 val_acc: 80.00000\n",
      "[1, 1760] train_loss: 0.66821 train_acc: 82.81250 val_loss: 0.96992 val_acc: 76.25000\n",
      "[1, 1770] train_loss: 0.64311 train_acc: 82.18750 val_loss: 0.63522 val_acc: 85.00000\n",
      "[1, 1780] train_loss: 0.57173 train_acc: 85.00000 val_loss: 1.04351 val_acc: 72.50000\n",
      "[1, 1790] train_loss: 0.57397 train_acc: 85.93750 val_loss: 1.01187 val_acc: 78.75000\n",
      "[1, 1800] train_loss: 0.68197 train_acc: 82.18750 val_loss: 0.91604 val_acc: 77.50000\n",
      "[1, 1810] train_loss: 0.75213 train_acc: 83.43750 val_loss: 0.91221 val_acc: 82.50000\n",
      "[1, 1820] train_loss: 0.55994 train_acc: 87.50000 val_loss: 1.15643 val_acc: 76.25000\n",
      "[1, 1830] train_loss: 0.52103 train_acc: 86.56250 val_loss: 0.60977 val_acc: 86.25000\n",
      "[1, 1840] train_loss: 0.59863 train_acc: 83.43750 val_loss: 0.92943 val_acc: 80.00000\n",
      "[1, 1850] train_loss: 0.45474 train_acc: 87.18750 val_loss: 0.66216 val_acc: 83.75000\n",
      "[1, 1860] train_loss: 0.56175 train_acc: 86.87500 val_loss: 0.86110 val_acc: 81.25000\n",
      "[1, 1870] train_loss: 0.60776 train_acc: 86.25000 val_loss: 0.80579 val_acc: 80.00000\n",
      "[1, 1880] train_loss: 0.54148 train_acc: 85.93750 val_loss: 0.69481 val_acc: 82.50000\n",
      "[1, 1890] train_loss: 0.56235 train_acc: 86.56250 val_loss: 0.36326 val_acc: 87.50000\n",
      "[1, 1900] train_loss: 0.55948 train_acc: 86.25000 val_loss: 0.98297 val_acc: 78.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1910] train_loss: 0.66915 train_acc: 83.12500 val_loss: 1.03178 val_acc: 78.75000\n",
      "[1, 1920] train_loss: 0.49276 train_acc: 85.93750 val_loss: 0.85943 val_acc: 78.75000\n",
      "[1, 1930] train_loss: 0.42823 train_acc: 89.68750 val_loss: 1.03843 val_acc: 77.50000\n",
      "[1, 1940] train_loss: 0.52246 train_acc: 85.31250 val_loss: 0.90317 val_acc: 77.50000\n",
      "[1, 1950] train_loss: 0.56214 train_acc: 87.18750 val_loss: 0.96489 val_acc: 76.25000\n",
      "[1, 1960] train_loss: 0.69935 train_acc: 81.56250 val_loss: 0.86402 val_acc: 77.50000\n",
      "[1, 1970] train_loss: 0.50422 train_acc: 87.81250 val_loss: 0.92163 val_acc: 76.25000\n",
      "[1, 1980] train_loss: 0.64416 train_acc: 83.75000 val_loss: 0.60458 val_acc: 81.25000\n",
      "[1, 1990] train_loss: 0.58195 train_acc: 85.31250 val_loss: 0.61025 val_acc: 83.75000\n",
      "[1, 2000] train_loss: 0.60712 train_acc: 83.75000 val_loss: 0.86620 val_acc: 81.25000\n",
      "[1, 2010] train_loss: 0.52470 train_acc: 87.50000 val_loss: 1.21596 val_acc: 72.50000\n",
      "[1, 2020] train_loss: 0.50559 train_acc: 85.93750 val_loss: 0.65023 val_acc: 80.00000\n",
      "[1, 2030] train_loss: 0.49969 train_acc: 86.87500 val_loss: 0.64345 val_acc: 87.50000\n",
      "[1, 2040] train_loss: 0.62676 train_acc: 80.62500 val_loss: 0.65324 val_acc: 80.00000\n",
      "[1, 2050] train_loss: 0.61447 train_acc: 83.12500 val_loss: 0.91944 val_acc: 77.50000\n",
      "[1, 2060] train_loss: 0.61427 train_acc: 82.50000 val_loss: 0.66679 val_acc: 83.75000\n",
      "[1, 2070] train_loss: 0.72078 train_acc: 80.62500 val_loss: 0.75083 val_acc: 81.25000\n",
      "[1, 2080] train_loss: 0.58670 train_acc: 84.68750 val_loss: 0.51446 val_acc: 86.25000\n",
      "[1, 2090] train_loss: 0.55404 train_acc: 83.43750 val_loss: 0.71094 val_acc: 82.50000\n",
      "[1, 2100] train_loss: 0.63833 train_acc: 84.06250 val_loss: 0.82767 val_acc: 81.25000\n",
      "[1, 2110] train_loss: 0.53411 train_acc: 86.25000 val_loss: 0.64401 val_acc: 83.75000\n",
      "[1, 2120] train_loss: 0.61858 train_acc: 84.37500 val_loss: 0.82551 val_acc: 77.50000\n",
      "[1, 2130] train_loss: 0.48330 train_acc: 86.25000 val_loss: 0.96133 val_acc: 77.50000\n",
      "[1, 2140] train_loss: 0.45987 train_acc: 87.50000 val_loss: 0.49875 val_acc: 90.00000\n",
      "[1, 2150] train_loss: 0.52262 train_acc: 86.56250 val_loss: 0.61663 val_acc: 82.50000\n",
      "[1, 2160] train_loss: 0.53829 train_acc: 85.00000 val_loss: 0.96664 val_acc: 81.25000\n",
      "[1, 2170] train_loss: 0.62831 train_acc: 83.75000 val_loss: 0.74435 val_acc: 81.25000\n",
      "[1, 2180] train_loss: 0.53467 train_acc: 86.87500 val_loss: 0.41783 val_acc: 87.50000\n",
      "[1, 2190] train_loss: 0.54148 train_acc: 84.37500 val_loss: 0.83924 val_acc: 80.00000\n",
      "[1, 2200] train_loss: 0.56052 train_acc: 85.31250 val_loss: 0.77213 val_acc: 83.75000\n",
      "[1, 2210] train_loss: 0.58662 train_acc: 84.06250 val_loss: 0.63441 val_acc: 82.50000\n",
      "[1, 2220] train_loss: 0.55233 train_acc: 84.06250 val_loss: 0.78333 val_acc: 80.00000\n",
      "[1, 2230] train_loss: 0.59446 train_acc: 85.62500 val_loss: 0.70259 val_acc: 85.00000\n",
      "[1, 2240] train_loss: 0.55134 train_acc: 84.06250 val_loss: 0.76144 val_acc: 77.50000\n",
      "[1, 2250] train_loss: 0.47027 train_acc: 86.87500 val_loss: 0.50938 val_acc: 90.00000\n",
      "[1, 2260] train_loss: 0.40565 train_acc: 89.06250 val_loss: 0.69998 val_acc: 81.25000\n",
      "[1, 2270] train_loss: 0.56954 train_acc: 86.25000 val_loss: 0.52238 val_acc: 85.00000\n",
      "[1, 2280] train_loss: 0.48593 train_acc: 87.81250 val_loss: 0.72486 val_acc: 83.75000\n",
      "[1, 2290] train_loss: 0.41567 train_acc: 90.31250 val_loss: 0.70996 val_acc: 82.50000\n",
      "[1, 2300] train_loss: 0.53770 train_acc: 86.25000 val_loss: 0.71266 val_acc: 85.00000\n",
      "[1, 2310] train_loss: 0.53384 train_acc: 86.25000 val_loss: 0.85991 val_acc: 81.25000\n",
      "[1, 2320] train_loss: 0.59006 train_acc: 85.00000 val_loss: 0.77612 val_acc: 81.25000\n",
      "[1, 2330] train_loss: 0.41061 train_acc: 89.06250 val_loss: 1.09261 val_acc: 77.50000\n",
      "[1, 2340] train_loss: 0.40286 train_acc: 88.12500 val_loss: 0.48351 val_acc: 88.75000\n",
      "[1, 2350] train_loss: 0.51150 train_acc: 88.12500 val_loss: 0.60736 val_acc: 83.75000\n",
      "[1, 2360] train_loss: 0.46268 train_acc: 87.50000 val_loss: 0.58687 val_acc: 88.75000\n",
      "[1, 2370] train_loss: 0.33146 train_acc: 92.18750 val_loss: 0.47202 val_acc: 87.50000\n",
      "[1, 2380] train_loss: 0.42224 train_acc: 88.43750 val_loss: 0.87612 val_acc: 78.75000\n",
      "[1, 2390] train_loss: 0.47027 train_acc: 85.62500 val_loss: 0.35076 val_acc: 92.50000\n",
      "[1, 2400] train_loss: 0.52827 train_acc: 87.81250 val_loss: 0.49975 val_acc: 91.25000\n",
      "[1, 2410] train_loss: 0.34061 train_acc: 90.93750 val_loss: 0.46733 val_acc: 88.75000\n",
      "[1, 2420] train_loss: 0.53493 train_acc: 84.68750 val_loss: 0.66931 val_acc: 86.25000\n",
      "[1, 2430] train_loss: 0.49470 train_acc: 88.75000 val_loss: 0.62555 val_acc: 83.75000\n",
      "[1, 2440] train_loss: 0.49118 train_acc: 86.87500 val_loss: 1.20675 val_acc: 71.25000\n",
      "[1, 2450] train_loss: 0.52192 train_acc: 85.31250 val_loss: 0.59964 val_acc: 86.25000\n",
      "[1, 2460] train_loss: 0.52167 train_acc: 87.81250 val_loss: 0.57542 val_acc: 82.50000\n",
      "[1, 2470] train_loss: 0.53577 train_acc: 87.18750 val_loss: 0.71241 val_acc: 82.50000\n",
      "[1, 2480] train_loss: 0.51272 train_acc: 86.25000 val_loss: 1.17214 val_acc: 76.25000\n",
      "[1, 2490] train_loss: 0.50202 train_acc: 84.68750 val_loss: 0.75814 val_acc: 78.75000\n",
      "[1, 2500] train_loss: 0.47838 train_acc: 86.25000 val_loss: 0.55474 val_acc: 83.75000\n",
      "[1, 2510] train_loss: 0.52235 train_acc: 84.37500 val_loss: 0.48991 val_acc: 86.25000\n",
      "[1, 2520] train_loss: 0.44829 train_acc: 90.00000 val_loss: 0.70226 val_acc: 82.50000\n",
      "[1, 2530] train_loss: 0.44288 train_acc: 87.81250 val_loss: 0.59906 val_acc: 82.50000\n",
      "[1, 2540] train_loss: 0.45801 train_acc: 87.18750 val_loss: 0.75112 val_acc: 85.00000\n",
      "[1, 2550] train_loss: 0.45504 train_acc: 87.50000 val_loss: 0.56855 val_acc: 82.50000\n",
      "[1, 2560] train_loss: 0.45311 train_acc: 86.25000 val_loss: 0.61467 val_acc: 82.50000\n",
      "[1, 2570] train_loss: 0.54114 train_acc: 84.37500 val_loss: 0.60922 val_acc: 90.00000\n",
      "[1, 2580] train_loss: 0.46823 train_acc: 87.50000 val_loss: 0.63985 val_acc: 82.50000\n",
      "[1, 2590] train_loss: 0.52762 train_acc: 87.50000 val_loss: 1.17381 val_acc: 75.00000\n",
      "[1, 2600] train_loss: 0.32499 train_acc: 90.62500 val_loss: 0.65243 val_acc: 87.50000\n",
      "[1, 2610] train_loss: 0.48147 train_acc: 85.93750 val_loss: 0.55650 val_acc: 87.50000\n",
      "[1, 2620] train_loss: 0.39342 train_acc: 89.68750 val_loss: 0.65679 val_acc: 83.75000\n",
      "[1, 2630] train_loss: 0.48218 train_acc: 87.81250 val_loss: 0.81010 val_acc: 80.00000\n",
      "[1, 2640] train_loss: 0.47525 train_acc: 88.75000 val_loss: 0.72242 val_acc: 81.25000\n",
      "[1, 2650] train_loss: 0.51712 train_acc: 87.18750 val_loss: 0.65161 val_acc: 87.50000\n",
      "[1, 2660] train_loss: 0.45630 train_acc: 89.68750 val_loss: 0.63622 val_acc: 82.50000\n",
      "[1, 2670] train_loss: 0.41947 train_acc: 88.75000 val_loss: 0.91802 val_acc: 77.50000\n",
      "[1, 2680] train_loss: 0.44306 train_acc: 87.81250 val_loss: 1.02482 val_acc: 82.50000\n",
      "[1, 2690] train_loss: 0.53670 train_acc: 86.25000 val_loss: 0.59769 val_acc: 86.25000\n",
      "[1, 2700] train_loss: 0.46205 train_acc: 88.43750 val_loss: 0.73492 val_acc: 80.00000\n",
      "[1, 2710] train_loss: 0.34285 train_acc: 89.06250 val_loss: 1.06056 val_acc: 80.00000\n",
      "[1, 2720] train_loss: 0.42290 train_acc: 87.18750 val_loss: 0.38193 val_acc: 88.75000\n",
      "[1, 2730] train_loss: 0.49806 train_acc: 89.37500 val_loss: 0.41951 val_acc: 88.75000\n",
      "[1, 2740] train_loss: 0.31345 train_acc: 91.25000 val_loss: 0.56768 val_acc: 88.75000\n",
      "[1, 2750] train_loss: 0.59103 train_acc: 83.43750 val_loss: 0.69584 val_acc: 81.25000\n",
      "[1, 2760] train_loss: 0.38322 train_acc: 88.75000 val_loss: 0.55558 val_acc: 86.25000\n",
      "[1, 2770] train_loss: 0.46093 train_acc: 87.81250 val_loss: 0.58911 val_acc: 86.25000\n",
      "[1, 2780] train_loss: 0.41597 train_acc: 90.00000 val_loss: 0.68798 val_acc: 81.25000\n",
      "[1, 2790] train_loss: 0.44013 train_acc: 88.43750 val_loss: 0.33740 val_acc: 90.00000\n",
      "[1, 2800] train_loss: 0.35354 train_acc: 90.31250 val_loss: 0.38229 val_acc: 90.00000\n",
      "[1, 2810] train_loss: 0.37936 train_acc: 88.75000 val_loss: 0.57659 val_acc: 81.25000\n",
      "[1, 2820] train_loss: 0.28024 train_acc: 90.62500 val_loss: 1.05857 val_acc: 76.25000\n",
      "[1, 2830] train_loss: 0.37342 train_acc: 90.00000 val_loss: 0.65481 val_acc: 81.25000\n",
      "[1, 2840] train_loss: 0.45204 train_acc: 86.87500 val_loss: 0.88735 val_acc: 81.25000\n",
      "[1, 2850] train_loss: 0.44186 train_acc: 88.43750 val_loss: 0.80761 val_acc: 83.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2860] train_loss: 0.27149 train_acc: 93.12500 val_loss: 0.62014 val_acc: 85.00000\n",
      "[1, 2870] train_loss: 0.34726 train_acc: 92.18750 val_loss: 0.98368 val_acc: 75.00000\n",
      "[1, 2880] train_loss: 0.55189 train_acc: 85.93750 val_loss: 0.76218 val_acc: 85.00000\n",
      "[1, 2890] train_loss: 0.44146 train_acc: 89.37500 val_loss: 0.84242 val_acc: 80.00000\n",
      "[1, 2900] train_loss: 0.47075 train_acc: 87.18750 val_loss: 0.94704 val_acc: 75.00000\n",
      "[1, 2910] train_loss: 0.49661 train_acc: 86.87500 val_loss: 0.72549 val_acc: 83.75000\n",
      "[1, 2920] train_loss: 0.38277 train_acc: 90.00000 val_loss: 1.05237 val_acc: 77.50000\n",
      "[1, 2930] train_loss: 0.37256 train_acc: 90.00000 val_loss: 0.53982 val_acc: 85.00000\n",
      "[1, 2940] train_loss: 0.30784 train_acc: 91.87500 val_loss: 0.59046 val_acc: 86.25000\n",
      "[1, 2950] train_loss: 0.36073 train_acc: 89.06250 val_loss: 0.41031 val_acc: 91.25000\n",
      "[1, 2960] train_loss: 0.31015 train_acc: 91.25000 val_loss: 0.78765 val_acc: 80.00000\n",
      "[1, 2970] train_loss: 0.33146 train_acc: 89.06250 val_loss: 0.46133 val_acc: 88.75000\n",
      "[1, 2980] train_loss: 0.32451 train_acc: 90.00000 val_loss: 0.85118 val_acc: 80.00000\n",
      "[1, 2990] train_loss: 0.27329 train_acc: 92.18750 val_loss: 0.61377 val_acc: 81.25000\n",
      "[1, 3000] train_loss: 0.33603 train_acc: 90.93750 val_loss: 0.26468 val_acc: 93.75000\n",
      "[1, 3010] train_loss: 0.47080 train_acc: 86.25000 val_loss: 0.53059 val_acc: 82.50000\n",
      "[1, 3020] train_loss: 0.47692 train_acc: 87.50000 val_loss: 0.63193 val_acc: 83.75000\n",
      "[1, 3030] train_loss: 0.38816 train_acc: 89.06250 val_loss: 0.66469 val_acc: 88.75000\n",
      "[1, 3040] train_loss: 0.32746 train_acc: 90.93750 val_loss: 0.50739 val_acc: 83.75000\n",
      "[1, 3050] train_loss: 0.35199 train_acc: 91.56250 val_loss: 1.08061 val_acc: 77.50000\n",
      "[1, 3060] train_loss: 0.28593 train_acc: 93.12500 val_loss: 0.89403 val_acc: 83.75000\n",
      "[1, 3070] train_loss: 0.29872 train_acc: 91.87500 val_loss: 0.60024 val_acc: 85.00000\n",
      "[1, 3080] train_loss: 0.39620 train_acc: 90.93750 val_loss: 0.77679 val_acc: 81.25000\n",
      "[1, 3090] train_loss: 0.31777 train_acc: 89.37500 val_loss: 0.87571 val_acc: 81.25000\n",
      "[1, 3100] train_loss: 0.43373 train_acc: 88.75000 val_loss: 0.53982 val_acc: 88.75000\n",
      "[1, 3110] train_loss: 0.37834 train_acc: 86.25000 val_loss: 0.55630 val_acc: 85.00000\n",
      "[1, 3120] train_loss: 0.35268 train_acc: 91.87500 val_loss: 0.49362 val_acc: 86.25000\n",
      "[1, 3130] train_loss: 0.41322 train_acc: 88.75000 val_loss: 0.83838 val_acc: 78.75000\n",
      "[1, 3140] train_loss: 0.37718 train_acc: 90.31250 val_loss: 0.87300 val_acc: 81.25000\n",
      "[1, 3150] train_loss: 0.26181 train_acc: 92.81250 val_loss: 0.47875 val_acc: 88.75000\n",
      "[1, 3160] train_loss: 0.27137 train_acc: 91.56250 val_loss: 0.46369 val_acc: 88.75000\n",
      "[1, 3170] train_loss: 0.41183 train_acc: 87.81250 val_loss: 0.55029 val_acc: 85.00000\n",
      "[1, 3180] train_loss: 0.36624 train_acc: 90.31250 val_loss: 0.40703 val_acc: 87.50000\n",
      "[1, 3190] train_loss: 0.37085 train_acc: 90.31250 val_loss: 0.87872 val_acc: 86.25000\n",
      "[1, 3200] train_loss: 0.31616 train_acc: 90.62500 val_loss: 0.74547 val_acc: 80.00000\n",
      "[1, 3210] train_loss: 0.33332 train_acc: 91.56250 val_loss: 0.65300 val_acc: 78.75000\n",
      "[1, 3220] train_loss: 0.40663 train_acc: 87.50000 val_loss: 0.46020 val_acc: 87.50000\n",
      "[1, 3230] train_loss: 0.36692 train_acc: 89.68750 val_loss: 0.34618 val_acc: 88.75000\n",
      "[1, 3240] train_loss: 0.33351 train_acc: 89.68750 val_loss: 0.63477 val_acc: 88.75000\n",
      "[1, 3250] train_loss: 0.38057 train_acc: 90.62500 val_loss: 0.58932 val_acc: 87.50000\n",
      "[1, 3260] train_loss: 0.28362 train_acc: 91.56250 val_loss: 0.38389 val_acc: 91.25000\n",
      "[1, 3270] train_loss: 0.23836 train_acc: 94.68750 val_loss: 0.70237 val_acc: 87.50000\n",
      "[1, 3280] train_loss: 0.34212 train_acc: 89.68750 val_loss: 0.45010 val_acc: 90.00000\n",
      "[1, 3290] train_loss: 0.32665 train_acc: 91.87500 val_loss: 0.51455 val_acc: 87.50000\n",
      "[1, 3300] train_loss: 0.33624 train_acc: 90.00000 val_loss: 0.87496 val_acc: 82.50000\n",
      "[1, 3310] train_loss: 0.35555 train_acc: 90.62500 val_loss: 0.51515 val_acc: 86.25000\n",
      "[1, 3320] train_loss: 0.36684 train_acc: 90.93750 val_loss: 0.65204 val_acc: 87.50000\n",
      "[1, 3330] train_loss: 0.32727 train_acc: 90.31250 val_loss: 0.44053 val_acc: 86.25000\n",
      "[1, 3340] train_loss: 0.40931 train_acc: 86.87500 val_loss: 0.72619 val_acc: 77.50000\n",
      "[1, 3350] train_loss: 0.39880 train_acc: 89.37500 val_loss: 0.60167 val_acc: 85.00000\n",
      "[1, 3360] train_loss: 0.29593 train_acc: 91.56250 val_loss: 1.23788 val_acc: 76.25000\n",
      "[1, 3370] train_loss: 0.36261 train_acc: 90.93750 val_loss: 0.53974 val_acc: 85.00000\n",
      "[1, 3380] train_loss: 0.26433 train_acc: 91.56250 val_loss: 0.64121 val_acc: 85.00000\n",
      "[1, 3390] train_loss: 0.29837 train_acc: 92.50000 val_loss: 0.70098 val_acc: 85.00000\n",
      "[1, 3400] train_loss: 0.24409 train_acc: 91.25000 val_loss: 0.51009 val_acc: 86.25000\n",
      "[1, 3410] train_loss: 0.25554 train_acc: 92.18750 val_loss: 0.58604 val_acc: 83.75000\n",
      "[1, 3420] train_loss: 0.23383 train_acc: 94.06250 val_loss: 0.55370 val_acc: 86.25000\n",
      "[1, 3430] train_loss: 0.28311 train_acc: 90.93750 val_loss: 0.64800 val_acc: 80.00000\n",
      "[1, 3440] train_loss: 0.34158 train_acc: 90.31250 val_loss: 0.67832 val_acc: 81.25000\n",
      "[1, 3450] train_loss: 0.37293 train_acc: 90.31250 val_loss: 0.43330 val_acc: 87.50000\n",
      "[1, 3460] train_loss: 0.46714 train_acc: 87.81250 val_loss: 0.46420 val_acc: 90.00000\n",
      "[1, 3470] train_loss: 0.30760 train_acc: 93.12500 val_loss: 0.48917 val_acc: 86.25000\n",
      "[1, 3480] train_loss: 0.28407 train_acc: 90.00000 val_loss: 0.41560 val_acc: 88.75000\n",
      "[1, 3490] train_loss: 0.36383 train_acc: 91.25000 val_loss: 0.58515 val_acc: 87.50000\n",
      "[1, 3500] train_loss: 0.27181 train_acc: 93.75000 val_loss: 0.66761 val_acc: 83.75000\n",
      "[1, 3510] train_loss: 0.29216 train_acc: 91.25000 val_loss: 0.30518 val_acc: 93.75000\n",
      "[1, 3520] train_loss: 0.31755 train_acc: 91.25000 val_loss: 0.77626 val_acc: 83.75000\n",
      "[1, 3530] train_loss: 0.36828 train_acc: 90.00000 val_loss: 0.29052 val_acc: 90.00000\n",
      "[1, 3540] train_loss: 0.27613 train_acc: 91.56250 val_loss: 0.47726 val_acc: 88.75000\n",
      "[1, 3550] train_loss: 0.36575 train_acc: 91.25000 val_loss: 0.61790 val_acc: 83.75000\n",
      "[1, 3560] train_loss: 0.30296 train_acc: 90.31250 val_loss: 0.82375 val_acc: 80.00000\n",
      "[1, 3570] train_loss: 0.30340 train_acc: 91.87500 val_loss: 0.31028 val_acc: 92.50000\n",
      "[1, 3580] train_loss: 0.37096 train_acc: 90.31250 val_loss: 0.79973 val_acc: 80.00000\n",
      "[1, 3590] train_loss: 0.28132 train_acc: 91.87500 val_loss: 0.22026 val_acc: 93.75000\n",
      "[1, 3600] train_loss: 0.32260 train_acc: 89.37500 val_loss: 0.48357 val_acc: 87.50000\n",
      "[1, 3610] train_loss: 0.30118 train_acc: 90.93750 val_loss: 0.53803 val_acc: 88.75000\n",
      "[1, 3620] train_loss: 0.35335 train_acc: 90.31250 val_loss: 0.50029 val_acc: 86.25000\n",
      "[1, 3630] train_loss: 0.41430 train_acc: 88.43750 val_loss: 0.55505 val_acc: 85.00000\n",
      "[1, 3640] train_loss: 0.32670 train_acc: 90.31250 val_loss: 0.30404 val_acc: 91.25000\n",
      "[1, 3650] train_loss: 0.32935 train_acc: 90.31250 val_loss: 0.25806 val_acc: 95.00000\n",
      "[1, 3660] train_loss: 0.35399 train_acc: 90.93750 val_loss: 0.73888 val_acc: 80.00000\n",
      "[1, 3670] train_loss: 0.38705 train_acc: 89.68750 val_loss: 0.72091 val_acc: 85.00000\n",
      "[1, 3680] train_loss: 0.50423 train_acc: 87.50000 val_loss: 0.58153 val_acc: 88.75000\n",
      "[1, 3690] train_loss: 0.31293 train_acc: 91.25000 val_loss: 0.47623 val_acc: 87.50000\n",
      "[1, 3700] train_loss: 0.24532 train_acc: 92.18750 val_loss: 0.44683 val_acc: 90.00000\n",
      "[1, 3710] train_loss: 0.39341 train_acc: 90.31250 val_loss: 0.47137 val_acc: 87.50000\n",
      "[1, 3720] train_loss: 0.27101 train_acc: 91.87500 val_loss: 1.16170 val_acc: 71.25000\n",
      "[1, 3730] train_loss: 0.28969 train_acc: 91.87500 val_loss: 0.69412 val_acc: 83.75000\n",
      "[1, 3740] train_loss: 0.27234 train_acc: 93.12500 val_loss: 0.68439 val_acc: 83.75000\n",
      "[1, 3750] train_loss: 0.27030 train_acc: 93.12500 val_loss: 0.21871 val_acc: 97.50000\n",
      "[1, 3760] train_loss: 0.34493 train_acc: 90.31250 val_loss: 0.71968 val_acc: 85.00000\n",
      "[1, 3770] train_loss: 0.32279 train_acc: 91.56250 val_loss: 0.85337 val_acc: 83.75000\n",
      "[1, 3780] train_loss: 0.25929 train_acc: 92.50000 val_loss: 0.38153 val_acc: 90.00000\n",
      "[1, 3790] train_loss: 0.31788 train_acc: 91.56250 val_loss: 0.47205 val_acc: 86.25000\n",
      "[1, 3800] train_loss: 0.23126 train_acc: 92.18750 val_loss: 0.44022 val_acc: 87.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3810] train_loss: 0.33605 train_acc: 90.62500 val_loss: 0.33161 val_acc: 87.50000\n",
      "[1, 3820] train_loss: 0.23283 train_acc: 95.00000 val_loss: 0.30723 val_acc: 92.50000\n",
      "[1, 3830] train_loss: 0.27322 train_acc: 94.06250 val_loss: 0.81467 val_acc: 82.50000\n",
      "[1, 3840] train_loss: 0.30187 train_acc: 91.25000 val_loss: 0.55793 val_acc: 83.75000\n",
      "[1, 3850] train_loss: 0.26246 train_acc: 92.18750 val_loss: 0.40149 val_acc: 87.50000\n",
      "[1, 3860] train_loss: 0.17746 train_acc: 95.62500 val_loss: 0.58915 val_acc: 87.50000\n",
      "[1, 3870] train_loss: 0.36316 train_acc: 89.37500 val_loss: 0.54474 val_acc: 86.25000\n",
      "[1, 3880] train_loss: 0.22939 train_acc: 91.56250 val_loss: 0.96574 val_acc: 80.00000\n",
      "[1, 3890] train_loss: 0.33491 train_acc: 92.18750 val_loss: 0.33943 val_acc: 92.50000\n",
      "[1, 3900] train_loss: 0.36293 train_acc: 89.37500 val_loss: 0.78117 val_acc: 85.00000\n",
      "[1, 3910] train_loss: 0.22439 train_acc: 93.75000 val_loss: 0.51880 val_acc: 88.75000\n",
      "[1, 3920] train_loss: 0.30043 train_acc: 91.87500 val_loss: 0.31426 val_acc: 91.25000\n",
      "[1, 3930] train_loss: 0.25821 train_acc: 91.56250 val_loss: 0.66572 val_acc: 85.00000\n",
      "[1, 3940] train_loss: 0.31942 train_acc: 90.62500 val_loss: 1.04421 val_acc: 73.75000\n",
      "[1, 3950] train_loss: 0.29307 train_acc: 91.25000 val_loss: 0.65762 val_acc: 86.25000\n",
      "[1, 3960] train_loss: 0.27068 train_acc: 91.56250 val_loss: 0.73802 val_acc: 83.75000\n",
      "[1, 3970] train_loss: 0.33688 train_acc: 89.06250 val_loss: 0.31959 val_acc: 91.25000\n",
      "[1, 3980] train_loss: 0.25569 train_acc: 91.56250 val_loss: 0.53093 val_acc: 86.25000\n",
      "[1, 3990] train_loss: 0.30081 train_acc: 91.87500 val_loss: 0.42531 val_acc: 85.00000\n",
      "[1, 4000] train_loss: 0.30273 train_acc: 90.62500 val_loss: 0.61781 val_acc: 78.75000\n",
      "[1, 4010] train_loss: 0.28575 train_acc: 92.50000 val_loss: 0.42832 val_acc: 85.00000\n",
      "[1, 4020] train_loss: 0.20200 train_acc: 92.81250 val_loss: 0.48078 val_acc: 85.00000\n",
      "[1, 4030] train_loss: 0.22379 train_acc: 94.37500 val_loss: 0.54095 val_acc: 86.25000\n",
      "[1, 4040] train_loss: 0.24139 train_acc: 92.50000 val_loss: 0.48105 val_acc: 85.00000\n",
      "[1, 4050] train_loss: 0.25029 train_acc: 93.12500 val_loss: 0.51754 val_acc: 86.25000\n",
      "[1, 4060] train_loss: 0.29764 train_acc: 91.87500 val_loss: 0.61728 val_acc: 87.50000\n",
      "[1, 4070] train_loss: 0.36472 train_acc: 89.37500 val_loss: 0.65159 val_acc: 83.75000\n",
      "[1, 4080] train_loss: 0.23865 train_acc: 93.43750 val_loss: 0.65258 val_acc: 88.75000\n",
      "[1, 4090] train_loss: 0.30985 train_acc: 92.81250 val_loss: 0.47537 val_acc: 86.25000\n",
      "[1, 4100] train_loss: 0.28657 train_acc: 92.18750 val_loss: 0.70671 val_acc: 86.25000\n",
      "[1, 4110] train_loss: 0.28448 train_acc: 92.50000 val_loss: 0.60553 val_acc: 85.00000\n",
      "[1, 4120] train_loss: 0.29504 train_acc: 91.25000 val_loss: 0.44872 val_acc: 87.50000\n",
      "[1, 4130] train_loss: 0.28859 train_acc: 90.62500 val_loss: 0.80559 val_acc: 85.00000\n",
      "[1, 4140] train_loss: 0.31982 train_acc: 91.56250 val_loss: 0.53498 val_acc: 91.25000\n",
      "[1, 4150] train_loss: 0.24371 train_acc: 93.75000 val_loss: 0.35698 val_acc: 88.75000\n",
      "[1, 4160] train_loss: 0.31416 train_acc: 91.87500 val_loss: 0.23287 val_acc: 95.00000\n",
      "[1, 4170] train_loss: 0.33425 train_acc: 93.12500 val_loss: 0.52444 val_acc: 86.25000\n",
      "[1, 4180] train_loss: 0.28404 train_acc: 92.18750 val_loss: 0.55045 val_acc: 92.50000\n",
      "[1, 4190] train_loss: 0.27005 train_acc: 93.43750 val_loss: 0.58159 val_acc: 86.25000\n",
      "[1, 4200] train_loss: 0.33806 train_acc: 90.93750 val_loss: 0.59697 val_acc: 88.75000\n",
      "[1, 4210] train_loss: 0.31515 train_acc: 90.31250 val_loss: 0.55430 val_acc: 88.75000\n",
      "[1, 4220] train_loss: 0.30924 train_acc: 91.25000 val_loss: 0.52342 val_acc: 87.50000\n",
      "[1, 4230] train_loss: 0.27142 train_acc: 91.56250 val_loss: 0.42883 val_acc: 91.25000\n",
      "[1, 4240] train_loss: 0.41240 train_acc: 90.31250 val_loss: 0.52667 val_acc: 87.50000\n",
      "[1, 4250] train_loss: 0.24473 train_acc: 92.81250 val_loss: 0.44158 val_acc: 90.00000\n",
      "[1, 4260] train_loss: 0.27579 train_acc: 90.93750 val_loss: 0.47924 val_acc: 85.00000\n",
      "[1, 4270] train_loss: 0.23136 train_acc: 92.81250 val_loss: 0.70776 val_acc: 80.00000\n",
      "[1, 4280] train_loss: 0.24277 train_acc: 92.50000 val_loss: 0.36470 val_acc: 88.75000\n",
      "[1, 4290] train_loss: 0.27199 train_acc: 92.18750 val_loss: 0.77054 val_acc: 80.00000\n",
      "[1, 4300] train_loss: 0.38258 train_acc: 90.93750 val_loss: 0.38401 val_acc: 90.00000\n",
      "[1, 4310] train_loss: 0.24893 train_acc: 91.87500 val_loss: 0.55000 val_acc: 87.50000\n",
      "[1, 4320] train_loss: 0.25945 train_acc: 91.87500 val_loss: 0.54954 val_acc: 85.00000\n",
      "[1, 4330] train_loss: 0.26428 train_acc: 93.12500 val_loss: 0.72545 val_acc: 87.50000\n",
      "[1, 4340] train_loss: 0.27226 train_acc: 93.43750 val_loss: 0.56466 val_acc: 85.00000\n",
      "[1, 4350] train_loss: 0.22408 train_acc: 94.37500 val_loss: 0.58307 val_acc: 87.50000\n",
      "[1, 4360] train_loss: 0.25664 train_acc: 91.87500 val_loss: 0.48834 val_acc: 87.50000\n",
      "[1, 4370] train_loss: 0.22796 train_acc: 91.87500 val_loss: 0.54952 val_acc: 86.25000\n",
      "[1, 4380] train_loss: 0.28814 train_acc: 91.56250 val_loss: 0.76888 val_acc: 87.50000\n",
      "[1, 4390] train_loss: 0.21848 train_acc: 92.81250 val_loss: 0.61987 val_acc: 87.50000\n",
      "[1, 4400] train_loss: 0.30743 train_acc: 92.50000 val_loss: 0.25476 val_acc: 91.25000\n",
      "[1, 4410] train_loss: 0.29516 train_acc: 90.93750 val_loss: 0.54326 val_acc: 85.00000\n",
      "[1, 4420] train_loss: 0.22881 train_acc: 92.81250 val_loss: 0.53880 val_acc: 86.25000\n",
      "[1, 4430] train_loss: 0.24494 train_acc: 91.87500 val_loss: 0.32369 val_acc: 93.75000\n",
      "[1, 4440] train_loss: 0.36175 train_acc: 90.62500 val_loss: 0.64115 val_acc: 85.00000\n",
      "[1, 4450] train_loss: 0.35737 train_acc: 90.62500 val_loss: 0.57762 val_acc: 90.00000\n",
      "[1, 4460] train_loss: 0.23700 train_acc: 92.18750 val_loss: 0.56058 val_acc: 88.75000\n",
      "[1, 4470] train_loss: 0.25227 train_acc: 92.50000 val_loss: 0.45269 val_acc: 86.25000\n",
      "[1, 4480] train_loss: 0.28721 train_acc: 91.25000 val_loss: 0.32066 val_acc: 92.50000\n",
      "[1, 4490] train_loss: 0.25619 train_acc: 94.06250 val_loss: 0.42526 val_acc: 83.75000\n",
      "[1, 4500] train_loss: 0.30824 train_acc: 91.87500 val_loss: 0.33782 val_acc: 91.25000\n",
      "[1, 4510] train_loss: 0.32921 train_acc: 90.00000 val_loss: 0.70849 val_acc: 81.25000\n",
      "[1, 4520] train_loss: 0.26899 train_acc: 92.81250 val_loss: 0.58752 val_acc: 87.50000\n",
      "[1, 4530] train_loss: 0.24509 train_acc: 93.12500 val_loss: 0.52334 val_acc: 85.00000\n",
      "[1, 4540] train_loss: 0.23372 train_acc: 94.06250 val_loss: 0.60193 val_acc: 87.50000\n",
      "[1, 4550] train_loss: 0.31626 train_acc: 90.62500 val_loss: 0.48118 val_acc: 90.00000\n",
      "[1, 4560] train_loss: 0.27545 train_acc: 92.81250 val_loss: 0.67186 val_acc: 80.00000\n",
      "[1, 4570] train_loss: 0.30324 train_acc: 93.43750 val_loss: 0.57287 val_acc: 88.75000\n",
      "[1, 4580] train_loss: 0.26173 train_acc: 93.43750 val_loss: 0.63233 val_acc: 83.75000\n",
      "[1, 4590] train_loss: 0.29585 train_acc: 90.93750 val_loss: 0.70381 val_acc: 83.75000\n",
      "[1, 4600] train_loss: 0.30480 train_acc: 90.62500 val_loss: 0.60971 val_acc: 87.50000\n",
      "[1, 4610] train_loss: 0.25602 train_acc: 92.50000 val_loss: 0.20725 val_acc: 93.75000\n",
      "[1, 4620] train_loss: 0.29937 train_acc: 90.31250 val_loss: 0.64460 val_acc: 81.25000\n",
      "[1, 4630] train_loss: 0.25857 train_acc: 92.18750 val_loss: 0.51589 val_acc: 81.25000\n",
      "[1, 4640] train_loss: 0.28824 train_acc: 91.25000 val_loss: 0.62163 val_acc: 87.50000\n",
      "[1, 4650] train_loss: 0.28993 train_acc: 91.25000 val_loss: 0.40877 val_acc: 90.00000\n",
      "[1, 4660] train_loss: 0.23046 train_acc: 93.43750 val_loss: 0.68600 val_acc: 87.50000\n",
      "[1, 4670] train_loss: 0.26911 train_acc: 91.87500 val_loss: 0.24968 val_acc: 96.25000\n",
      "[1, 4680] train_loss: 0.23835 train_acc: 92.81250 val_loss: 0.49893 val_acc: 85.00000\n",
      "[1, 4690] train_loss: 0.22587 train_acc: 93.12500 val_loss: 0.52815 val_acc: 83.75000\n",
      "[1, 4700] train_loss: 0.27911 train_acc: 91.56250 val_loss: 0.15453 val_acc: 95.00000\n",
      "[1, 4710] train_loss: 0.29455 train_acc: 93.12500 val_loss: 0.35913 val_acc: 86.25000\n",
      "[1, 4720] train_loss: 0.14652 train_acc: 94.68750 val_loss: 0.55146 val_acc: 88.75000\n",
      "[1, 4730] train_loss: 0.27861 train_acc: 92.18750 val_loss: 0.51243 val_acc: 91.25000\n",
      "[1, 4740] train_loss: 0.20579 train_acc: 92.50000 val_loss: 0.59893 val_acc: 87.50000\n",
      "[1, 4750] train_loss: 0.21300 train_acc: 93.75000 val_loss: 0.42919 val_acc: 86.25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4760] train_loss: 0.19207 train_acc: 94.06250 val_loss: 0.45476 val_acc: 86.25000\n",
      "[1, 4770] train_loss: 0.19983 train_acc: 94.06250 val_loss: 0.49958 val_acc: 87.50000\n",
      "[1, 4780] train_loss: 0.22542 train_acc: 93.12500 val_loss: 0.34619 val_acc: 90.00000\n",
      "[1, 4790] train_loss: 0.19190 train_acc: 95.00000 val_loss: 0.63103 val_acc: 87.50000\n",
      "[1, 4800] train_loss: 0.24342 train_acc: 94.37500 val_loss: 0.69784 val_acc: 81.25000\n",
      "[1, 4810] train_loss: 0.29656 train_acc: 93.12500 val_loss: 0.54071 val_acc: 86.25000\n",
      "[1, 4820] train_loss: 0.25357 train_acc: 93.43750 val_loss: 0.68980 val_acc: 83.75000\n",
      "[1, 4830] train_loss: 0.25014 train_acc: 93.75000 val_loss: 0.36258 val_acc: 91.25000\n",
      "[1, 4840] train_loss: 0.16078 train_acc: 94.68750 val_loss: 0.56112 val_acc: 85.00000\n",
      "[1, 4850] train_loss: 0.27115 train_acc: 94.68750 val_loss: 0.63589 val_acc: 88.75000\n",
      "[1, 4860] train_loss: 0.14607 train_acc: 96.25000 val_loss: 0.91928 val_acc: 83.75000\n",
      "[1, 4870] train_loss: 0.25520 train_acc: 92.18750 val_loss: 0.72028 val_acc: 85.00000\n",
      "[1, 4880] train_loss: 0.31163 train_acc: 91.56250 val_loss: 0.88741 val_acc: 82.50000\n",
      "[1, 4890] train_loss: 0.27475 train_acc: 91.56250 val_loss: 0.50064 val_acc: 88.75000\n",
      "[1, 4900] train_loss: 0.27835 train_acc: 92.50000 val_loss: 0.85131 val_acc: 80.00000\n",
      "[1, 4910] train_loss: 0.16395 train_acc: 94.37500 val_loss: 0.79775 val_acc: 82.50000\n",
      "[1, 4920] train_loss: 0.24899 train_acc: 92.50000 val_loss: 0.71029 val_acc: 81.25000\n",
      "[1, 4930] train_loss: 0.20131 train_acc: 94.06250 val_loss: 0.44306 val_acc: 85.00000\n",
      "[1, 4940] train_loss: 0.21637 train_acc: 94.06250 val_loss: 0.82698 val_acc: 85.00000\n",
      "[1, 4950] train_loss: 0.29302 train_acc: 93.75000 val_loss: 0.74154 val_acc: 85.00000\n",
      "[1, 4960] train_loss: 0.22945 train_acc: 93.12500 val_loss: 0.33119 val_acc: 92.50000\n",
      "[1, 4970] train_loss: 0.22356 train_acc: 94.37500 val_loss: 0.49714 val_acc: 88.75000\n",
      "[1, 4980] train_loss: 0.29109 train_acc: 92.81250 val_loss: 0.41287 val_acc: 91.25000\n",
      "[1, 4990] train_loss: 0.30624 train_acc: 91.87500 val_loss: 0.73045 val_acc: 81.25000\n",
      "[1, 5000] train_loss: 0.23847 train_acc: 93.12500 val_loss: 0.60888 val_acc: 86.25000\n",
      "[1, 5010] train_loss: 0.26130 train_acc: 92.81250 val_loss: 0.50702 val_acc: 85.00000\n",
      "[1, 5020] train_loss: 0.27162 train_acc: 91.56250 val_loss: 0.51155 val_acc: 85.00000\n",
      "[1, 5030] train_loss: 0.19874 train_acc: 94.68750 val_loss: 0.71101 val_acc: 83.75000\n",
      "[1, 5040] train_loss: 0.21468 train_acc: 94.06250 val_loss: 0.44358 val_acc: 88.75000\n",
      "[1, 5050] train_loss: 0.18959 train_acc: 94.06250 val_loss: 0.40855 val_acc: 87.50000\n",
      "[1, 5060] train_loss: 0.19679 train_acc: 95.00000 val_loss: 0.24848 val_acc: 92.50000\n",
      "[1, 5070] train_loss: 0.16369 train_acc: 94.68750 val_loss: 0.47204 val_acc: 88.75000\n",
      "[1, 5080] train_loss: 0.23353 train_acc: 94.37500 val_loss: 0.42647 val_acc: 90.00000\n",
      "[1, 5090] train_loss: 0.18963 train_acc: 94.06250 val_loss: 0.44283 val_acc: 91.25000\n",
      "[1, 5100] train_loss: 0.20145 train_acc: 93.43750 val_loss: 0.50768 val_acc: 88.75000\n",
      "[1, 5110] train_loss: 0.25153 train_acc: 92.81250 val_loss: 0.55190 val_acc: 88.75000\n",
      "[1, 5120] train_loss: 0.23887 train_acc: 93.75000 val_loss: 0.73612 val_acc: 83.75000\n",
      "[1, 5130] train_loss: 0.17681 train_acc: 95.62500 val_loss: 0.46686 val_acc: 83.75000\n",
      "[1, 5140] train_loss: 0.15906 train_acc: 95.31250 val_loss: 0.70250 val_acc: 86.25000\n",
      "[1, 5150] train_loss: 0.23766 train_acc: 93.12500 val_loss: 0.41744 val_acc: 90.00000\n",
      "[1, 5160] train_loss: 0.19782 train_acc: 94.06250 val_loss: 0.66445 val_acc: 82.50000\n",
      "[1, 5170] train_loss: 0.18117 train_acc: 93.43750 val_loss: 0.31734 val_acc: 92.50000\n",
      "[1, 5180] train_loss: 0.24504 train_acc: 93.43750 val_loss: 0.34081 val_acc: 91.25000\n",
      "[1, 5190] train_loss: 0.21532 train_acc: 93.43750 val_loss: 0.19917 val_acc: 92.50000\n",
      "[1, 5200] train_loss: 0.23471 train_acc: 93.43750 val_loss: 0.33131 val_acc: 92.50000\n",
      "[1, 5210] train_loss: 0.20251 train_acc: 95.31250 val_loss: 0.48426 val_acc: 91.25000\n",
      "[1, 5220] train_loss: 0.14068 train_acc: 95.93750 val_loss: 0.28073 val_acc: 87.50000\n",
      "[1, 5230] train_loss: 0.17201 train_acc: 95.31250 val_loss: 0.57941 val_acc: 87.50000\n",
      "[1, 5240] train_loss: 0.20361 train_acc: 94.06250 val_loss: 0.32990 val_acc: 91.25000\n",
      "[1, 5250] train_loss: 0.23537 train_acc: 93.12500 val_loss: 0.59867 val_acc: 86.25000\n",
      "[1, 5260] train_loss: 0.25621 train_acc: 93.75000 val_loss: 0.48422 val_acc: 88.75000\n",
      "[1, 5270] train_loss: 0.22165 train_acc: 93.43750 val_loss: 0.57045 val_acc: 90.00000\n",
      "[1, 5280] train_loss: 0.27313 train_acc: 91.56250 val_loss: 0.29416 val_acc: 88.75000\n",
      "[1, 5290] train_loss: 0.20781 train_acc: 93.75000 val_loss: 0.24834 val_acc: 92.50000\n",
      "[1, 5300] train_loss: 0.26660 train_acc: 91.87500 val_loss: 0.65103 val_acc: 83.75000\n",
      "[1, 5310] train_loss: 0.23461 train_acc: 93.43750 val_loss: 0.67733 val_acc: 87.50000\n",
      "[1, 5320] train_loss: 0.14273 train_acc: 95.00000 val_loss: 0.46117 val_acc: 92.50000\n",
      "[1, 5330] train_loss: 0.23637 train_acc: 93.43750 val_loss: 0.28259 val_acc: 88.75000\n",
      "[1, 5340] train_loss: 0.20763 train_acc: 94.06250 val_loss: 0.83399 val_acc: 78.75000\n",
      "[1, 5350] train_loss: 0.30249 train_acc: 91.56250 val_loss: 0.78331 val_acc: 83.75000\n",
      "[1, 5360] train_loss: 0.30657 train_acc: 90.93750 val_loss: 0.26601 val_acc: 92.50000\n",
      "[1, 5370] train_loss: 0.27962 train_acc: 91.25000 val_loss: 0.56181 val_acc: 90.00000\n",
      "[1, 5380] train_loss: 0.25569 train_acc: 92.81250 val_loss: 0.58107 val_acc: 85.00000\n",
      "[1, 5390] train_loss: 0.24776 train_acc: 92.18750 val_loss: 0.24169 val_acc: 91.25000\n",
      "[1, 5400] train_loss: 0.19715 train_acc: 94.06250 val_loss: 0.38043 val_acc: 85.00000\n",
      "[1, 5410] train_loss: 0.12799 train_acc: 97.50000 val_loss: 0.42457 val_acc: 88.75000\n",
      "[1, 5420] train_loss: 0.19382 train_acc: 94.37500 val_loss: 0.33514 val_acc: 92.50000\n",
      "[1, 5430] train_loss: 0.16381 train_acc: 96.25000 val_loss: 0.45524 val_acc: 86.25000\n",
      "[1, 5440] train_loss: 0.23932 train_acc: 95.93750 val_loss: 0.55622 val_acc: 87.50000\n",
      "[1, 5450] train_loss: 0.22217 train_acc: 94.68750 val_loss: 0.42402 val_acc: 83.75000\n",
      "[1, 5460] train_loss: 0.19041 train_acc: 94.37500 val_loss: 0.25729 val_acc: 91.25000\n",
      "[1, 5470] train_loss: 0.25944 train_acc: 93.43750 val_loss: 0.58121 val_acc: 85.00000\n",
      "[1, 5480] train_loss: 0.18696 train_acc: 94.06250 val_loss: 0.80786 val_acc: 85.00000\n",
      "[1, 5490] train_loss: 0.22113 train_acc: 94.06250 val_loss: 0.44634 val_acc: 87.50000\n",
      "[1, 5500] train_loss: 0.22080 train_acc: 94.06250 val_loss: 0.54811 val_acc: 86.25000\n",
      "[1, 5510] train_loss: 0.25762 train_acc: 93.12500 val_loss: 0.65426 val_acc: 86.25000\n",
      "[1, 5520] train_loss: 0.21643 train_acc: 92.81250 val_loss: 0.71068 val_acc: 82.50000\n",
      "[1, 5530] train_loss: 0.28655 train_acc: 92.50000 val_loss: 0.49054 val_acc: 91.25000\n",
      "[1, 5540] train_loss: 0.21650 train_acc: 95.62500 val_loss: 0.47052 val_acc: 91.25000\n",
      "[1, 5550] train_loss: 0.32081 train_acc: 90.62500 val_loss: 0.33853 val_acc: 90.00000\n",
      "[1, 5560] train_loss: 0.22935 train_acc: 94.37500 val_loss: 0.44802 val_acc: 92.50000\n",
      "[1, 5570] train_loss: 0.15880 train_acc: 95.31250 val_loss: 0.85310 val_acc: 83.75000\n",
      "[1, 5580] train_loss: 0.18392 train_acc: 95.31250 val_loss: 0.79567 val_acc: 87.50000\n",
      "[1, 5590] train_loss: 0.15331 train_acc: 94.68750 val_loss: 1.19045 val_acc: 77.50000\n",
      "[1, 5600] train_loss: 0.23067 train_acc: 93.12500 val_loss: 0.15570 val_acc: 93.75000\n",
      "[1, 5610] train_loss: 0.25890 train_acc: 94.06250 val_loss: 0.51140 val_acc: 88.75000\n",
      "[1, 5620] train_loss: 0.16260 train_acc: 95.62500 val_loss: 0.55090 val_acc: 87.50000\n",
      "[1, 5630] train_loss: 0.22619 train_acc: 93.43750 val_loss: 0.47485 val_acc: 90.00000\n",
      "[1, 5640] train_loss: 0.09349 train_acc: 97.50000 val_loss: 0.73294 val_acc: 87.50000\n",
      "[1, 5650] train_loss: 0.23091 train_acc: 92.18750 val_loss: 0.54642 val_acc: 85.00000\n",
      "[1, 5660] train_loss: 0.19006 train_acc: 92.81250 val_loss: 0.97419 val_acc: 81.25000\n",
      "[1, 5670] train_loss: 0.21044 train_acc: 94.06250 val_loss: 0.36628 val_acc: 88.75000\n",
      "[1, 5680] train_loss: 0.20668 train_acc: 96.25000 val_loss: 0.38255 val_acc: 88.75000\n",
      "[1, 5690] train_loss: 0.16664 train_acc: 95.93750 val_loss: 0.47379 val_acc: 87.50000\n",
      "[1, 5700] train_loss: 0.22127 train_acc: 94.06250 val_loss: 0.69184 val_acc: 81.25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5710] train_loss: 0.15362 train_acc: 94.68750 val_loss: 0.53752 val_acc: 88.75000\n",
      "[1, 5720] train_loss: 0.10817 train_acc: 96.25000 val_loss: 0.75661 val_acc: 85.00000\n",
      "[1, 5730] train_loss: 0.19824 train_acc: 94.68750 val_loss: 0.57773 val_acc: 87.50000\n",
      "[1, 5740] train_loss: 0.25168 train_acc: 94.37500 val_loss: 0.36683 val_acc: 93.75000\n",
      "[1, 5750] train_loss: 0.23767 train_acc: 92.81250 val_loss: 0.06553 val_acc: 98.75000\n",
      "[1, 5760] train_loss: 0.15287 train_acc: 95.31250 val_loss: 0.28160 val_acc: 91.25000\n",
      "[1, 5770] train_loss: 0.14819 train_acc: 95.00000 val_loss: 0.67839 val_acc: 87.50000\n",
      "[1, 5780] train_loss: 0.17647 train_acc: 94.06250 val_loss: 0.87088 val_acc: 83.75000\n",
      "[1, 5790] train_loss: 0.17584 train_acc: 95.93750 val_loss: 0.72760 val_acc: 86.25000\n",
      "[1, 5800] train_loss: 0.22250 train_acc: 92.81250 val_loss: 0.75082 val_acc: 82.50000\n",
      "[1, 5810] train_loss: 0.23927 train_acc: 93.43750 val_loss: 0.30363 val_acc: 95.00000\n",
      "[1, 5820] train_loss: 0.20909 train_acc: 94.37500 val_loss: 0.27844 val_acc: 90.00000\n",
      "[1, 5830] train_loss: 0.22528 train_acc: 93.12500 val_loss: 1.15200 val_acc: 75.00000\n",
      "[1, 5840] train_loss: 0.17786 train_acc: 95.00000 val_loss: 0.35225 val_acc: 93.75000\n",
      "[1, 5850] train_loss: 0.22094 train_acc: 93.75000 val_loss: 0.51060 val_acc: 87.50000\n",
      "[1, 5860] train_loss: 0.19671 train_acc: 94.68750 val_loss: 0.73095 val_acc: 85.00000\n",
      "[1, 5870] train_loss: 0.20017 train_acc: 94.37500 val_loss: 0.90178 val_acc: 83.75000\n",
      "[1, 5880] train_loss: 0.15884 train_acc: 94.68750 val_loss: 0.54104 val_acc: 86.25000\n",
      "[1, 5890] train_loss: 0.23324 train_acc: 94.06250 val_loss: 0.52988 val_acc: 86.25000\n",
      "[1, 5900] train_loss: 0.23141 train_acc: 92.81250 val_loss: 0.43092 val_acc: 87.50000\n",
      "[1, 5910] train_loss: 0.21957 train_acc: 95.00000 val_loss: 0.64004 val_acc: 90.00000\n",
      "[1, 5920] train_loss: 0.20680 train_acc: 93.43750 val_loss: 0.39309 val_acc: 90.00000\n",
      "[1, 5930] train_loss: 0.19553 train_acc: 93.43750 val_loss: 0.62565 val_acc: 87.50000\n",
      "[1, 5940] train_loss: 0.19139 train_acc: 93.75000 val_loss: 0.49003 val_acc: 91.25000\n",
      "[1, 5950] train_loss: 0.16473 train_acc: 95.31250 val_loss: 0.53211 val_acc: 87.50000\n",
      "[1, 5960] train_loss: 0.12681 train_acc: 96.56250 val_loss: 0.55302 val_acc: 88.75000\n",
      "[1, 5970] train_loss: 0.24258 train_acc: 94.06250 val_loss: 0.59303 val_acc: 86.25000\n",
      "[1, 5980] train_loss: 0.15972 train_acc: 95.31250 val_loss: 0.50465 val_acc: 87.50000\n",
      "[1, 5990] train_loss: 0.21532 train_acc: 94.68750 val_loss: 0.46701 val_acc: 90.00000\n",
      "[1, 6000] train_loss: 0.22334 train_acc: 92.81250 val_loss: 0.84993 val_acc: 85.00000\n",
      "[1, 6010] train_loss: 0.27536 train_acc: 94.06250 val_loss: 0.47597 val_acc: 88.75000\n",
      "[1, 6020] train_loss: 0.22723 train_acc: 94.06250 val_loss: 0.20527 val_acc: 91.25000\n",
      "[1, 6030] train_loss: 0.17939 train_acc: 95.00000 val_loss: 0.32238 val_acc: 92.50000\n",
      "[1, 6040] train_loss: 0.14216 train_acc: 96.25000 val_loss: 0.67728 val_acc: 87.50000\n",
      "[1, 6050] train_loss: 0.15039 train_acc: 95.31250 val_loss: 0.31128 val_acc: 91.25000\n",
      "[1, 6060] train_loss: 0.23052 train_acc: 92.81250 val_loss: 0.23208 val_acc: 91.25000\n",
      "[1, 6070] train_loss: 0.22751 train_acc: 93.75000 val_loss: 0.39815 val_acc: 87.50000\n",
      "[1, 6080] train_loss: 0.17045 train_acc: 95.62500 val_loss: 0.65685 val_acc: 86.25000\n",
      "[1, 6090] train_loss: 0.08953 train_acc: 97.50000 val_loss: 0.53510 val_acc: 88.75000\n",
      "[1, 6100] train_loss: 0.27528 train_acc: 95.00000 val_loss: 0.74128 val_acc: 82.50000\n",
      "[1, 6110] train_loss: 0.16231 train_acc: 94.68750 val_loss: 0.34408 val_acc: 91.25000\n",
      "[1, 6120] train_loss: 0.10260 train_acc: 96.87500 val_loss: 0.45867 val_acc: 90.00000\n",
      "[1, 6130] train_loss: 0.18154 train_acc: 95.31250 val_loss: 0.29964 val_acc: 95.00000\n",
      "[1, 6140] train_loss: 0.20921 train_acc: 94.37500 val_loss: 0.49985 val_acc: 87.50000\n",
      "[1, 6150] train_loss: 0.27912 train_acc: 92.18750 val_loss: 0.75576 val_acc: 85.00000\n",
      "[1, 6160] train_loss: 0.18751 train_acc: 94.68750 val_loss: 0.57423 val_acc: 88.75000\n",
      "[1, 6170] train_loss: 0.22108 train_acc: 93.12500 val_loss: 0.46170 val_acc: 90.00000\n",
      "[1, 6180] train_loss: 0.21192 train_acc: 93.43750 val_loss: 0.61818 val_acc: 88.75000\n",
      "[1, 6190] train_loss: 0.23970 train_acc: 90.93750 val_loss: 0.17209 val_acc: 96.25000\n",
      "[1, 6200] train_loss: 0.27500 train_acc: 92.81250 val_loss: 0.20034 val_acc: 95.00000\n",
      "[1, 6210] train_loss: 0.19846 train_acc: 94.06250 val_loss: 0.50824 val_acc: 88.75000\n",
      "[1, 6220] train_loss: 0.18535 train_acc: 93.43750 val_loss: 0.31613 val_acc: 92.50000\n",
      "[1, 6230] train_loss: 0.18086 train_acc: 94.68750 val_loss: 0.58357 val_acc: 85.00000\n",
      "[1, 6240] train_loss: 0.19155 train_acc: 94.06250 val_loss: 0.52819 val_acc: 87.50000\n",
      "[1, 6250] train_loss: 0.21968 train_acc: 91.87500 val_loss: 0.41559 val_acc: 91.25000\n",
      "[1, 6260] train_loss: 0.16781 train_acc: 95.31250 val_loss: 0.60197 val_acc: 85.00000\n",
      "[1, 6270] train_loss: 0.14644 train_acc: 95.62500 val_loss: 0.35956 val_acc: 92.50000\n",
      "[1, 6280] train_loss: 0.19627 train_acc: 93.43750 val_loss: 0.25023 val_acc: 93.75000\n",
      "[1, 6290] train_loss: 0.16164 train_acc: 94.68750 val_loss: 0.45814 val_acc: 88.75000\n",
      "[1, 6300] train_loss: 0.19482 train_acc: 95.00000 val_loss: 0.32282 val_acc: 88.75000\n",
      "[1, 6310] train_loss: 0.16927 train_acc: 95.00000 val_loss: 0.66625 val_acc: 86.25000\n",
      "[1, 6320] train_loss: 0.23403 train_acc: 94.06250 val_loss: 0.69647 val_acc: 87.50000\n",
      "[1, 6330] train_loss: 0.18449 train_acc: 95.00000 val_loss: 0.59564 val_acc: 83.75000\n",
      "[1, 6340] train_loss: 0.21162 train_acc: 93.75000 val_loss: 0.87265 val_acc: 83.75000\n",
      "[1, 6350] train_loss: 0.18081 train_acc: 94.37500 val_loss: 0.26054 val_acc: 91.25000\n",
      "[1, 6360] train_loss: 0.18277 train_acc: 94.68750 val_loss: 0.71953 val_acc: 87.50000\n",
      "[1, 6370] train_loss: 0.24371 train_acc: 94.68750 val_loss: 0.51510 val_acc: 91.25000\n",
      "[1, 6380] train_loss: 0.12905 train_acc: 94.68750 val_loss: 0.52477 val_acc: 85.00000\n",
      "[1, 6390] train_loss: 0.24448 train_acc: 93.12500 val_loss: 0.31210 val_acc: 92.50000\n",
      "[1, 6400] train_loss: 0.26531 train_acc: 92.50000 val_loss: 0.46839 val_acc: 91.25000\n",
      "[1, 6410] train_loss: 0.19063 train_acc: 94.68750 val_loss: 0.85792 val_acc: 81.25000\n",
      "[1, 6420] train_loss: 0.18482 train_acc: 95.31250 val_loss: 0.44341 val_acc: 87.50000\n",
      "[1, 6430] train_loss: 0.23628 train_acc: 94.37500 val_loss: 0.43210 val_acc: 88.75000\n",
      "[1, 6440] train_loss: 0.21066 train_acc: 95.00000 val_loss: 0.64460 val_acc: 86.25000\n",
      "[1, 6450] train_loss: 0.16176 train_acc: 95.31250 val_loss: 0.49289 val_acc: 90.00000\n",
      "[1, 6460] train_loss: 0.16064 train_acc: 95.00000 val_loss: 1.03980 val_acc: 81.25000\n",
      "[1, 6470] train_loss: 0.22569 train_acc: 92.81250 val_loss: 0.27233 val_acc: 91.25000\n",
      "[1, 6480] train_loss: 0.18318 train_acc: 95.00000 val_loss: 0.40553 val_acc: 90.00000\n",
      "[1, 6490] train_loss: 0.29316 train_acc: 92.81250 val_loss: 0.35301 val_acc: 90.00000\n",
      "[1, 6500] train_loss: 0.20400 train_acc: 93.12500 val_loss: 0.51121 val_acc: 90.00000\n",
      "[1, 6510] train_loss: 0.15838 train_acc: 95.31250 val_loss: 0.52307 val_acc: 86.25000\n",
      "[1, 6520] train_loss: 0.15235 train_acc: 95.93750 val_loss: 0.61593 val_acc: 85.00000\n",
      "[1, 6530] train_loss: 0.23398 train_acc: 92.81250 val_loss: 0.49417 val_acc: 85.00000\n",
      "[1, 6540] train_loss: 0.16417 train_acc: 95.62500 val_loss: 0.38198 val_acc: 91.25000\n",
      "[1, 6550] train_loss: 0.18295 train_acc: 93.75000 val_loss: 0.21553 val_acc: 93.75000\n",
      "[1, 6560] train_loss: 0.19284 train_acc: 94.68750 val_loss: 0.28498 val_acc: 92.50000\n",
      "[1, 6570] train_loss: 0.12334 train_acc: 95.31250 val_loss: 0.49344 val_acc: 86.25000\n",
      "[1, 6580] train_loss: 0.12555 train_acc: 95.62500 val_loss: 0.35588 val_acc: 88.75000\n",
      "[1, 6590] train_loss: 0.19224 train_acc: 95.31250 val_loss: 0.51133 val_acc: 90.00000\n",
      "[1, 6600] train_loss: 0.25811 train_acc: 92.81250 val_loss: 0.53503 val_acc: 86.25000\n",
      "[1, 6610] train_loss: 0.17780 train_acc: 95.31250 val_loss: 1.01949 val_acc: 85.00000\n",
      "[1, 6620] train_loss: 0.20457 train_acc: 95.00000 val_loss: 0.83735 val_acc: 83.75000\n",
      "[1, 6630] train_loss: 0.18687 train_acc: 95.00000 val_loss: 0.56018 val_acc: 88.75000\n",
      "[1, 6640] train_loss: 0.16911 train_acc: 94.68750 val_loss: 0.54453 val_acc: 87.50000\n",
      "[1, 6650] train_loss: 0.11018 train_acc: 95.62500 val_loss: 0.43253 val_acc: 87.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6660] train_loss: 0.19330 train_acc: 95.00000 val_loss: 0.48684 val_acc: 86.25000\n",
      "[1, 6670] train_loss: 0.32832 train_acc: 93.43750 val_loss: 0.64027 val_acc: 86.25000\n",
      "[1, 6680] train_loss: 0.10907 train_acc: 96.87500 val_loss: 0.63176 val_acc: 90.00000\n",
      "[1, 6690] train_loss: 0.15077 train_acc: 95.00000 val_loss: 0.70784 val_acc: 87.50000\n",
      "[1, 6700] train_loss: 0.19644 train_acc: 93.43750 val_loss: 0.48575 val_acc: 88.75000\n",
      "[1, 6710] train_loss: 0.17466 train_acc: 95.93750 val_loss: 0.44522 val_acc: 88.75000\n",
      "[1, 6720] train_loss: 0.23231 train_acc: 92.50000 val_loss: 0.67932 val_acc: 87.50000\n",
      "[1, 6730] train_loss: 0.16683 train_acc: 95.00000 val_loss: 0.43735 val_acc: 91.25000\n",
      "[1, 6740] train_loss: 0.20713 train_acc: 94.37500 val_loss: 0.64584 val_acc: 88.75000\n",
      "[1, 6750] train_loss: 0.23035 train_acc: 93.43750 val_loss: 0.47669 val_acc: 87.50000\n",
      "[1, 6760] train_loss: 0.24550 train_acc: 94.06250 val_loss: 0.70705 val_acc: 82.50000\n",
      "[1, 6770] train_loss: 0.12372 train_acc: 96.87500 val_loss: 0.94739 val_acc: 82.50000\n",
      "[1, 6780] train_loss: 0.18669 train_acc: 95.31250 val_loss: 0.46216 val_acc: 91.25000\n",
      "[1, 6790] train_loss: 0.22178 train_acc: 93.43750 val_loss: 0.50184 val_acc: 88.75000\n",
      "[1, 6800] train_loss: 0.22535 train_acc: 93.43750 val_loss: 0.41509 val_acc: 88.75000\n",
      "[1, 6810] train_loss: 0.25333 train_acc: 93.75000 val_loss: 0.44700 val_acc: 88.75000\n",
      "[1, 6820] train_loss: 0.21790 train_acc: 94.68750 val_loss: 0.40382 val_acc: 91.25000\n",
      "[1, 6830] train_loss: 0.17633 train_acc: 94.68750 val_loss: 0.85170 val_acc: 82.50000\n",
      "[1, 6840] train_loss: 0.13898 train_acc: 95.62500 val_loss: 0.57997 val_acc: 83.75000\n",
      "[1, 6850] train_loss: 0.16142 train_acc: 94.37500 val_loss: 1.00530 val_acc: 85.00000\n",
      "[1, 6860] train_loss: 0.20593 train_acc: 94.68750 val_loss: 0.62581 val_acc: 87.50000\n",
      "[1, 6870] train_loss: 0.30738 train_acc: 92.81250 val_loss: 0.47004 val_acc: 91.25000\n",
      "[1, 6880] train_loss: 0.15702 train_acc: 95.62500 val_loss: 0.35382 val_acc: 91.25000\n",
      "[1, 6890] train_loss: 0.16154 train_acc: 96.25000 val_loss: 0.25831 val_acc: 92.50000\n",
      "[1, 6900] train_loss: 0.20695 train_acc: 93.12500 val_loss: 0.35825 val_acc: 87.50000\n",
      "[1, 6910] train_loss: 0.13635 train_acc: 95.62500 val_loss: 0.47861 val_acc: 85.00000\n",
      "[1, 6920] train_loss: 0.16497 train_acc: 94.37500 val_loss: 0.51238 val_acc: 90.00000\n",
      "[1, 6930] train_loss: 0.16924 train_acc: 95.00000 val_loss: 0.47982 val_acc: 86.25000\n",
      "[1, 6940] train_loss: 0.25020 train_acc: 93.43750 val_loss: 0.59796 val_acc: 87.50000\n",
      "[1, 6950] train_loss: 0.20365 train_acc: 95.00000 val_loss: 0.45761 val_acc: 92.50000\n",
      "[1, 6960] train_loss: 0.21893 train_acc: 94.06250 val_loss: 0.41862 val_acc: 85.00000\n",
      "[1, 6970] train_loss: 0.24670 train_acc: 93.12500 val_loss: 0.57535 val_acc: 86.25000\n",
      "[1, 6980] train_loss: 0.19989 train_acc: 95.31250 val_loss: 0.53468 val_acc: 85.00000\n",
      "[1, 6990] train_loss: 0.15280 train_acc: 94.06250 val_loss: 0.68487 val_acc: 85.00000\n",
      "[1, 7000] train_loss: 0.18884 train_acc: 94.68750 val_loss: 0.33955 val_acc: 92.50000\n",
      "[1, 7010] train_loss: 0.16544 train_acc: 95.31250 val_loss: 0.37487 val_acc: 93.75000\n",
      "[1, 7020] train_loss: 0.20078 train_acc: 94.37500 val_loss: 0.56118 val_acc: 87.50000\n",
      "[1, 7030] train_loss: 0.15654 train_acc: 95.00000 val_loss: 0.31854 val_acc: 92.50000\n",
      "[1, 7040] train_loss: 0.18890 train_acc: 95.93750 val_loss: 0.29048 val_acc: 90.00000\n",
      "[1, 7050] train_loss: 0.22363 train_acc: 94.68750 val_loss: 0.59616 val_acc: 87.50000\n",
      "[1, 7060] train_loss: 0.09456 train_acc: 97.81250 val_loss: 0.66241 val_acc: 86.25000\n",
      "[1, 7070] train_loss: 0.16884 train_acc: 95.62500 val_loss: 0.59061 val_acc: 86.25000\n",
      "[1, 7080] train_loss: 0.15630 train_acc: 96.87500 val_loss: 0.50883 val_acc: 90.00000\n",
      "[1, 7090] train_loss: 0.23722 train_acc: 95.00000 val_loss: 0.66123 val_acc: 87.50000\n",
      "[1, 7100] train_loss: 0.20404 train_acc: 94.06250 val_loss: 0.45467 val_acc: 88.75000\n",
      "[1, 7110] train_loss: 0.15989 train_acc: 94.68750 val_loss: 0.32724 val_acc: 91.25000\n",
      "[1, 7120] train_loss: 0.19736 train_acc: 96.56250 val_loss: 0.40755 val_acc: 88.75000\n",
      "[1, 7130] train_loss: 0.29368 train_acc: 93.43750 val_loss: 0.60890 val_acc: 85.00000\n",
      "[1, 7140] train_loss: 0.18672 train_acc: 94.68750 val_loss: 0.84832 val_acc: 87.50000\n",
      "[1, 7150] train_loss: 0.14459 train_acc: 95.62500 val_loss: 0.69349 val_acc: 81.25000\n",
      "[1, 7160] train_loss: 0.16353 train_acc: 94.37500 val_loss: 0.47193 val_acc: 88.75000\n",
      "[1, 7170] train_loss: 0.13808 train_acc: 95.62500 val_loss: 0.51031 val_acc: 88.75000\n",
      "[1, 7180] train_loss: 0.21455 train_acc: 94.06250 val_loss: 0.68067 val_acc: 85.00000\n",
      "[1, 7190] train_loss: 0.13554 train_acc: 96.25000 val_loss: 0.71040 val_acc: 86.25000\n",
      "[1, 7200] train_loss: 0.13889 train_acc: 95.93750 val_loss: 0.31310 val_acc: 90.00000\n",
      "[1, 7210] train_loss: 0.16984 train_acc: 95.62500 val_loss: 0.62871 val_acc: 86.25000\n",
      "[1, 7220] train_loss: 0.14111 train_acc: 95.93750 val_loss: 0.39963 val_acc: 91.25000\n",
      "[1, 7230] train_loss: 0.10854 train_acc: 95.93750 val_loss: 0.49528 val_acc: 87.50000\n",
      "[1, 7240] train_loss: 0.23304 train_acc: 95.00000 val_loss: 0.46826 val_acc: 83.75000\n",
      "[1, 7250] train_loss: 0.15359 train_acc: 95.31250 val_loss: 0.32149 val_acc: 91.25000\n",
      "[1, 7260] train_loss: 0.15964 train_acc: 95.62500 val_loss: 0.24201 val_acc: 93.75000\n",
      "[1, 7270] train_loss: 0.09492 train_acc: 96.56250 val_loss: 0.53405 val_acc: 85.00000\n",
      "[1, 7280] train_loss: 0.18956 train_acc: 94.68750 val_loss: 0.43552 val_acc: 88.75000\n",
      "[1, 7290] train_loss: 0.14059 train_acc: 96.25000 val_loss: 0.34592 val_acc: 92.50000\n",
      "[1, 7300] train_loss: 0.14659 train_acc: 95.00000 val_loss: 0.51001 val_acc: 86.25000\n",
      "[1, 7310] train_loss: 0.20536 train_acc: 93.75000 val_loss: 0.37877 val_acc: 91.25000\n",
      "[1, 7320] train_loss: 0.14132 train_acc: 96.25000 val_loss: 0.32764 val_acc: 92.50000\n",
      "[1, 7330] train_loss: 0.11940 train_acc: 96.87500 val_loss: 0.59389 val_acc: 86.25000\n",
      "[1, 7340] train_loss: 0.12464 train_acc: 96.25000 val_loss: 0.53584 val_acc: 86.25000\n",
      "[1, 7350] train_loss: 0.15480 train_acc: 95.31250 val_loss: 0.28993 val_acc: 92.50000\n",
      "[1, 7360] train_loss: 0.18793 train_acc: 93.43750 val_loss: 0.78826 val_acc: 90.00000\n",
      "[1, 7370] train_loss: 0.14008 train_acc: 95.93750 val_loss: 0.39607 val_acc: 91.25000\n",
      "[1, 7380] train_loss: 0.09487 train_acc: 97.50000 val_loss: 0.44959 val_acc: 92.50000\n",
      "[1, 7390] train_loss: 0.09927 train_acc: 96.87500 val_loss: 0.65240 val_acc: 87.50000\n",
      "[1, 7400] train_loss: 0.13249 train_acc: 95.93750 val_loss: 0.78096 val_acc: 82.50000\n",
      "[1, 7410] train_loss: 0.09408 train_acc: 96.87500 val_loss: 0.67915 val_acc: 81.25000\n",
      "[1, 7420] train_loss: 0.12273 train_acc: 97.81250 val_loss: 0.68100 val_acc: 85.00000\n",
      "[1, 7430] train_loss: 0.11153 train_acc: 96.87500 val_loss: 0.40119 val_acc: 85.00000\n",
      "[1, 7440] train_loss: 0.12259 train_acc: 96.56250 val_loss: 0.24348 val_acc: 95.00000\n",
      "[1, 7450] train_loss: 0.14817 train_acc: 95.62500 val_loss: 0.34469 val_acc: 91.25000\n",
      "[1, 7460] train_loss: 0.13804 train_acc: 95.62500 val_loss: 0.37927 val_acc: 88.75000\n",
      "[1, 7470] train_loss: 0.20938 train_acc: 94.37500 val_loss: 0.30246 val_acc: 95.00000\n",
      "[1, 7480] train_loss: 0.13707 train_acc: 95.93750 val_loss: 0.24037 val_acc: 93.75000\n",
      "[1, 7490] train_loss: 0.19582 train_acc: 93.75000 val_loss: 0.29903 val_acc: 90.00000\n",
      "[1, 7500] train_loss: 0.18158 train_acc: 94.68750 val_loss: 0.10144 val_acc: 98.75000\n",
      "[1, 7510] train_loss: 0.16326 train_acc: 95.31250 val_loss: 0.29947 val_acc: 91.25000\n",
      "[1, 7520] train_loss: 0.14521 train_acc: 96.25000 val_loss: 0.37761 val_acc: 90.00000\n",
      "[1, 7530] train_loss: 0.12719 train_acc: 95.62500 val_loss: 0.53887 val_acc: 87.50000\n",
      "[1, 7540] train_loss: 0.12900 train_acc: 97.18750 val_loss: 0.67072 val_acc: 86.25000\n",
      "[1, 7550] train_loss: 0.15881 train_acc: 97.18750 val_loss: 0.51760 val_acc: 91.25000\n",
      "[1, 7560] train_loss: 0.22981 train_acc: 95.31250 val_loss: 0.91502 val_acc: 78.75000\n",
      "[1, 7570] train_loss: 0.11715 train_acc: 95.00000 val_loss: 0.44967 val_acc: 91.25000\n",
      "[1, 7580] train_loss: 0.15298 train_acc: 96.25000 val_loss: 0.63554 val_acc: 83.75000\n",
      "[1, 7590] train_loss: 0.24568 train_acc: 94.06250 val_loss: 0.73067 val_acc: 85.00000\n",
      "[1, 7600] train_loss: 0.12723 train_acc: 94.68750 val_loss: 0.91885 val_acc: 83.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7610] train_loss: 0.14794 train_acc: 95.31250 val_loss: 0.53518 val_acc: 88.75000\n",
      "[1, 7620] train_loss: 0.12593 train_acc: 96.25000 val_loss: 0.77935 val_acc: 88.75000\n",
      "[1, 7630] train_loss: 0.11032 train_acc: 96.25000 val_loss: 0.73140 val_acc: 88.75000\n",
      "[1, 7640] train_loss: 0.19832 train_acc: 93.75000 val_loss: 0.41938 val_acc: 90.00000\n",
      "[1, 7650] train_loss: 0.08681 train_acc: 97.81250 val_loss: 0.55785 val_acc: 83.75000\n",
      "[1, 7660] train_loss: 0.16651 train_acc: 94.68750 val_loss: 0.70441 val_acc: 83.75000\n",
      "[1, 7670] train_loss: 0.23161 train_acc: 94.37500 val_loss: 0.60863 val_acc: 87.50000\n",
      "[1, 7680] train_loss: 0.14714 train_acc: 95.00000 val_loss: 0.40922 val_acc: 87.50000\n",
      "[1, 7690] train_loss: 0.15771 train_acc: 95.31250 val_loss: 0.38107 val_acc: 91.25000\n",
      "[1, 7700] train_loss: 0.24213 train_acc: 93.43750 val_loss: 0.41375 val_acc: 88.75000\n",
      "[1, 7710] train_loss: 0.24980 train_acc: 95.62500 val_loss: 0.49070 val_acc: 88.75000\n",
      "[1, 7720] train_loss: 0.19542 train_acc: 94.06250 val_loss: 0.39614 val_acc: 91.25000\n",
      "[1, 7730] train_loss: 0.17308 train_acc: 96.25000 val_loss: 0.20921 val_acc: 93.75000\n",
      "[1, 7740] train_loss: 0.26040 train_acc: 92.50000 val_loss: 0.60727 val_acc: 88.75000\n",
      "[1, 7750] train_loss: 0.25058 train_acc: 93.43750 val_loss: 0.69115 val_acc: 85.00000\n",
      "[1, 7760] train_loss: 0.26151 train_acc: 92.81250 val_loss: 0.41772 val_acc: 91.25000\n",
      "[1, 7770] train_loss: 0.19084 train_acc: 94.68750 val_loss: 0.18410 val_acc: 95.00000\n",
      "[1, 7780] train_loss: 0.22797 train_acc: 94.37500 val_loss: 0.79822 val_acc: 87.50000\n",
      "[1, 7790] train_loss: 0.13598 train_acc: 96.56250 val_loss: 0.83324 val_acc: 85.00000\n",
      "[1, 7800] train_loss: 0.15994 train_acc: 94.68750 val_loss: 0.67103 val_acc: 87.50000\n",
      "[1, 7810] train_loss: 0.14336 train_acc: 95.93750 val_loss: 0.71707 val_acc: 86.25000\n",
      "[1, 7820] train_loss: 0.22334 train_acc: 94.37500 val_loss: 0.68841 val_acc: 87.50000\n",
      "[1, 7830] train_loss: 0.19863 train_acc: 94.06250 val_loss: 0.46030 val_acc: 88.75000\n",
      "[1, 7840] train_loss: 0.18863 train_acc: 94.37500 val_loss: 0.64670 val_acc: 87.50000\n",
      "[1, 7850] train_loss: 0.13218 train_acc: 96.25000 val_loss: 0.55062 val_acc: 88.75000\n",
      "[1, 7860] train_loss: 0.19655 train_acc: 94.37500 val_loss: 0.51433 val_acc: 83.75000\n",
      "[1, 7870] train_loss: 0.06845 train_acc: 97.81250 val_loss: 0.54888 val_acc: 87.50000\n",
      "[1, 7880] train_loss: 0.14478 train_acc: 96.87500 val_loss: 0.49877 val_acc: 88.75000\n",
      "[1, 7890] train_loss: 0.21078 train_acc: 94.06250 val_loss: 0.45940 val_acc: 87.50000\n",
      "[1, 7900] train_loss: 0.07636 train_acc: 98.12500 val_loss: 0.44727 val_acc: 86.25000\n",
      "[1, 7910] train_loss: 0.12199 train_acc: 96.87500 val_loss: 0.56282 val_acc: 90.00000\n",
      "[1, 7920] train_loss: 0.17214 train_acc: 95.00000 val_loss: 0.68980 val_acc: 85.00000\n",
      "[1, 7930] train_loss: 0.11288 train_acc: 96.87500 val_loss: 0.70094 val_acc: 87.50000\n",
      "[1, 7940] train_loss: 0.13137 train_acc: 95.93750 val_loss: 0.87413 val_acc: 87.50000\n",
      "[1, 7950] train_loss: 0.07789 train_acc: 97.18750 val_loss: 0.80279 val_acc: 81.25000\n",
      "[1, 7960] train_loss: 0.11230 train_acc: 96.87500 val_loss: 0.56032 val_acc: 85.00000\n",
      "[1, 7970] train_loss: 0.20712 train_acc: 95.00000 val_loss: 0.29856 val_acc: 92.50000\n",
      "[1, 7980] train_loss: 0.11262 train_acc: 96.25000 val_loss: 0.54743 val_acc: 86.25000\n",
      "[1, 7990] train_loss: 0.16671 train_acc: 95.00000 val_loss: 0.59141 val_acc: 81.25000\n",
      "[1, 8000] train_loss: 0.23506 train_acc: 93.75000 val_loss: 0.74355 val_acc: 90.00000\n",
      "[1, 8010] train_loss: 0.18726 train_acc: 94.68750 val_loss: 0.53426 val_acc: 88.75000\n",
      "[1, 8020] train_loss: 0.13038 train_acc: 96.25000 val_loss: 0.65870 val_acc: 87.50000\n",
      "[1, 8030] train_loss: 0.13649 train_acc: 96.56250 val_loss: 0.47718 val_acc: 87.50000\n",
      "[1, 8040] train_loss: 0.24444 train_acc: 92.81250 val_loss: 0.17792 val_acc: 93.75000\n",
      "[1, 8050] train_loss: 0.22242 train_acc: 95.00000 val_loss: 0.64879 val_acc: 83.75000\n",
      "[1, 8060] train_loss: 0.18310 train_acc: 94.37500 val_loss: 0.41950 val_acc: 92.50000\n",
      "[1, 8070] train_loss: 0.16983 train_acc: 95.62500 val_loss: 0.44104 val_acc: 91.25000\n",
      "[1, 8080] train_loss: 0.26340 train_acc: 94.06250 val_loss: 0.29873 val_acc: 92.50000\n",
      "[1, 8090] train_loss: 0.12811 train_acc: 96.56250 val_loss: 0.59873 val_acc: 90.00000\n",
      "[1, 8100] train_loss: 0.18352 train_acc: 95.62500 val_loss: 0.54916 val_acc: 81.25000\n",
      "[1, 8110] train_loss: 0.16134 train_acc: 95.31250 val_loss: 0.59011 val_acc: 87.50000\n",
      "[1, 8120] train_loss: 0.14866 train_acc: 95.62500 val_loss: 0.34631 val_acc: 91.25000\n",
      "[1, 8130] train_loss: 0.10326 train_acc: 96.87500 val_loss: 0.44139 val_acc: 86.25000\n",
      "[1, 8140] train_loss: 0.06497 train_acc: 98.12500 val_loss: 0.51756 val_acc: 86.25000\n",
      "[1, 8150] train_loss: 0.08951 train_acc: 96.56250 val_loss: 0.53421 val_acc: 86.25000\n",
      "[1, 8160] train_loss: 0.10061 train_acc: 97.81250 val_loss: 0.13033 val_acc: 96.25000\n",
      "[1, 8170] train_loss: 0.09627 train_acc: 97.18750 val_loss: 0.21906 val_acc: 93.75000\n",
      "[1, 8180] train_loss: 0.09695 train_acc: 96.56250 val_loss: 0.41815 val_acc: 87.50000\n",
      "[1, 8190] train_loss: 0.14499 train_acc: 94.68750 val_loss: 0.56390 val_acc: 85.00000\n",
      "[1, 8200] train_loss: 0.15849 train_acc: 95.00000 val_loss: 0.23092 val_acc: 93.75000\n",
      "[1, 8210] train_loss: 0.16125 train_acc: 94.68750 val_loss: 0.52807 val_acc: 88.75000\n",
      "[1, 8220] train_loss: 0.12521 train_acc: 95.62500 val_loss: 0.39494 val_acc: 92.50000\n",
      "[1, 8230] train_loss: 0.31919 train_acc: 92.81250 val_loss: 0.58070 val_acc: 83.75000\n",
      "[1, 8240] train_loss: 0.12228 train_acc: 96.25000 val_loss: 0.46799 val_acc: 87.50000\n",
      "[1, 8250] train_loss: 0.16584 train_acc: 95.62500 val_loss: 0.49999 val_acc: 87.50000\n",
      "[1, 8260] train_loss: 0.18807 train_acc: 93.43750 val_loss: 0.68127 val_acc: 83.75000\n",
      "[1, 8270] train_loss: 0.15345 train_acc: 95.62500 val_loss: 0.78222 val_acc: 83.75000\n",
      "[1, 8280] train_loss: 0.14449 train_acc: 96.87500 val_loss: 0.38822 val_acc: 91.25000\n",
      "[1, 8290] train_loss: 0.13815 train_acc: 95.62500 val_loss: 0.35553 val_acc: 90.00000\n",
      "[1, 8300] train_loss: 0.09858 train_acc: 97.18750 val_loss: 0.58401 val_acc: 88.75000\n",
      "[1, 8310] train_loss: 0.09894 train_acc: 96.56250 val_loss: 0.29611 val_acc: 93.75000\n",
      "[1, 8320] train_loss: 0.11823 train_acc: 95.93750 val_loss: 0.25186 val_acc: 93.75000\n",
      "[1, 8330] train_loss: 0.15859 train_acc: 95.00000 val_loss: 0.64968 val_acc: 87.50000\n",
      "[1, 8340] train_loss: 0.11178 train_acc: 96.56250 val_loss: 0.29086 val_acc: 91.25000\n",
      "[1, 8350] train_loss: 0.17371 train_acc: 94.68750 val_loss: 0.71226 val_acc: 83.75000\n",
      "[1, 8360] train_loss: 0.12841 train_acc: 96.56250 val_loss: 0.29505 val_acc: 92.50000\n",
      "[1, 8370] train_loss: 0.13329 train_acc: 95.62500 val_loss: 0.20980 val_acc: 92.50000\n",
      "[1, 8380] train_loss: 0.11078 train_acc: 96.56250 val_loss: 0.77672 val_acc: 86.25000\n",
      "[1, 8390] train_loss: 0.10353 train_acc: 96.87500 val_loss: 0.45609 val_acc: 90.00000\n",
      "[1, 8400] train_loss: 0.19178 train_acc: 95.31250 val_loss: 0.44113 val_acc: 92.50000\n",
      "[1, 8410] train_loss: 0.11624 train_acc: 96.56250 val_loss: 0.38784 val_acc: 93.75000\n",
      "[1, 8420] train_loss: 0.11221 train_acc: 96.25000 val_loss: 0.56924 val_acc: 88.75000\n",
      "[1, 8430] train_loss: 0.07869 train_acc: 97.18750 val_loss: 0.57606 val_acc: 86.25000\n",
      "[1, 8440] train_loss: 0.16717 train_acc: 94.37500 val_loss: 0.49433 val_acc: 91.25000\n",
      "[1, 8450] train_loss: 0.20493 train_acc: 94.68750 val_loss: 0.26016 val_acc: 92.50000\n",
      "[1, 8460] train_loss: 0.17515 train_acc: 94.37500 val_loss: 0.42426 val_acc: 90.00000\n",
      "[1, 8470] train_loss: 0.10844 train_acc: 97.18750 val_loss: 0.67494 val_acc: 82.50000\n",
      "[1, 8480] train_loss: 0.14910 train_acc: 95.00000 val_loss: 0.43871 val_acc: 87.50000\n",
      "[1, 8490] train_loss: 0.13920 train_acc: 95.31250 val_loss: 0.23422 val_acc: 92.50000\n",
      "[1, 8500] train_loss: 0.19641 train_acc: 95.00000 val_loss: 0.24970 val_acc: 93.75000\n",
      "[1, 8510] train_loss: 0.10734 train_acc: 96.87500 val_loss: 0.58245 val_acc: 86.25000\n",
      "[1, 8520] train_loss: 0.19064 train_acc: 94.68750 val_loss: 0.57191 val_acc: 90.00000\n",
      "[1, 8530] train_loss: 0.22956 train_acc: 93.75000 val_loss: 0.45428 val_acc: 91.25000\n",
      "[1, 8540] train_loss: 0.17509 train_acc: 96.25000 val_loss: 0.37957 val_acc: 92.50000\n",
      "[1, 8550] train_loss: 0.24088 train_acc: 92.81250 val_loss: 0.57400 val_acc: 87.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8560] train_loss: 0.18917 train_acc: 95.31250 val_loss: 0.67793 val_acc: 88.75000\n",
      "[1, 8570] train_loss: 0.13620 train_acc: 97.18750 val_loss: 0.61921 val_acc: 87.50000\n",
      "[1, 8580] train_loss: 0.13261 train_acc: 94.68750 val_loss: 0.58995 val_acc: 90.00000\n",
      "[1, 8590] train_loss: 0.30838 train_acc: 92.81250 val_loss: 0.34005 val_acc: 90.00000\n",
      "[1, 8600] train_loss: 0.17230 train_acc: 95.31250 val_loss: 0.76850 val_acc: 85.00000\n",
      "[1, 8610] train_loss: 0.08973 train_acc: 96.87500 val_loss: 0.46597 val_acc: 90.00000\n",
      "[1, 8620] train_loss: 0.16499 train_acc: 95.00000 val_loss: 0.63478 val_acc: 83.75000\n",
      "[1, 8630] train_loss: 0.09861 train_acc: 96.25000 val_loss: 0.17658 val_acc: 96.25000\n",
      "[1, 8640] train_loss: 0.10733 train_acc: 96.87500 val_loss: 0.55962 val_acc: 85.00000\n",
      "[1, 8650] train_loss: 0.24351 train_acc: 94.06250 val_loss: 0.10359 val_acc: 96.25000\n",
      "[1, 8660] train_loss: 0.25733 train_acc: 91.25000 val_loss: 0.50742 val_acc: 85.00000\n",
      "[1, 8670] train_loss: 0.12080 train_acc: 95.93750 val_loss: 0.55258 val_acc: 86.25000\n",
      "[1, 8680] train_loss: 0.17914 train_acc: 95.00000 val_loss: 0.58669 val_acc: 90.00000\n",
      "[1, 8690] train_loss: 0.17870 train_acc: 94.68750 val_loss: 0.54129 val_acc: 86.25000\n",
      "[1, 8700] train_loss: 0.11106 train_acc: 96.25000 val_loss: 0.56334 val_acc: 87.50000\n",
      "[1, 8710] train_loss: 0.22119 train_acc: 93.75000 val_loss: 0.23963 val_acc: 95.00000\n",
      "[1, 8720] train_loss: 0.16061 train_acc: 95.62500 val_loss: 0.64130 val_acc: 88.75000\n",
      "[1, 8730] train_loss: 0.20814 train_acc: 94.68750 val_loss: 0.62468 val_acc: 85.00000\n",
      "[1, 8740] train_loss: 0.10287 train_acc: 96.56250 val_loss: 0.46551 val_acc: 91.25000\n",
      "[1, 8750] train_loss: 0.17064 train_acc: 95.93750 val_loss: 0.14835 val_acc: 93.21429\n",
      "[1, 8760] train_loss: 0.11254 train_acc: 96.56250 val_loss: 0.92067 val_acc: 86.25000\n",
      "[1, 8770] train_loss: 0.16117 train_acc: 95.62500 val_loss: 0.26450 val_acc: 92.50000\n",
      "[1, 8780] train_loss: 0.22484 train_acc: 94.06250 val_loss: 0.16668 val_acc: 95.00000\n",
      "[1, 8790] train_loss: 0.16364 train_acc: 96.25000 val_loss: 0.53500 val_acc: 87.50000\n",
      "[1, 8800] train_loss: 0.12738 train_acc: 96.25000 val_loss: 0.63300 val_acc: 87.50000\n",
      "[1, 8810] train_loss: 0.12689 train_acc: 96.87500 val_loss: 0.24001 val_acc: 92.50000\n",
      "[1, 8820] train_loss: 0.15649 train_acc: 96.25000 val_loss: 0.37786 val_acc: 92.50000\n",
      "[1, 8830] train_loss: 0.13860 train_acc: 96.25000 val_loss: 0.77498 val_acc: 87.50000\n",
      "[1, 8840] train_loss: 0.13418 train_acc: 96.25000 val_loss: 0.18568 val_acc: 93.75000\n",
      "[1, 8850] train_loss: 0.09086 train_acc: 97.81250 val_loss: 0.29002 val_acc: 91.25000\n",
      "[1, 8860] train_loss: 0.16214 train_acc: 95.62500 val_loss: 0.40479 val_acc: 93.75000\n",
      "[1, 8870] train_loss: 0.08299 train_acc: 96.56250 val_loss: 0.49841 val_acc: 92.50000\n",
      "[1, 8880] train_loss: 0.15356 train_acc: 96.87500 val_loss: 0.36247 val_acc: 90.00000\n",
      "[1, 8890] train_loss: 0.17782 train_acc: 95.00000 val_loss: 0.28282 val_acc: 92.50000\n",
      "[1, 8900] train_loss: 0.14044 train_acc: 96.25000 val_loss: 0.49332 val_acc: 87.50000\n",
      "[1, 8910] train_loss: 0.11557 train_acc: 96.56250 val_loss: 0.22023 val_acc: 91.25000\n",
      "[1, 8920] train_loss: 0.14136 train_acc: 96.56250 val_loss: 0.50401 val_acc: 86.25000\n",
      "[1, 8930] train_loss: 0.19687 train_acc: 94.68750 val_loss: 0.49266 val_acc: 87.50000\n",
      "[1, 8940] train_loss: 0.14422 train_acc: 94.37500 val_loss: 0.59481 val_acc: 87.50000\n",
      "[1, 8950] train_loss: 0.16533 train_acc: 96.25000 val_loss: 0.82501 val_acc: 82.50000\n",
      "[1, 8960] train_loss: 0.14763 train_acc: 95.62500 val_loss: 0.18561 val_acc: 95.00000\n",
      "[1, 8970] train_loss: 0.12535 train_acc: 94.37500 val_loss: 0.68368 val_acc: 85.00000\n",
      "[1, 8980] train_loss: 0.14086 train_acc: 95.93750 val_loss: 0.79496 val_acc: 86.25000\n",
      "[1, 8990] train_loss: 0.12447 train_acc: 95.62500 val_loss: 0.62575 val_acc: 86.25000\n",
      "[1, 9000] train_loss: 0.13797 train_acc: 95.93750 val_loss: 0.69804 val_acc: 86.25000\n",
      "[1, 9010] train_loss: 0.12786 train_acc: 96.25000 val_loss: 0.40911 val_acc: 88.75000\n",
      "[1, 9020] train_loss: 0.16871 train_acc: 95.93750 val_loss: 0.72711 val_acc: 88.75000\n",
      "[1, 9030] train_loss: 0.16259 train_acc: 95.00000 val_loss: 0.28614 val_acc: 91.25000\n",
      "[1, 9040] train_loss: 0.16469 train_acc: 96.25000 val_loss: 0.68404 val_acc: 83.75000\n",
      "[1, 9050] train_loss: 0.13667 train_acc: 95.93750 val_loss: 0.47159 val_acc: 90.00000\n",
      "[1, 9060] train_loss: 0.07920 train_acc: 97.81250 val_loss: 0.63588 val_acc: 87.50000\n",
      "[1, 9070] train_loss: 0.15790 train_acc: 96.25000 val_loss: 0.32595 val_acc: 92.50000\n",
      "[1, 9080] train_loss: 0.12784 train_acc: 95.00000 val_loss: 0.80945 val_acc: 85.00000\n",
      "[1, 9090] train_loss: 0.10525 train_acc: 97.50000 val_loss: 0.47150 val_acc: 92.50000\n",
      "[1, 9100] train_loss: 0.13554 train_acc: 95.62500 val_loss: 0.18371 val_acc: 95.00000\n",
      "[1, 9110] train_loss: 0.10202 train_acc: 96.87500 val_loss: 0.55459 val_acc: 88.75000\n",
      "[1, 9120] train_loss: 0.17116 train_acc: 95.93750 val_loss: 0.72282 val_acc: 81.25000\n",
      "[1, 9130] train_loss: 0.17358 train_acc: 95.62500 val_loss: 0.68548 val_acc: 86.25000\n",
      "[1, 9140] train_loss: 0.16006 train_acc: 95.93750 val_loss: 0.47304 val_acc: 88.75000\n",
      "[1, 9150] train_loss: 0.15148 train_acc: 95.62500 val_loss: 0.26264 val_acc: 93.75000\n",
      "[1, 9160] train_loss: 0.19663 train_acc: 95.31250 val_loss: 0.15095 val_acc: 93.75000\n",
      "[1, 9170] train_loss: 0.20550 train_acc: 94.68750 val_loss: 0.61626 val_acc: 87.50000\n",
      "[1, 9180] train_loss: 0.15562 train_acc: 95.00000 val_loss: 0.52325 val_acc: 87.50000\n",
      "[1, 9190] train_loss: 0.15871 train_acc: 93.75000 val_loss: 0.29825 val_acc: 92.50000\n",
      "[1, 9200] train_loss: 0.12097 train_acc: 95.93750 val_loss: 0.43301 val_acc: 92.50000\n",
      "[1, 9210] train_loss: 0.10492 train_acc: 96.56250 val_loss: 0.42973 val_acc: 92.50000\n",
      "[1, 9220] train_loss: 0.17531 train_acc: 95.00000 val_loss: 0.30174 val_acc: 90.00000\n",
      "[1, 9230] train_loss: 0.11170 train_acc: 97.50000 val_loss: 0.28223 val_acc: 91.25000\n",
      "[1, 9240] train_loss: 0.10667 train_acc: 96.56250 val_loss: 0.52074 val_acc: 88.75000\n",
      "[1, 9250] train_loss: 0.14237 train_acc: 97.18750 val_loss: 0.49701 val_acc: 90.00000\n",
      "[1, 9260] train_loss: 0.11718 train_acc: 96.25000 val_loss: 0.41898 val_acc: 86.25000\n",
      "[1, 9270] train_loss: 0.07389 train_acc: 98.12500 val_loss: 0.47690 val_acc: 88.75000\n",
      "[1, 9280] train_loss: 0.17889 train_acc: 94.37500 val_loss: 0.53391 val_acc: 91.25000\n",
      "[1, 9290] train_loss: 0.12216 train_acc: 95.93750 val_loss: 0.43267 val_acc: 93.75000\n",
      "[1, 9300] train_loss: 0.07838 train_acc: 96.56250 val_loss: 0.87040 val_acc: 86.25000\n",
      "[1, 9310] train_loss: 0.08371 train_acc: 96.25000 val_loss: 0.86180 val_acc: 82.50000\n",
      "[1, 9320] train_loss: 0.13715 train_acc: 94.37500 val_loss: 0.53849 val_acc: 88.75000\n",
      "[1, 9330] train_loss: 0.12908 train_acc: 95.62500 val_loss: 0.29894 val_acc: 93.75000\n",
      "[1, 9340] train_loss: 0.12326 train_acc: 95.93750 val_loss: 0.28643 val_acc: 93.75000\n",
      "[1, 9350] train_loss: 0.12076 train_acc: 96.25000 val_loss: 0.60534 val_acc: 86.25000\n",
      "[1, 9360] train_loss: 0.12701 train_acc: 96.56250 val_loss: 0.47908 val_acc: 91.25000\n",
      "[1, 9370] train_loss: 0.06460 train_acc: 98.12500 val_loss: 0.60147 val_acc: 88.75000\n",
      "[1, 9380] train_loss: 0.20617 train_acc: 95.62500 val_loss: 0.50873 val_acc: 91.25000\n",
      "[1, 9390] train_loss: 0.20085 train_acc: 94.37500 val_loss: 0.36431 val_acc: 91.25000\n",
      "[1, 9400] train_loss: 0.13261 train_acc: 95.62500 val_loss: 0.71002 val_acc: 87.50000\n",
      "[1, 9410] train_loss: 0.13480 train_acc: 96.87500 val_loss: 0.36090 val_acc: 92.50000\n",
      "[1, 9420] train_loss: 0.12750 train_acc: 96.87500 val_loss: 0.78432 val_acc: 81.25000\n",
      "[1, 9430] train_loss: 0.13736 train_acc: 94.68750 val_loss: 0.72245 val_acc: 82.50000\n",
      "[1, 9440] train_loss: 0.09812 train_acc: 97.50000 val_loss: 0.56385 val_acc: 86.25000\n",
      "[1, 9450] train_loss: 0.10766 train_acc: 97.18750 val_loss: 0.43290 val_acc: 92.50000\n",
      "[1, 9460] train_loss: 0.12307 train_acc: 96.87500 val_loss: 0.67178 val_acc: 85.00000\n",
      "[1, 9470] train_loss: 0.10040 train_acc: 96.87500 val_loss: 0.28814 val_acc: 93.75000\n",
      "[1, 9480] train_loss: 0.07520 train_acc: 98.12500 val_loss: 0.46966 val_acc: 87.50000\n",
      "[1, 9490] train_loss: 0.08214 train_acc: 97.18750 val_loss: 0.50264 val_acc: 88.75000\n",
      "[1, 9500] train_loss: 0.11173 train_acc: 97.50000 val_loss: 0.68899 val_acc: 83.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9510] train_loss: 0.10126 train_acc: 96.56250 val_loss: 0.48139 val_acc: 91.25000\n",
      "[1, 9520] train_loss: 0.10380 train_acc: 95.62500 val_loss: 0.45743 val_acc: 90.00000\n",
      "[1, 9530] train_loss: 0.09528 train_acc: 98.43750 val_loss: 0.50405 val_acc: 86.25000\n",
      "[1, 9540] train_loss: 0.18908 train_acc: 94.68750 val_loss: 0.44927 val_acc: 85.00000\n",
      "[1, 9550] train_loss: 0.10208 train_acc: 96.87500 val_loss: 0.44168 val_acc: 88.75000\n",
      "[1, 9560] train_loss: 0.09186 train_acc: 96.87500 val_loss: 0.34060 val_acc: 92.50000\n",
      "[1, 9570] train_loss: 0.12451 train_acc: 95.93750 val_loss: 0.41367 val_acc: 88.75000\n",
      "[1, 9580] train_loss: 0.06757 train_acc: 97.50000 val_loss: 0.46560 val_acc: 91.25000\n",
      "[1, 9590] train_loss: 0.11071 train_acc: 97.18750 val_loss: 0.63186 val_acc: 88.75000\n",
      "[1, 9600] train_loss: 0.18198 train_acc: 94.68750 val_loss: 0.89959 val_acc: 82.50000\n",
      "[1, 9610] train_loss: 0.10787 train_acc: 96.87500 val_loss: 0.46808 val_acc: 88.75000\n",
      "[1, 9620] train_loss: 0.10282 train_acc: 96.87500 val_loss: 0.37520 val_acc: 87.50000\n",
      "[1, 9630] train_loss: 0.21913 train_acc: 94.68750 val_loss: 0.72622 val_acc: 91.25000\n",
      "[1, 9640] train_loss: 0.10289 train_acc: 96.87500 val_loss: 0.31302 val_acc: 95.00000\n",
      "[1, 9650] train_loss: 0.10991 train_acc: 95.93750 val_loss: 0.42326 val_acc: 90.00000\n",
      "[1, 9660] train_loss: 0.14310 train_acc: 95.93750 val_loss: 0.50136 val_acc: 87.50000\n",
      "[1, 9670] train_loss: 0.12263 train_acc: 96.87500 val_loss: 0.61308 val_acc: 86.25000\n",
      "[1, 9680] train_loss: 0.11821 train_acc: 96.87500 val_loss: 0.35414 val_acc: 92.50000\n",
      "[1, 9690] train_loss: 0.13110 train_acc: 95.62500 val_loss: 0.54737 val_acc: 90.00000\n",
      "[1, 9700] train_loss: 0.14798 train_acc: 96.25000 val_loss: 0.44883 val_acc: 88.75000\n",
      "[1, 9710] train_loss: 0.10608 train_acc: 95.62500 val_loss: 0.47528 val_acc: 92.50000\n",
      "[1, 9720] train_loss: 0.15929 train_acc: 95.31250 val_loss: 0.72673 val_acc: 90.00000\n",
      "[1, 9730] train_loss: 0.11854 train_acc: 95.62500 val_loss: 0.57263 val_acc: 91.25000\n",
      "[1, 9740] train_loss: 0.09700 train_acc: 96.87500 val_loss: 0.63732 val_acc: 88.75000\n",
      "[1, 9750] train_loss: 0.09567 train_acc: 97.50000 val_loss: 0.48999 val_acc: 88.75000\n",
      "[1, 9760] train_loss: 0.07736 train_acc: 97.81250 val_loss: 0.30903 val_acc: 92.50000\n",
      "[1, 9770] train_loss: 0.09821 train_acc: 97.18750 val_loss: 0.30448 val_acc: 88.75000\n",
      "[1, 9780] train_loss: 0.11790 train_acc: 96.25000 val_loss: 0.41624 val_acc: 88.75000\n",
      "[1, 9790] train_loss: 0.08232 train_acc: 97.50000 val_loss: 0.73414 val_acc: 86.25000\n",
      "[1, 9800] train_loss: 0.06331 train_acc: 98.43750 val_loss: 0.51515 val_acc: 87.50000\n",
      "[1, 9810] train_loss: 0.10550 train_acc: 97.18750 val_loss: 0.38496 val_acc: 92.50000\n",
      "[1, 9820] train_loss: 0.12448 train_acc: 96.87500 val_loss: 0.86112 val_acc: 86.25000\n",
      "[1, 9830] train_loss: 0.13012 train_acc: 96.87500 val_loss: 0.80107 val_acc: 88.75000\n",
      "[1, 9840] train_loss: 0.11926 train_acc: 96.56250 val_loss: 0.66980 val_acc: 83.75000\n",
      "[1, 9850] train_loss: 0.18200 train_acc: 95.93750 val_loss: 0.65757 val_acc: 88.75000\n",
      "[1, 9860] train_loss: 0.11371 train_acc: 97.81250 val_loss: 0.48543 val_acc: 90.00000\n",
      "[1, 9870] train_loss: 0.11258 train_acc: 96.25000 val_loss: 0.51577 val_acc: 88.75000\n",
      "[1, 9880] train_loss: 0.09143 train_acc: 97.18750 val_loss: 0.33357 val_acc: 90.00000\n",
      "[1, 9890] train_loss: 0.08851 train_acc: 97.50000 val_loss: 0.82510 val_acc: 86.25000\n",
      "[1, 9900] train_loss: 0.12603 train_acc: 95.93750 val_loss: 0.53810 val_acc: 88.75000\n",
      "[1, 9910] train_loss: 0.07766 train_acc: 98.75000 val_loss: 0.46632 val_acc: 88.75000\n",
      "[1, 9920] train_loss: 0.07628 train_acc: 98.43750 val_loss: 0.38261 val_acc: 90.00000\n",
      "[1, 9930] train_loss: 0.08518 train_acc: 97.18750 val_loss: 0.38129 val_acc: 88.75000\n",
      "[1, 9940] train_loss: 0.14982 train_acc: 96.25000 val_loss: 0.33196 val_acc: 88.75000\n",
      "[1, 9950] train_loss: 0.16893 train_acc: 95.93750 val_loss: 0.56737 val_acc: 90.00000\n",
      "[1, 9960] train_loss: 0.14954 train_acc: 95.00000 val_loss: 0.57768 val_acc: 85.00000\n",
      "[1, 0] train_loss: 0.04348 train_acc: 100.00000 val_loss: 0.01605 val_acc: 100.00000\n",
      "[1, 10] train_loss: 0.13297 train_acc: 95.93750 val_loss: 0.69378 val_acc: 87.50000\n",
      "[1, 20] train_loss: 0.08801 train_acc: 97.18750 val_loss: 0.41575 val_acc: 91.25000\n",
      "[1, 30] train_loss: 0.10573 train_acc: 96.87500 val_loss: 0.42809 val_acc: 87.50000\n",
      "[1, 40] train_loss: 0.06980 train_acc: 97.18750 val_loss: 0.64216 val_acc: 87.50000\n",
      "[1, 50] train_loss: 0.10774 train_acc: 96.25000 val_loss: 0.43204 val_acc: 91.25000\n",
      "[1, 60] train_loss: 0.06296 train_acc: 98.43750 val_loss: 0.50470 val_acc: 90.00000\n",
      "[1, 70] train_loss: 0.14096 train_acc: 95.31250 val_loss: 0.33504 val_acc: 90.00000\n",
      "[1, 80] train_loss: 0.06164 train_acc: 97.81250 val_loss: 0.60198 val_acc: 92.50000\n",
      "[1, 90] train_loss: 0.14212 train_acc: 96.25000 val_loss: 0.83989 val_acc: 91.25000\n",
      "[1, 100] train_loss: 0.11229 train_acc: 97.81250 val_loss: 0.40476 val_acc: 87.50000\n",
      "[1, 110] train_loss: 0.13279 train_acc: 96.25000 val_loss: 0.65034 val_acc: 87.50000\n",
      "[1, 120] train_loss: 0.10252 train_acc: 97.18750 val_loss: 0.64109 val_acc: 87.50000\n",
      "[1, 130] train_loss: 0.11187 train_acc: 96.25000 val_loss: 0.63660 val_acc: 86.25000\n",
      "[1, 140] train_loss: 0.09547 train_acc: 96.56250 val_loss: 0.80708 val_acc: 80.00000\n",
      "[1, 150] train_loss: 0.04914 train_acc: 98.12500 val_loss: 0.24032 val_acc: 90.00000\n",
      "[1, 160] train_loss: 0.06222 train_acc: 96.87500 val_loss: 0.85299 val_acc: 86.25000\n",
      "[1, 170] train_loss: 0.06785 train_acc: 97.81250 val_loss: 0.57614 val_acc: 92.50000\n",
      "[1, 180] train_loss: 0.11087 train_acc: 96.87500 val_loss: 0.76552 val_acc: 90.00000\n",
      "[1, 190] train_loss: 0.11085 train_acc: 97.50000 val_loss: 0.85421 val_acc: 82.50000\n",
      "[1, 200] train_loss: 0.06025 train_acc: 98.43750 val_loss: 0.47999 val_acc: 91.25000\n",
      "[1, 210] train_loss: 0.07651 train_acc: 97.81250 val_loss: 0.34452 val_acc: 92.50000\n",
      "[1, 220] train_loss: 0.09943 train_acc: 97.18750 val_loss: 0.22044 val_acc: 91.25000\n",
      "[1, 230] train_loss: 0.11188 train_acc: 97.50000 val_loss: 0.79745 val_acc: 90.00000\n",
      "[1, 240] train_loss: 0.09239 train_acc: 96.56250 val_loss: 0.52649 val_acc: 92.50000\n",
      "[1, 250] train_loss: 0.04421 train_acc: 98.12500 val_loss: 0.89045 val_acc: 86.25000\n",
      "[1, 260] train_loss: 0.05243 train_acc: 98.12500 val_loss: 0.22255 val_acc: 93.75000\n",
      "[1, 270] train_loss: 0.07608 train_acc: 97.18750 val_loss: 0.39525 val_acc: 91.25000\n",
      "[1, 280] train_loss: 0.05310 train_acc: 99.37500 val_loss: 0.35551 val_acc: 88.75000\n",
      "[1, 290] train_loss: 0.06386 train_acc: 98.43750 val_loss: 0.80223 val_acc: 85.00000\n",
      "[1, 300] train_loss: 0.05227 train_acc: 97.81250 val_loss: 0.41008 val_acc: 88.75000\n",
      "[1, 310] train_loss: 0.09891 train_acc: 97.18750 val_loss: 0.29782 val_acc: 92.50000\n",
      "[1, 320] train_loss: 0.06739 train_acc: 97.50000 val_loss: 0.85080 val_acc: 83.75000\n",
      "[1, 330] train_loss: 0.05312 train_acc: 98.12500 val_loss: 0.70099 val_acc: 91.25000\n",
      "[1, 340] train_loss: 0.15722 train_acc: 96.25000 val_loss: 0.38082 val_acc: 91.25000\n",
      "[1, 350] train_loss: 0.16326 train_acc: 95.31250 val_loss: 0.22852 val_acc: 93.75000\n",
      "[1, 360] train_loss: 0.14446 train_acc: 96.25000 val_loss: 0.82567 val_acc: 82.50000\n",
      "[1, 370] train_loss: 0.09283 train_acc: 96.25000 val_loss: 0.25873 val_acc: 91.25000\n",
      "[1, 380] train_loss: 0.16972 train_acc: 96.56250 val_loss: 0.45509 val_acc: 88.75000\n",
      "[1, 390] train_loss: 0.11505 train_acc: 98.43750 val_loss: 0.44225 val_acc: 91.25000\n",
      "[1, 400] train_loss: 0.05690 train_acc: 98.43750 val_loss: 0.38354 val_acc: 86.25000\n",
      "[1, 410] train_loss: 0.11727 train_acc: 96.56250 val_loss: 0.50570 val_acc: 90.00000\n",
      "[1, 420] train_loss: 0.02281 train_acc: 99.37500 val_loss: 0.65199 val_acc: 88.75000\n",
      "[1, 430] train_loss: 0.09795 train_acc: 97.18750 val_loss: 0.51887 val_acc: 86.25000\n",
      "[1, 440] train_loss: 0.14071 train_acc: 95.93750 val_loss: 0.41438 val_acc: 92.50000\n",
      "[1, 450] train_loss: 0.12755 train_acc: 97.18750 val_loss: 0.36509 val_acc: 92.50000\n",
      "[1, 460] train_loss: 0.04550 train_acc: 98.75000 val_loss: 0.21203 val_acc: 93.75000\n",
      "[1, 470] train_loss: 0.11865 train_acc: 97.81250 val_loss: 0.53884 val_acc: 91.25000\n",
      "[1, 480] train_loss: 0.06121 train_acc: 98.12500 val_loss: 0.57548 val_acc: 88.75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 490] train_loss: 0.06451 train_acc: 98.43750 val_loss: 0.19694 val_acc: 96.25000\n",
      "[1, 500] train_loss: 0.06106 train_acc: 97.50000 val_loss: 0.34096 val_acc: 92.50000\n",
      "[1, 510] train_loss: 0.09551 train_acc: 97.50000 val_loss: 0.41712 val_acc: 92.50000\n",
      "[1, 520] train_loss: 0.06062 train_acc: 98.12500 val_loss: 0.29507 val_acc: 95.00000\n",
      "[1, 530] train_loss: 0.07101 train_acc: 97.81250 val_loss: 0.23710 val_acc: 93.75000\n",
      "[1, 540] train_loss: 0.06598 train_acc: 97.18750 val_loss: 0.50024 val_acc: 88.75000\n",
      "[1, 550] train_loss: 0.06720 train_acc: 98.75000 val_loss: 0.52064 val_acc: 90.00000\n",
      "[1, 560] train_loss: 0.09073 train_acc: 97.50000 val_loss: 0.74903 val_acc: 88.75000\n",
      "[1, 570] train_loss: 0.06103 train_acc: 98.43750 val_loss: 0.40770 val_acc: 91.25000\n",
      "[1, 580] train_loss: 0.07600 train_acc: 97.50000 val_loss: 0.29608 val_acc: 93.75000\n",
      "[1, 590] train_loss: 0.03570 train_acc: 98.43750 val_loss: 0.69423 val_acc: 85.00000\n",
      "[1, 600] train_loss: 0.07749 train_acc: 97.18750 val_loss: 0.59945 val_acc: 86.25000\n",
      "[1, 610] train_loss: 0.10826 train_acc: 96.25000 val_loss: 0.37858 val_acc: 92.50000\n",
      "[1, 620] train_loss: 0.10006 train_acc: 96.87500 val_loss: 0.34018 val_acc: 93.75000\n",
      "[1, 630] train_loss: 0.07462 train_acc: 96.56250 val_loss: 0.73424 val_acc: 85.00000\n",
      "[1, 640] train_loss: 0.13227 train_acc: 97.18750 val_loss: 0.26036 val_acc: 90.00000\n",
      "[1, 650] train_loss: 0.07004 train_acc: 97.50000 val_loss: 0.39745 val_acc: 90.00000\n",
      "[1, 660] train_loss: 0.09599 train_acc: 97.18750 val_loss: 0.24812 val_acc: 95.00000\n",
      "[1, 670] train_loss: 0.13904 train_acc: 95.93750 val_loss: 0.31644 val_acc: 92.50000\n",
      "[1, 680] train_loss: 0.12236 train_acc: 97.18750 val_loss: 0.56722 val_acc: 87.50000\n",
      "[1, 690] train_loss: 0.08228 train_acc: 97.50000 val_loss: 0.45124 val_acc: 90.00000\n",
      "[1, 700] train_loss: 0.10411 train_acc: 96.87500 val_loss: 0.22477 val_acc: 91.25000\n",
      "[1, 710] train_loss: 0.09456 train_acc: 96.56250 val_loss: 0.46787 val_acc: 87.50000\n",
      "[1, 720] train_loss: 0.09985 train_acc: 95.93750 val_loss: 0.98830 val_acc: 81.25000\n",
      "[1, 730] train_loss: 0.10213 train_acc: 96.56250 val_loss: 0.37867 val_acc: 93.75000\n",
      "[1, 740] train_loss: 0.08461 train_acc: 96.56250 val_loss: 0.32656 val_acc: 95.00000\n",
      "[1, 750] train_loss: 0.13773 train_acc: 95.62500 val_loss: 0.27585 val_acc: 88.75000\n",
      "[1, 760] train_loss: 0.10988 train_acc: 97.50000 val_loss: 0.63175 val_acc: 83.75000\n",
      "[1, 770] train_loss: 0.06399 train_acc: 98.43750 val_loss: 0.39488 val_acc: 93.75000\n",
      "[1, 780] train_loss: 0.11155 train_acc: 95.93750 val_loss: 0.84444 val_acc: 82.50000\n",
      "[1, 790] train_loss: 0.15710 train_acc: 95.31250 val_loss: 0.32696 val_acc: 91.25000\n",
      "[1, 800] train_loss: 0.10318 train_acc: 96.87500 val_loss: 0.35340 val_acc: 91.25000\n",
      "[1, 810] train_loss: 0.13962 train_acc: 94.37500 val_loss: 0.46352 val_acc: 87.50000\n",
      "[1, 820] train_loss: 0.08896 train_acc: 97.50000 val_loss: 0.48300 val_acc: 90.00000\n",
      "[1, 830] train_loss: 0.11052 train_acc: 97.18750 val_loss: 0.98709 val_acc: 83.75000\n",
      "[1, 840] train_loss: 0.12451 train_acc: 95.93750 val_loss: 1.10207 val_acc: 83.75000\n",
      "[1, 850] train_loss: 0.11243 train_acc: 97.50000 val_loss: 0.56000 val_acc: 87.50000\n",
      "[1, 860] train_loss: 0.05703 train_acc: 98.12500 val_loss: 0.50264 val_acc: 88.75000\n",
      "[1, 870] train_loss: 0.11142 train_acc: 97.18750 val_loss: 0.45202 val_acc: 91.25000\n",
      "[1, 880] train_loss: 0.30014 train_acc: 95.31250 val_loss: 0.45518 val_acc: 90.00000\n",
      "[1, 890] train_loss: 0.18732 train_acc: 96.56250 val_loss: 0.51926 val_acc: 88.75000\n",
      "[1, 900] train_loss: 0.11744 train_acc: 97.50000 val_loss: 0.98158 val_acc: 86.25000\n",
      "[1, 910] train_loss: 0.07410 train_acc: 96.87500 val_loss: 0.71809 val_acc: 85.00000\n",
      "[1, 920] train_loss: 0.10233 train_acc: 96.87500 val_loss: 0.59618 val_acc: 91.25000\n",
      "[1, 930] train_loss: 0.04973 train_acc: 98.75000 val_loss: 0.55596 val_acc: 85.00000\n",
      "[1, 940] train_loss: 0.05734 train_acc: 98.43750 val_loss: 0.47508 val_acc: 90.00000\n",
      "[1, 950] train_loss: 0.06412 train_acc: 97.18750 val_loss: 0.56731 val_acc: 91.25000\n",
      "[1, 960] train_loss: 0.07025 train_acc: 97.50000 val_loss: 0.53227 val_acc: 88.75000\n",
      "[1, 970] train_loss: 0.10744 train_acc: 95.93750 val_loss: 0.40525 val_acc: 88.75000\n",
      "[1, 980] train_loss: 0.14767 train_acc: 96.56250 val_loss: 0.44736 val_acc: 85.00000\n",
      "[1, 990] train_loss: 0.07370 train_acc: 98.43750 val_loss: 0.72031 val_acc: 88.75000\n",
      "[1, 1000] train_loss: 0.13517 train_acc: 96.25000 val_loss: 0.29224 val_acc: 92.50000\n",
      "[1, 1010] train_loss: 0.06692 train_acc: 97.18750 val_loss: 0.65072 val_acc: 88.75000\n",
      "[1, 1020] train_loss: 0.14446 train_acc: 95.31250 val_loss: 0.29593 val_acc: 93.75000\n",
      "[1, 1030] train_loss: 0.03621 train_acc: 99.37500 val_loss: 0.69799 val_acc: 82.50000\n",
      "[1, 1040] train_loss: 0.08491 train_acc: 97.50000 val_loss: 0.25031 val_acc: 93.75000\n",
      "[1, 1050] train_loss: 0.09580 train_acc: 97.18750 val_loss: 0.52483 val_acc: 86.25000\n",
      "[1, 1060] train_loss: 0.07236 train_acc: 97.81250 val_loss: 0.63579 val_acc: 85.00000\n",
      "[1, 1070] train_loss: 0.08649 train_acc: 98.43750 val_loss: 1.27596 val_acc: 78.75000\n",
      "[1, 1080] train_loss: 0.11652 train_acc: 96.87500 val_loss: 0.55271 val_acc: 90.00000\n",
      "[1, 1090] train_loss: 0.09759 train_acc: 96.56250 val_loss: 0.28867 val_acc: 92.50000\n",
      "[1, 1100] train_loss: 0.07693 train_acc: 97.50000 val_loss: 0.52321 val_acc: 87.50000\n",
      "[1, 1110] train_loss: 0.19841 train_acc: 95.31250 val_loss: 0.47948 val_acc: 88.75000\n",
      "[1, 1120] train_loss: 0.09012 train_acc: 95.93750 val_loss: 0.39804 val_acc: 92.50000\n",
      "[1, 1130] train_loss: 0.10214 train_acc: 97.18750 val_loss: 0.84439 val_acc: 81.25000\n",
      "[1, 1140] train_loss: 0.12823 train_acc: 95.93750 val_loss: 0.27592 val_acc: 92.50000\n",
      "[1, 1150] train_loss: 0.09277 train_acc: 98.12500 val_loss: 0.50094 val_acc: 82.50000\n",
      "[1, 1160] train_loss: 0.10366 train_acc: 96.87500 val_loss: 0.51021 val_acc: 90.00000\n",
      "[1, 1170] train_loss: 0.08608 train_acc: 97.50000 val_loss: 0.75697 val_acc: 85.00000\n",
      "[1, 1180] train_loss: 0.06906 train_acc: 97.50000 val_loss: 0.34190 val_acc: 91.25000\n",
      "[1, 1190] train_loss: 0.06460 train_acc: 97.50000 val_loss: 0.17436 val_acc: 95.00000\n",
      "[1, 1200] train_loss: 0.10549 train_acc: 97.18750 val_loss: 1.17019 val_acc: 82.50000\n",
      "[1, 1210] train_loss: 0.16757 train_acc: 95.00000 val_loss: 0.48759 val_acc: 88.75000\n",
      "[1, 1220] train_loss: 0.12131 train_acc: 95.62500 val_loss: 0.57324 val_acc: 85.00000\n",
      "[1, 1230] train_loss: 0.14978 train_acc: 95.62500 val_loss: 0.66963 val_acc: 90.00000\n",
      "[1, 1240] train_loss: 0.05388 train_acc: 98.43750 val_loss: 0.25889 val_acc: 92.50000\n",
      "[1, 1250] train_loss: 0.06785 train_acc: 97.50000 val_loss: 0.88459 val_acc: 85.00000\n",
      "[1, 1260] train_loss: 0.11306 train_acc: 97.18750 val_loss: 0.40670 val_acc: 90.00000\n",
      "[1, 1270] train_loss: 0.06927 train_acc: 97.81250 val_loss: 0.74477 val_acc: 85.00000\n",
      "[1, 1280] train_loss: 0.15161 train_acc: 95.62500 val_loss: 0.84160 val_acc: 88.75000\n",
      "[1, 1290] train_loss: 0.10394 train_acc: 96.87500 val_loss: 0.66618 val_acc: 87.50000\n",
      "[1, 1300] train_loss: 0.09029 train_acc: 97.18750 val_loss: 0.24960 val_acc: 93.75000\n",
      "[1, 1310] train_loss: 0.05750 train_acc: 98.12500 val_loss: 0.41706 val_acc: 93.75000\n",
      "[1, 1320] train_loss: 0.10825 train_acc: 96.25000 val_loss: 0.51464 val_acc: 87.50000\n",
      "[1, 1330] train_loss: 0.10310 train_acc: 96.87500 val_loss: 0.39261 val_acc: 92.50000\n",
      "[1, 1340] train_loss: 0.08307 train_acc: 96.87500 val_loss: 0.78063 val_acc: 85.00000\n",
      "[1, 1350] train_loss: 0.15044 train_acc: 96.87500 val_loss: 0.32326 val_acc: 90.00000\n",
      "[1, 1360] train_loss: 0.13797 train_acc: 96.25000 val_loss: 0.52781 val_acc: 88.75000\n",
      "[1, 1370] train_loss: 0.17782 train_acc: 95.62500 val_loss: 0.67155 val_acc: 87.50000\n",
      "[1, 1380] train_loss: 0.09449 train_acc: 96.87500 val_loss: 0.40282 val_acc: 92.50000\n",
      "[1, 1390] train_loss: 0.08730 train_acc: 96.56250 val_loss: 0.29552 val_acc: 91.25000\n",
      "[1, 1400] train_loss: 0.03306 train_acc: 98.75000 val_loss: 0.32891 val_acc: 92.50000\n",
      "[1, 1410] train_loss: 0.10688 train_acc: 97.18750 val_loss: 0.41059 val_acc: 93.75000\n",
      "[1, 1420] train_loss: 0.09249 train_acc: 96.25000 val_loss: 0.73025 val_acc: 87.50000\n",
      "[1, 1430] train_loss: 0.06327 train_acc: 97.81250 val_loss: 0.49205 val_acc: 87.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1440] train_loss: 0.06986 train_acc: 98.12500 val_loss: 0.48398 val_acc: 92.50000\n",
      "[1, 1450] train_loss: 0.05220 train_acc: 98.75000 val_loss: 0.61115 val_acc: 85.00000\n",
      "[1, 1460] train_loss: 0.13074 train_acc: 96.25000 val_loss: 0.43755 val_acc: 92.50000\n",
      "[1, 1470] train_loss: 0.08117 train_acc: 98.43750 val_loss: 0.56764 val_acc: 88.75000\n",
      "[1, 1480] train_loss: 0.10902 train_acc: 97.18750 val_loss: 0.32648 val_acc: 93.75000\n",
      "[1, 1490] train_loss: 0.09345 train_acc: 97.18750 val_loss: 0.54614 val_acc: 90.00000\n",
      "[1, 1500] train_loss: 0.06955 train_acc: 98.43750 val_loss: 0.51331 val_acc: 87.50000\n",
      "[1, 1510] train_loss: 0.13687 train_acc: 97.18750 val_loss: 0.58733 val_acc: 91.25000\n",
      "[1, 1520] train_loss: 0.09270 train_acc: 97.18750 val_loss: 0.51034 val_acc: 88.75000\n",
      "[1, 1530] train_loss: 0.03997 train_acc: 99.06250 val_loss: 0.33792 val_acc: 90.00000\n",
      "[1, 1540] train_loss: 0.10609 train_acc: 97.18750 val_loss: 0.88755 val_acc: 88.75000\n",
      "[1, 1550] train_loss: 0.08065 train_acc: 97.50000 val_loss: 0.54050 val_acc: 86.25000\n",
      "[1, 1560] train_loss: 0.17377 train_acc: 95.62500 val_loss: 0.38731 val_acc: 92.50000\n",
      "[1, 1570] train_loss: 0.18839 train_acc: 95.62500 val_loss: 0.23800 val_acc: 93.75000\n",
      "[1, 1580] train_loss: 0.09958 train_acc: 97.18750 val_loss: 0.61059 val_acc: 88.75000\n",
      "[1, 1590] train_loss: 0.09612 train_acc: 96.25000 val_loss: 0.27094 val_acc: 93.75000\n",
      "[1, 1600] train_loss: 0.15155 train_acc: 95.62500 val_loss: 0.77627 val_acc: 88.75000\n",
      "[1, 1610] train_loss: 0.08356 train_acc: 96.87500 val_loss: 0.73236 val_acc: 86.25000\n",
      "[1, 1620] train_loss: 0.16957 train_acc: 95.62500 val_loss: 0.56055 val_acc: 87.50000\n",
      "[1, 1630] train_loss: 0.13802 train_acc: 96.25000 val_loss: 0.65598 val_acc: 87.50000\n",
      "[1, 1640] train_loss: 0.13450 train_acc: 95.00000 val_loss: 0.20911 val_acc: 93.75000\n",
      "[1, 1650] train_loss: 0.10704 train_acc: 95.93750 val_loss: 0.64064 val_acc: 86.25000\n",
      "[1, 1660] train_loss: 0.07810 train_acc: 96.56250 val_loss: 0.49333 val_acc: 87.50000\n",
      "[1, 1670] train_loss: 0.11178 train_acc: 96.25000 val_loss: 0.53972 val_acc: 91.25000\n",
      "[1, 1680] train_loss: 0.07248 train_acc: 97.50000 val_loss: 0.26428 val_acc: 93.75000\n",
      "[1, 1690] train_loss: 0.05703 train_acc: 97.18750 val_loss: 0.50078 val_acc: 91.25000\n",
      "[1, 1700] train_loss: 0.07398 train_acc: 98.12500 val_loss: 0.40322 val_acc: 93.75000\n",
      "[1, 1710] train_loss: 0.07267 train_acc: 97.81250 val_loss: 0.71894 val_acc: 87.50000\n",
      "[1, 1720] train_loss: 0.12320 train_acc: 96.25000 val_loss: 0.69670 val_acc: 90.00000\n",
      "[1, 1730] train_loss: 0.08586 train_acc: 96.56250 val_loss: 0.35655 val_acc: 88.75000\n",
      "[1, 1740] train_loss: 0.10868 train_acc: 96.25000 val_loss: 1.04095 val_acc: 83.75000\n",
      "[1, 1750] train_loss: 0.05342 train_acc: 97.81250 val_loss: 0.61038 val_acc: 91.25000\n",
      "[1, 1760] train_loss: 0.11002 train_acc: 95.31250 val_loss: 0.13966 val_acc: 96.25000\n",
      "[1, 1770] train_loss: 0.08536 train_acc: 98.12500 val_loss: 0.68484 val_acc: 85.00000\n",
      "[1, 1780] train_loss: 0.18181 train_acc: 94.68750 val_loss: 0.40042 val_acc: 90.00000\n",
      "[1, 1790] train_loss: 0.15477 train_acc: 95.31250 val_loss: 1.02122 val_acc: 80.00000\n",
      "[1, 1800] train_loss: 0.15249 train_acc: 96.87500 val_loss: 0.33382 val_acc: 90.00000\n",
      "[1, 1810] train_loss: 0.04805 train_acc: 99.06250 val_loss: 0.64707 val_acc: 86.25000\n",
      "[1, 1820] train_loss: 0.26444 train_acc: 93.12500 val_loss: 0.90263 val_acc: 86.25000\n",
      "[1, 1830] train_loss: 0.16379 train_acc: 95.62500 val_loss: 0.64669 val_acc: 86.25000\n",
      "[1, 1840] train_loss: 0.10725 train_acc: 97.18750 val_loss: 0.38630 val_acc: 90.00000\n",
      "[1, 1850] train_loss: 0.08399 train_acc: 97.81250 val_loss: 0.51179 val_acc: 91.25000\n",
      "[1, 1860] train_loss: 0.15608 train_acc: 97.18750 val_loss: 0.28092 val_acc: 91.25000\n",
      "[1, 1870] train_loss: 0.07758 train_acc: 97.18750 val_loss: 0.46425 val_acc: 92.50000\n",
      "[1, 1880] train_loss: 0.08798 train_acc: 95.93750 val_loss: 0.76580 val_acc: 87.50000\n",
      "[1, 1890] train_loss: 0.08117 train_acc: 97.18750 val_loss: 0.22923 val_acc: 95.00000\n",
      "[1, 1900] train_loss: 0.05544 train_acc: 98.12500 val_loss: 0.54378 val_acc: 87.50000\n",
      "[1, 1910] train_loss: 0.08418 train_acc: 96.87500 val_loss: 0.24056 val_acc: 95.00000\n",
      "[1, 1920] train_loss: 0.08894 train_acc: 97.18750 val_loss: 0.52320 val_acc: 90.00000\n",
      "[1, 1930] train_loss: 0.07210 train_acc: 97.81250 val_loss: 0.95100 val_acc: 86.25000\n",
      "[1, 1940] train_loss: 0.08599 train_acc: 98.12500 val_loss: 0.31455 val_acc: 93.75000\n",
      "[1, 1950] train_loss: 0.06650 train_acc: 96.87500 val_loss: 1.16403 val_acc: 78.75000\n",
      "[1, 1960] train_loss: 0.10624 train_acc: 97.50000 val_loss: 0.47398 val_acc: 91.25000\n",
      "[1, 1970] train_loss: 0.06798 train_acc: 97.81250 val_loss: 0.40138 val_acc: 88.75000\n",
      "[1, 1980] train_loss: 0.08110 train_acc: 97.50000 val_loss: 0.56630 val_acc: 93.75000\n",
      "[1, 1990] train_loss: 0.12981 train_acc: 96.56250 val_loss: 0.47969 val_acc: 87.50000\n",
      "[1, 2000] train_loss: 0.12543 train_acc: 96.56250 val_loss: 0.63754 val_acc: 92.50000\n",
      "[1, 2010] train_loss: 0.08920 train_acc: 97.18750 val_loss: 0.57324 val_acc: 87.50000\n",
      "[1, 2020] train_loss: 0.09352 train_acc: 96.56250 val_loss: 0.46565 val_acc: 91.25000\n",
      "[1, 2030] train_loss: 0.11881 train_acc: 96.25000 val_loss: 0.34647 val_acc: 91.25000\n",
      "[1, 2040] train_loss: 0.05464 train_acc: 98.43750 val_loss: 0.69217 val_acc: 87.50000\n",
      "[1, 2050] train_loss: 0.11140 train_acc: 96.25000 val_loss: 0.51949 val_acc: 87.50000\n",
      "[1, 2060] train_loss: 0.12215 train_acc: 96.25000 val_loss: 0.67388 val_acc: 87.50000\n",
      "[1, 2070] train_loss: 0.11019 train_acc: 96.87500 val_loss: 0.64760 val_acc: 83.75000\n",
      "[1, 2080] train_loss: 0.09193 train_acc: 97.18750 val_loss: 0.91916 val_acc: 77.50000\n",
      "[1, 2090] train_loss: 0.11070 train_acc: 95.31250 val_loss: 0.31060 val_acc: 92.50000\n",
      "[1, 2100] train_loss: 0.10715 train_acc: 96.56250 val_loss: 0.60246 val_acc: 85.00000\n",
      "[1, 2110] train_loss: 0.11342 train_acc: 96.25000 val_loss: 0.89815 val_acc: 85.00000\n",
      "[1, 2120] train_loss: 0.09696 train_acc: 97.18750 val_loss: 0.34985 val_acc: 91.25000\n",
      "[1, 2130] train_loss: 0.17924 train_acc: 95.31250 val_loss: 0.74258 val_acc: 85.00000\n",
      "[1, 2140] train_loss: 0.06624 train_acc: 98.12500 val_loss: 0.79862 val_acc: 86.25000\n",
      "[1, 2150] train_loss: 0.08249 train_acc: 97.50000 val_loss: 0.88429 val_acc: 80.00000\n",
      "[1, 2160] train_loss: 0.11095 train_acc: 97.18750 val_loss: 0.38016 val_acc: 90.00000\n",
      "[1, 2170] train_loss: 0.12178 train_acc: 95.93750 val_loss: 1.02931 val_acc: 82.50000\n",
      "[1, 2180] train_loss: 0.19188 train_acc: 94.68750 val_loss: 0.38054 val_acc: 91.25000\n",
      "[1, 2190] train_loss: 0.06998 train_acc: 97.81250 val_loss: 0.34687 val_acc: 92.50000\n",
      "[1, 2200] train_loss: 0.08513 train_acc: 97.50000 val_loss: 0.22426 val_acc: 95.00000\n",
      "[1, 2210] train_loss: 0.14411 train_acc: 96.87500 val_loss: 0.82688 val_acc: 85.00000\n",
      "[1, 2220] train_loss: 0.14046 train_acc: 96.87500 val_loss: 0.31126 val_acc: 93.75000\n",
      "[1, 2230] train_loss: 0.11178 train_acc: 96.56250 val_loss: 0.58122 val_acc: 87.50000\n",
      "[1, 2240] train_loss: 0.07896 train_acc: 97.50000 val_loss: 0.35688 val_acc: 92.50000\n",
      "[1, 2250] train_loss: 0.08718 train_acc: 97.50000 val_loss: 0.80562 val_acc: 83.75000\n",
      "[1, 2260] train_loss: 0.16862 train_acc: 95.31250 val_loss: 0.85224 val_acc: 86.25000\n",
      "[1, 2270] train_loss: 0.11100 train_acc: 97.50000 val_loss: 0.59183 val_acc: 85.00000\n",
      "[1, 2280] train_loss: 0.12663 train_acc: 96.87500 val_loss: 0.30242 val_acc: 92.50000\n",
      "[1, 2290] train_loss: 0.10895 train_acc: 97.18750 val_loss: 0.42612 val_acc: 91.25000\n",
      "[1, 2300] train_loss: 0.11134 train_acc: 97.18750 val_loss: 0.41857 val_acc: 91.25000\n",
      "[1, 2310] train_loss: 0.11093 train_acc: 95.62500 val_loss: 0.21989 val_acc: 96.25000\n",
      "[1, 2320] train_loss: 0.10429 train_acc: 97.50000 val_loss: 0.43074 val_acc: 90.00000\n",
      "[1, 2330] train_loss: 0.08850 train_acc: 96.25000 val_loss: 0.30324 val_acc: 93.75000\n",
      "[1, 2340] train_loss: 0.10294 train_acc: 96.87500 val_loss: 0.39509 val_acc: 92.50000\n",
      "[1, 2350] train_loss: 0.08496 train_acc: 97.50000 val_loss: 0.20081 val_acc: 93.75000\n",
      "[1, 2360] train_loss: 0.07391 train_acc: 97.81250 val_loss: 0.31272 val_acc: 91.25000\n",
      "[1, 2370] train_loss: 0.10154 train_acc: 96.87500 val_loss: 0.57327 val_acc: 90.00000\n",
      "[1, 2380] train_loss: 0.08174 train_acc: 97.18750 val_loss: 0.29386 val_acc: 91.25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2390] train_loss: 0.24667 train_acc: 93.43750 val_loss: 0.70783 val_acc: 87.50000\n",
      "[1, 2400] train_loss: 0.15187 train_acc: 96.56250 val_loss: 0.70925 val_acc: 88.75000\n",
      "[1, 2410] train_loss: 0.11525 train_acc: 96.25000 val_loss: 0.27280 val_acc: 90.00000\n",
      "[1, 2420] train_loss: 0.13660 train_acc: 95.93750 val_loss: 0.63594 val_acc: 87.50000\n",
      "[1, 2430] train_loss: 0.11360 train_acc: 95.93750 val_loss: 0.33105 val_acc: 91.25000\n",
      "[1, 2440] train_loss: 0.07778 train_acc: 97.81250 val_loss: 0.39030 val_acc: 91.25000\n",
      "[1, 2450] train_loss: 0.13082 train_acc: 95.31250 val_loss: 0.79757 val_acc: 85.00000\n",
      "[1, 2460] train_loss: 0.16327 train_acc: 95.93750 val_loss: 0.73297 val_acc: 88.75000\n",
      "[1, 2470] train_loss: 0.10100 train_acc: 98.12500 val_loss: 0.39840 val_acc: 90.00000\n",
      "[1, 2480] train_loss: 0.12539 train_acc: 96.25000 val_loss: 0.72293 val_acc: 88.75000\n",
      "[1, 2490] train_loss: 0.11407 train_acc: 97.18750 val_loss: 0.41015 val_acc: 88.75000\n",
      "[1, 2500] train_loss: 0.12576 train_acc: 97.50000 val_loss: 0.28528 val_acc: 92.50000\n",
      "[1, 2510] train_loss: 0.09033 train_acc: 97.50000 val_loss: 0.65002 val_acc: 88.75000\n",
      "[1, 2520] train_loss: 0.18590 train_acc: 94.06250 val_loss: 0.72496 val_acc: 88.75000\n",
      "[1, 2530] train_loss: 0.12396 train_acc: 96.25000 val_loss: 0.46695 val_acc: 91.25000\n",
      "[1, 2540] train_loss: 0.13411 train_acc: 96.56250 val_loss: 0.58988 val_acc: 86.25000\n",
      "[1, 2550] train_loss: 0.11554 train_acc: 98.12500 val_loss: 0.36031 val_acc: 92.50000\n",
      "[1, 2560] train_loss: 0.16335 train_acc: 95.93750 val_loss: 0.42888 val_acc: 90.00000\n",
      "[1, 2570] train_loss: 0.07622 train_acc: 97.50000 val_loss: 0.75525 val_acc: 88.75000\n",
      "[1, 2580] train_loss: 0.14694 train_acc: 96.56250 val_loss: 0.55275 val_acc: 85.00000\n",
      "[1, 2590] train_loss: 0.11363 train_acc: 96.25000 val_loss: 0.49054 val_acc: 92.50000\n",
      "[1, 2600] train_loss: 0.07669 train_acc: 97.81250 val_loss: 0.12292 val_acc: 95.00000\n",
      "[1, 2610] train_loss: 0.05767 train_acc: 98.75000 val_loss: 0.57338 val_acc: 86.25000\n",
      "[1, 2620] train_loss: 0.09495 train_acc: 96.25000 val_loss: 0.48035 val_acc: 95.00000\n",
      "[1, 2630] train_loss: 0.13040 train_acc: 96.25000 val_loss: 0.34758 val_acc: 90.00000\n",
      "[1, 2640] train_loss: 0.09960 train_acc: 97.50000 val_loss: 0.36025 val_acc: 93.75000\n",
      "[1, 2650] train_loss: 0.04291 train_acc: 99.06250 val_loss: 0.69218 val_acc: 85.00000\n",
      "[1, 2660] train_loss: 0.08756 train_acc: 98.12500 val_loss: 0.79866 val_acc: 88.75000\n",
      "[1, 2670] train_loss: 0.12209 train_acc: 96.25000 val_loss: 0.53565 val_acc: 90.00000\n",
      "[1, 2680] train_loss: 0.09391 train_acc: 97.50000 val_loss: 0.51684 val_acc: 91.25000\n",
      "[1, 2690] train_loss: 0.12785 train_acc: 97.50000 val_loss: 0.22906 val_acc: 95.00000\n",
      "[1, 2700] train_loss: 0.06954 train_acc: 96.87500 val_loss: 0.37407 val_acc: 92.50000\n",
      "[1, 2710] train_loss: 0.09645 train_acc: 98.12500 val_loss: 0.38849 val_acc: 90.00000\n",
      "[1, 2720] train_loss: 0.14865 train_acc: 95.93750 val_loss: 0.42256 val_acc: 91.25000\n",
      "[1, 2730] train_loss: 0.05491 train_acc: 97.81250 val_loss: 0.14476 val_acc: 96.25000\n",
      "[1, 2740] train_loss: 0.13190 train_acc: 96.87500 val_loss: 0.71183 val_acc: 87.50000\n",
      "[1, 2750] train_loss: 0.13149 train_acc: 95.62500 val_loss: 0.31583 val_acc: 92.50000\n",
      "[1, 2760] train_loss: 0.09836 train_acc: 97.50000 val_loss: 0.64338 val_acc: 88.75000\n",
      "[1, 2770] train_loss: 0.09399 train_acc: 96.87500 val_loss: 0.52061 val_acc: 88.75000\n",
      "[1, 2780] train_loss: 0.06686 train_acc: 97.50000 val_loss: 0.63752 val_acc: 82.50000\n",
      "[1, 2790] train_loss: 0.06153 train_acc: 97.81250 val_loss: 0.52125 val_acc: 87.50000\n",
      "[1, 2800] train_loss: 0.08233 train_acc: 97.18750 val_loss: 0.28677 val_acc: 90.00000\n",
      "[1, 2810] train_loss: 0.05716 train_acc: 98.43750 val_loss: 0.47013 val_acc: 81.25000\n",
      "[1, 2820] train_loss: 0.12194 train_acc: 95.31250 val_loss: 0.74696 val_acc: 86.25000\n",
      "[1, 2830] train_loss: 0.10662 train_acc: 96.87500 val_loss: 0.46644 val_acc: 90.00000\n",
      "[1, 2840] train_loss: 0.04183 train_acc: 98.43750 val_loss: 0.69469 val_acc: 85.00000\n",
      "[1, 2850] train_loss: 0.15772 train_acc: 95.93750 val_loss: 0.64145 val_acc: 85.00000\n",
      "[1, 2860] train_loss: 0.14535 train_acc: 96.56250 val_loss: 0.62424 val_acc: 88.75000\n",
      "[1, 2870] train_loss: 0.07295 train_acc: 98.12500 val_loss: 0.88859 val_acc: 83.75000\n",
      "[1, 2880] train_loss: 0.11353 train_acc: 96.87500 val_loss: 0.40323 val_acc: 92.50000\n",
      "[1, 2890] train_loss: 0.08613 train_acc: 97.50000 val_loss: 0.57458 val_acc: 91.25000\n",
      "[1, 2900] train_loss: 0.04174 train_acc: 98.43750 val_loss: 0.38796 val_acc: 92.50000\n",
      "[1, 2910] train_loss: 0.06312 train_acc: 98.43750 val_loss: 0.54224 val_acc: 87.50000\n",
      "[1, 2920] train_loss: 0.14079 train_acc: 96.87500 val_loss: 0.44440 val_acc: 90.00000\n",
      "[1, 2930] train_loss: 0.09530 train_acc: 96.87500 val_loss: 0.70054 val_acc: 90.00000\n",
      "[1, 2940] train_loss: 0.13720 train_acc: 95.93750 val_loss: 0.82899 val_acc: 88.75000\n",
      "[1, 2950] train_loss: 0.16875 train_acc: 95.31250 val_loss: 0.78970 val_acc: 88.75000\n",
      "[1, 2960] train_loss: 0.06376 train_acc: 97.81250 val_loss: 0.90159 val_acc: 80.00000\n",
      "[1, 2970] train_loss: 0.08910 train_acc: 97.50000 val_loss: 0.27982 val_acc: 91.25000\n",
      "[1, 2980] train_loss: 0.07773 train_acc: 97.81250 val_loss: 0.44275 val_acc: 90.00000\n",
      "[1, 2990] train_loss: 0.09687 train_acc: 97.50000 val_loss: 0.17134 val_acc: 96.25000\n",
      "[1, 3000] train_loss: 0.10495 train_acc: 96.87500 val_loss: 0.50131 val_acc: 91.25000\n",
      "[1, 3010] train_loss: 0.11570 train_acc: 96.25000 val_loss: 0.61384 val_acc: 86.25000\n",
      "[1, 3020] train_loss: 0.06536 train_acc: 98.12500 val_loss: 0.24404 val_acc: 92.50000\n",
      "[1, 3030] train_loss: 0.07843 train_acc: 96.87500 val_loss: 0.20497 val_acc: 95.00000\n",
      "[1, 3040] train_loss: 0.14510 train_acc: 95.62500 val_loss: 0.44203 val_acc: 92.50000\n",
      "[1, 3050] train_loss: 0.10411 train_acc: 96.87500 val_loss: 0.31646 val_acc: 90.00000\n",
      "[1, 3060] train_loss: 0.08051 train_acc: 97.18750 val_loss: 0.33955 val_acc: 92.50000\n",
      "[1, 3070] train_loss: 0.06173 train_acc: 97.50000 val_loss: 0.44924 val_acc: 87.50000\n",
      "[1, 3080] train_loss: 0.13896 train_acc: 95.93750 val_loss: 0.45517 val_acc: 86.25000\n",
      "[1, 3090] train_loss: 0.08508 train_acc: 97.50000 val_loss: 0.25801 val_acc: 92.50000\n",
      "[1, 3100] train_loss: 0.10478 train_acc: 96.87500 val_loss: 1.10748 val_acc: 82.50000\n",
      "[1, 3110] train_loss: 0.04472 train_acc: 98.43750 val_loss: 0.89418 val_acc: 85.00000\n",
      "[1, 3120] train_loss: 0.01869 train_acc: 99.68750 val_loss: 0.49448 val_acc: 92.50000\n",
      "[1, 3130] train_loss: 0.12838 train_acc: 96.87500 val_loss: 0.70179 val_acc: 88.75000\n",
      "[1, 3140] train_loss: 0.13806 train_acc: 97.18750 val_loss: 0.40731 val_acc: 90.00000\n",
      "[1, 3150] train_loss: 0.09841 train_acc: 97.50000 val_loss: 0.57870 val_acc: 88.75000\n",
      "[1, 3160] train_loss: 0.09144 train_acc: 96.56250 val_loss: 0.39237 val_acc: 90.00000\n",
      "[1, 3170] train_loss: 0.05080 train_acc: 98.43750 val_loss: 0.72515 val_acc: 87.50000\n",
      "[1, 3180] train_loss: 0.08530 train_acc: 97.50000 val_loss: 0.32804 val_acc: 92.50000\n",
      "[1, 3190] train_loss: 0.17967 train_acc: 95.31250 val_loss: 0.56056 val_acc: 91.25000\n",
      "[1, 3200] train_loss: 0.04625 train_acc: 98.12500 val_loss: 0.62642 val_acc: 91.25000\n",
      "[1, 3210] train_loss: 0.07956 train_acc: 97.18750 val_loss: 0.40996 val_acc: 91.25000\n",
      "[1, 3220] train_loss: 0.10984 train_acc: 96.56250 val_loss: 0.39196 val_acc: 92.50000\n",
      "[1, 3230] train_loss: 0.13087 train_acc: 96.87500 val_loss: 0.44738 val_acc: 91.25000\n",
      "[1, 3240] train_loss: 0.12858 train_acc: 95.31250 val_loss: 0.46313 val_acc: 91.25000\n",
      "[1, 3250] train_loss: 0.07240 train_acc: 97.81250 val_loss: 0.91041 val_acc: 87.50000\n",
      "[1, 3260] train_loss: 0.05842 train_acc: 97.18750 val_loss: 0.84114 val_acc: 86.25000\n",
      "[1, 3270] train_loss: 0.08307 train_acc: 98.12500 val_loss: 0.53055 val_acc: 88.75000\n",
      "[1, 3280] train_loss: 0.08798 train_acc: 97.18750 val_loss: 0.22537 val_acc: 92.50000\n",
      "[1, 3290] train_loss: 0.10485 train_acc: 98.43750 val_loss: 0.34376 val_acc: 92.50000\n",
      "[1, 3300] train_loss: 0.04651 train_acc: 99.06250 val_loss: 0.29150 val_acc: 92.50000\n",
      "[1, 3310] train_loss: 0.15745 train_acc: 95.62500 val_loss: 0.83030 val_acc: 88.75000\n",
      "[1, 3320] train_loss: 0.14800 train_acc: 97.50000 val_loss: 0.55267 val_acc: 87.50000\n",
      "[1, 3330] train_loss: 0.12960 train_acc: 96.87500 val_loss: 0.47871 val_acc: 91.25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3340] train_loss: 0.10815 train_acc: 97.18750 val_loss: 0.36861 val_acc: 92.50000\n",
      "[1, 3350] train_loss: 0.11892 train_acc: 97.18750 val_loss: 0.82082 val_acc: 86.25000\n",
      "[1, 3360] train_loss: 0.07921 train_acc: 97.18750 val_loss: 0.35922 val_acc: 92.50000\n",
      "[1, 3370] train_loss: 0.07269 train_acc: 97.81250 val_loss: 0.71686 val_acc: 87.50000\n",
      "[1, 3380] train_loss: 0.07299 train_acc: 98.43750 val_loss: 0.59601 val_acc: 90.00000\n",
      "[1, 3390] train_loss: 0.20987 train_acc: 94.68750 val_loss: 0.85784 val_acc: 86.25000\n",
      "[1, 3400] train_loss: 0.07550 train_acc: 97.50000 val_loss: 0.87683 val_acc: 88.75000\n",
      "[1, 3410] train_loss: 0.10850 train_acc: 96.25000 val_loss: 0.43329 val_acc: 92.50000\n",
      "[1, 3420] train_loss: 0.06591 train_acc: 97.81250 val_loss: 0.44507 val_acc: 90.00000\n",
      "[1, 3430] train_loss: 0.09305 train_acc: 97.18750 val_loss: 0.77435 val_acc: 85.00000\n",
      "[1, 3440] train_loss: 0.06161 train_acc: 98.12500 val_loss: 0.67939 val_acc: 91.25000\n",
      "[1, 3450] train_loss: 0.12987 train_acc: 96.87500 val_loss: 0.42549 val_acc: 93.75000\n",
      "[1, 3460] train_loss: 0.05692 train_acc: 98.75000 val_loss: 0.27469 val_acc: 91.25000\n",
      "[1, 3470] train_loss: 0.04916 train_acc: 98.43750 val_loss: 0.52322 val_acc: 87.50000\n",
      "[1, 3480] train_loss: 0.07609 train_acc: 97.50000 val_loss: 0.32523 val_acc: 93.75000\n",
      "[1, 3490] train_loss: 0.07496 train_acc: 98.12500 val_loss: 0.62426 val_acc: 85.00000\n",
      "[1, 3500] train_loss: 0.07023 train_acc: 97.81250 val_loss: 0.48846 val_acc: 88.75000\n",
      "[1, 3510] train_loss: 0.06280 train_acc: 98.43750 val_loss: 0.59585 val_acc: 90.00000\n",
      "[1, 3520] train_loss: 0.09761 train_acc: 97.50000 val_loss: 0.63045 val_acc: 90.00000\n",
      "[1, 3530] train_loss: 0.08165 train_acc: 96.56250 val_loss: 0.48700 val_acc: 90.00000\n",
      "[1, 3540] train_loss: 0.07247 train_acc: 98.43750 val_loss: 0.73031 val_acc: 83.75000\n",
      "[1, 3550] train_loss: 0.11155 train_acc: 96.56250 val_loss: 0.72656 val_acc: 88.75000\n",
      "[1, 3560] train_loss: 0.14788 train_acc: 95.62500 val_loss: 0.71996 val_acc: 86.25000\n",
      "[1, 3570] train_loss: 0.07893 train_acc: 97.18750 val_loss: 0.90725 val_acc: 85.00000\n",
      "[1, 3580] train_loss: 0.16121 train_acc: 95.00000 val_loss: 0.62094 val_acc: 87.50000\n",
      "[1, 3590] train_loss: 0.10239 train_acc: 97.50000 val_loss: 0.41845 val_acc: 88.75000\n",
      "[1, 3600] train_loss: 0.04304 train_acc: 98.12500 val_loss: 0.48756 val_acc: 90.00000\n",
      "[1, 3610] train_loss: 0.11370 train_acc: 96.56250 val_loss: 0.44145 val_acc: 93.75000\n",
      "[1, 3620] train_loss: 0.07469 train_acc: 97.81250 val_loss: 0.70642 val_acc: 87.50000\n",
      "[1, 3630] train_loss: 0.07127 train_acc: 97.50000 val_loss: 0.74673 val_acc: 87.50000\n",
      "[1, 3640] train_loss: 0.04132 train_acc: 99.06250 val_loss: 0.41977 val_acc: 91.25000\n",
      "[1, 3650] train_loss: 0.01650 train_acc: 99.68750 val_loss: 0.40550 val_acc: 91.25000\n",
      "[1, 3660] train_loss: 0.14892 train_acc: 96.87500 val_loss: 0.60374 val_acc: 86.25000\n",
      "[1, 3670] train_loss: 0.04620 train_acc: 97.81250 val_loss: 0.39872 val_acc: 91.25000\n",
      "[1, 3680] train_loss: 0.08703 train_acc: 97.81250 val_loss: 0.88057 val_acc: 85.00000\n",
      "[1, 3690] train_loss: 0.09041 train_acc: 97.81250 val_loss: 0.30043 val_acc: 92.50000\n",
      "[1, 3700] train_loss: 0.11008 train_acc: 97.81250 val_loss: 0.58009 val_acc: 90.00000\n",
      "[1, 3710] train_loss: 0.13881 train_acc: 95.62500 val_loss: 0.44092 val_acc: 88.75000\n",
      "[1, 3720] train_loss: 0.07435 train_acc: 97.81250 val_loss: 0.41266 val_acc: 92.50000\n",
      "[1, 3730] train_loss: 0.12068 train_acc: 97.18750 val_loss: 0.34229 val_acc: 95.00000\n",
      "[1, 3740] train_loss: 0.05603 train_acc: 97.81250 val_loss: 0.27322 val_acc: 91.25000\n",
      "[1, 3750] train_loss: 0.08596 train_acc: 97.81250 val_loss: 0.62830 val_acc: 88.75000\n",
      "[1, 3760] train_loss: 0.10997 train_acc: 97.50000 val_loss: 0.44810 val_acc: 92.50000\n",
      "[1, 3770] train_loss: 0.04155 train_acc: 99.06250 val_loss: 0.80906 val_acc: 88.75000\n",
      "[1, 3780] train_loss: 0.09491 train_acc: 95.62500 val_loss: 0.23781 val_acc: 96.25000\n",
      "[1, 3790] train_loss: 0.11179 train_acc: 96.56250 val_loss: 0.39235 val_acc: 91.25000\n",
      "[1, 3800] train_loss: 0.09901 train_acc: 96.56250 val_loss: 0.40468 val_acc: 91.25000\n",
      "[1, 3810] train_loss: 0.08462 train_acc: 98.12500 val_loss: 0.69391 val_acc: 86.25000\n",
      "[1, 3820] train_loss: 0.05316 train_acc: 98.75000 val_loss: 0.61607 val_acc: 86.25000\n",
      "[1, 3830] train_loss: 0.17504 train_acc: 95.31250 val_loss: 0.27674 val_acc: 92.50000\n",
      "[1, 3840] train_loss: 0.12214 train_acc: 97.81250 val_loss: 0.60228 val_acc: 85.00000\n",
      "[1, 3850] train_loss: 0.08531 train_acc: 97.81250 val_loss: 0.43199 val_acc: 91.25000\n",
      "[1, 3860] train_loss: 0.12579 train_acc: 96.56250 val_loss: 0.58592 val_acc: 90.00000\n",
      "[1, 3870] train_loss: 0.13571 train_acc: 95.62500 val_loss: 0.58278 val_acc: 87.50000\n",
      "[1, 3880] train_loss: 0.16030 train_acc: 95.31250 val_loss: 0.48263 val_acc: 91.25000\n",
      "[1, 3890] train_loss: 0.12939 train_acc: 95.00000 val_loss: 0.47408 val_acc: 92.50000\n",
      "[1, 3900] train_loss: 0.06305 train_acc: 98.12500 val_loss: 0.68972 val_acc: 90.00000\n",
      "[1, 3910] train_loss: 0.15532 train_acc: 96.56250 val_loss: 0.60490 val_acc: 88.75000\n",
      "[1, 3920] train_loss: 0.16851 train_acc: 95.31250 val_loss: 0.34727 val_acc: 90.00000\n",
      "[1, 3930] train_loss: 0.06163 train_acc: 97.18750 val_loss: 0.84306 val_acc: 82.50000\n",
      "[1, 3940] train_loss: 0.09861 train_acc: 98.12500 val_loss: 0.41996 val_acc: 93.75000\n",
      "[1, 3950] train_loss: 0.07867 train_acc: 96.87500 val_loss: 0.36444 val_acc: 88.75000\n",
      "[1, 3960] train_loss: 0.09060 train_acc: 96.87500 val_loss: 0.46002 val_acc: 95.00000\n",
      "[1, 3970] train_loss: 0.07389 train_acc: 97.18750 val_loss: 0.41926 val_acc: 91.25000\n",
      "[1, 3980] train_loss: 0.06009 train_acc: 97.81250 val_loss: 0.38891 val_acc: 88.75000\n",
      "[1, 3990] train_loss: 0.12662 train_acc: 97.18750 val_loss: 0.36193 val_acc: 90.00000\n",
      "[1, 4000] train_loss: 0.06491 train_acc: 97.18750 val_loss: 0.56819 val_acc: 92.50000\n",
      "[1, 4010] train_loss: 0.06468 train_acc: 98.75000 val_loss: 0.57807 val_acc: 91.25000\n",
      "[1, 4020] train_loss: 0.14790 train_acc: 95.93750 val_loss: 0.42947 val_acc: 90.00000\n",
      "[1, 4030] train_loss: 0.04779 train_acc: 98.12500 val_loss: 0.20852 val_acc: 92.50000\n",
      "[1, 4040] train_loss: 0.09314 train_acc: 97.50000 val_loss: 0.37058 val_acc: 92.50000\n",
      "[1, 4050] train_loss: 0.09116 train_acc: 96.56250 val_loss: 0.35417 val_acc: 93.75000\n",
      "[1, 4060] train_loss: 0.08105 train_acc: 97.18750 val_loss: 0.35771 val_acc: 92.50000\n",
      "[1, 4070] train_loss: 0.12043 train_acc: 96.56250 val_loss: 0.50589 val_acc: 86.25000\n",
      "[1, 4080] train_loss: 0.05846 train_acc: 98.43750 val_loss: 0.68790 val_acc: 87.50000\n",
      "[1, 4090] train_loss: 0.09336 train_acc: 97.18750 val_loss: 0.34892 val_acc: 92.50000\n",
      "[1, 4100] train_loss: 0.05124 train_acc: 98.12500 val_loss: 0.39233 val_acc: 92.50000\n",
      "[1, 4110] train_loss: 0.06253 train_acc: 98.12500 val_loss: 0.81201 val_acc: 88.75000\n",
      "[1, 4120] train_loss: 0.07459 train_acc: 97.50000 val_loss: 0.39337 val_acc: 92.50000\n",
      "[1, 4130] train_loss: 0.10145 train_acc: 97.50000 val_loss: 0.29908 val_acc: 96.25000\n",
      "[1, 4140] train_loss: 0.12184 train_acc: 95.62500 val_loss: 0.38465 val_acc: 93.75000\n",
      "[1, 4150] train_loss: 0.15377 train_acc: 94.68750 val_loss: 0.30304 val_acc: 92.50000\n",
      "[1, 4160] train_loss: 0.07355 train_acc: 97.50000 val_loss: 0.27639 val_acc: 93.75000\n",
      "[1, 4170] train_loss: 0.09103 train_acc: 97.18750 val_loss: 0.60854 val_acc: 88.75000\n",
      "[1, 4180] train_loss: 0.11707 train_acc: 96.87500 val_loss: 0.51764 val_acc: 91.25000\n",
      "[1, 4190] train_loss: 0.08238 train_acc: 97.81250 val_loss: 0.66197 val_acc: 91.25000\n",
      "[1, 4200] train_loss: 0.12825 train_acc: 97.50000 val_loss: 0.77725 val_acc: 90.00000\n",
      "[1, 4210] train_loss: 0.10411 train_acc: 96.56250 val_loss: 0.69018 val_acc: 86.25000\n",
      "[1, 4220] train_loss: 0.09857 train_acc: 96.87500 val_loss: 0.33682 val_acc: 92.50000\n",
      "[1, 4230] train_loss: 0.05636 train_acc: 99.06250 val_loss: 0.37187 val_acc: 91.25000\n",
      "[1, 4240] train_loss: 0.06843 train_acc: 97.81250 val_loss: 0.66357 val_acc: 91.25000\n",
      "[1, 4250] train_loss: 0.03937 train_acc: 98.75000 val_loss: 0.24911 val_acc: 93.75000\n",
      "[1, 4260] train_loss: 0.09334 train_acc: 96.87500 val_loss: 0.67687 val_acc: 88.75000\n",
      "[1, 4270] train_loss: 0.03326 train_acc: 98.75000 val_loss: 0.24390 val_acc: 93.75000\n",
      "[1, 4280] train_loss: 0.15489 train_acc: 96.87500 val_loss: 0.39504 val_acc: 92.50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4290] train_loss: 0.10129 train_acc: 97.81250 val_loss: 0.40143 val_acc: 87.50000\n",
      "[1, 4300] train_loss: 0.07626 train_acc: 98.75000 val_loss: 0.58411 val_acc: 87.50000\n",
      "[1, 4310] train_loss: 0.11388 train_acc: 95.93750 val_loss: 0.61236 val_acc: 91.25000\n",
      "[1, 4320] train_loss: 0.05210 train_acc: 98.43750 val_loss: 0.49833 val_acc: 90.00000\n",
      "[1, 4330] train_loss: 0.12738 train_acc: 95.31250 val_loss: 0.44489 val_acc: 88.75000\n",
      "[1, 4340] train_loss: 0.06683 train_acc: 98.75000 val_loss: 0.20247 val_acc: 95.00000\n",
      "[1, 4350] train_loss: 0.08708 train_acc: 97.50000 val_loss: 0.51060 val_acc: 90.00000\n",
      "[1, 4360] train_loss: 0.11919 train_acc: 95.00000 val_loss: 0.25071 val_acc: 96.25000\n",
      "[1, 4370] train_loss: 0.06879 train_acc: 97.18750 val_loss: 0.69149 val_acc: 87.50000\n",
      "[1, 4380] train_loss: 0.07543 train_acc: 97.81250 val_loss: 0.56537 val_acc: 93.75000\n",
      "[1, 4390] train_loss: 0.09535 train_acc: 97.81250 val_loss: 0.45880 val_acc: 93.75000\n",
      "[1, 4400] train_loss: 0.05258 train_acc: 98.43750 val_loss: 0.69698 val_acc: 90.00000\n",
      "[1, 4410] train_loss: 0.07600 train_acc: 96.56250 val_loss: 0.37457 val_acc: 92.50000\n",
      "[1, 4420] train_loss: 0.09765 train_acc: 97.18750 val_loss: 0.86163 val_acc: 85.00000\n",
      "[1, 4430] train_loss: 0.09648 train_acc: 98.12500 val_loss: 0.70827 val_acc: 88.75000\n",
      "[1, 4440] train_loss: 0.12723 train_acc: 96.87500 val_loss: 0.16254 val_acc: 95.00000\n",
      "[1, 4450] train_loss: 0.12547 train_acc: 95.93750 val_loss: 0.21831 val_acc: 92.50000\n",
      "[1, 4460] train_loss: 0.09675 train_acc: 97.81250 val_loss: 0.55954 val_acc: 87.50000\n",
      "[1, 4470] train_loss: 0.07441 train_acc: 98.43750 val_loss: 0.40347 val_acc: 93.75000\n",
      "[1, 4480] train_loss: 0.07152 train_acc: 98.75000 val_loss: 0.72300 val_acc: 93.75000\n",
      "[1, 4490] train_loss: 0.12134 train_acc: 96.56250 val_loss: 0.30350 val_acc: 93.75000\n",
      "[1, 4500] train_loss: 0.06176 train_acc: 97.81250 val_loss: 0.18958 val_acc: 95.00000\n",
      "[1, 4510] train_loss: 0.07010 train_acc: 98.12500 val_loss: 0.68270 val_acc: 88.75000\n",
      "[1, 4520] train_loss: 0.11972 train_acc: 97.18750 val_loss: 0.74183 val_acc: 87.50000\n",
      "[1, 4530] train_loss: 0.12125 train_acc: 96.25000 val_loss: 0.34487 val_acc: 93.75000\n",
      "[1, 4540] train_loss: 0.10964 train_acc: 96.25000 val_loss: 0.45944 val_acc: 95.00000\n",
      "[1, 4550] train_loss: 0.06250 train_acc: 98.12500 val_loss: 0.40305 val_acc: 91.25000\n",
      "[1, 4560] train_loss: 0.11143 train_acc: 97.50000 val_loss: 0.85958 val_acc: 86.25000\n",
      "[1, 4570] train_loss: 0.12209 train_acc: 95.93750 val_loss: 0.70193 val_acc: 86.25000\n",
      "[1, 4580] train_loss: 0.10500 train_acc: 97.18750 val_loss: 0.48832 val_acc: 88.75000\n",
      "[1, 4590] train_loss: 0.10838 train_acc: 97.18750 val_loss: 0.38473 val_acc: 90.00000\n",
      "[1, 4600] train_loss: 0.08764 train_acc: 97.50000 val_loss: 0.25920 val_acc: 93.75000\n",
      "[1, 4610] train_loss: 0.07040 train_acc: 97.81250 val_loss: 0.94971 val_acc: 83.75000\n",
      "[1, 4620] train_loss: 0.10734 train_acc: 97.18750 val_loss: 0.36407 val_acc: 92.50000\n",
      "[1, 4630] train_loss: 0.07963 train_acc: 98.12500 val_loss: 0.96472 val_acc: 85.00000\n",
      "[1, 4640] train_loss: 0.07387 train_acc: 98.75000 val_loss: 0.58694 val_acc: 86.25000\n",
      "[1, 4650] train_loss: 0.03745 train_acc: 99.06250 val_loss: 0.66103 val_acc: 85.00000\n",
      "[1, 4660] train_loss: 0.06313 train_acc: 98.12500 val_loss: 0.54948 val_acc: 88.75000\n",
      "[1, 4670] train_loss: 0.08016 train_acc: 97.50000 val_loss: 0.54104 val_acc: 90.00000\n",
      "[1, 4680] train_loss: 0.06924 train_acc: 98.75000 val_loss: 0.43388 val_acc: 93.75000\n",
      "[1, 4690] train_loss: 0.11401 train_acc: 97.18750 val_loss: 0.48510 val_acc: 86.25000\n",
      "[1, 4700] train_loss: 0.12168 train_acc: 96.87500 val_loss: 0.54224 val_acc: 91.25000\n",
      "[1, 4710] train_loss: 0.12070 train_acc: 97.50000 val_loss: 0.26393 val_acc: 95.00000\n",
      "[1, 4720] train_loss: 0.08153 train_acc: 97.50000 val_loss: 0.32628 val_acc: 95.00000\n",
      "[1, 4730] train_loss: 0.11001 train_acc: 96.56250 val_loss: 0.32625 val_acc: 92.50000\n",
      "[1, 4740] train_loss: 0.09582 train_acc: 96.87500 val_loss: 0.67161 val_acc: 88.75000\n",
      "[1, 4750] train_loss: 0.12569 train_acc: 96.56250 val_loss: 0.57646 val_acc: 83.75000\n",
      "[1, 4760] train_loss: 0.08934 train_acc: 97.18750 val_loss: 0.50045 val_acc: 91.25000\n",
      "[1, 4770] train_loss: 0.06119 train_acc: 98.43750 val_loss: 0.43209 val_acc: 91.25000\n",
      "[1, 4780] train_loss: 0.05811 train_acc: 98.75000 val_loss: 0.60900 val_acc: 91.25000\n",
      "[1, 4790] train_loss: 0.06009 train_acc: 97.81250 val_loss: 0.14968 val_acc: 96.25000\n",
      "[1, 4800] train_loss: 0.14647 train_acc: 95.93750 val_loss: 0.35959 val_acc: 90.00000\n",
      "[1, 4810] train_loss: 0.08966 train_acc: 97.81250 val_loss: 0.32735 val_acc: 92.50000\n",
      "[1, 4820] train_loss: 0.11900 train_acc: 96.87500 val_loss: 0.21499 val_acc: 93.75000\n",
      "[1, 4830] train_loss: 0.17083 train_acc: 95.00000 val_loss: 0.51735 val_acc: 87.50000\n",
      "[1, 4840] train_loss: 0.05196 train_acc: 98.12500 val_loss: 0.64998 val_acc: 85.00000\n",
      "[1, 4850] train_loss: 0.09532 train_acc: 97.50000 val_loss: 0.73776 val_acc: 87.50000\n",
      "[1, 4860] train_loss: 0.07602 train_acc: 97.81250 val_loss: 0.59142 val_acc: 87.50000\n",
      "[1, 4870] train_loss: 0.08540 train_acc: 97.50000 val_loss: 0.45025 val_acc: 90.00000\n",
      "[1, 4880] train_loss: 0.05061 train_acc: 98.43750 val_loss: 0.22372 val_acc: 93.75000\n",
      "[1, 4890] train_loss: 0.05448 train_acc: 98.12500 val_loss: 0.43601 val_acc: 90.00000\n",
      "[1, 4900] train_loss: 0.06170 train_acc: 98.75000 val_loss: 0.71310 val_acc: 91.25000\n",
      "[1, 4910] train_loss: 0.03736 train_acc: 99.06250 val_loss: 0.31229 val_acc: 90.00000\n",
      "[1, 4920] train_loss: 0.08641 train_acc: 97.18750 val_loss: 0.52444 val_acc: 92.50000\n",
      "[1, 4930] train_loss: 0.05633 train_acc: 97.50000 val_loss: 0.42623 val_acc: 92.50000\n",
      "[1, 4940] train_loss: 0.06893 train_acc: 97.18750 val_loss: 0.40432 val_acc: 93.75000\n",
      "[1, 4950] train_loss: 0.09853 train_acc: 97.50000 val_loss: 0.51311 val_acc: 92.50000\n",
      "[1, 4960] train_loss: 0.08598 train_acc: 98.43750 val_loss: 0.64023 val_acc: 90.00000\n",
      "[1, 4970] train_loss: 0.05225 train_acc: 98.12500 val_loss: 0.32262 val_acc: 92.50000\n",
      "[1, 4980] train_loss: 0.02801 train_acc: 99.37500 val_loss: 0.01617 val_acc: 100.00000\n",
      "[1, 4990] train_loss: 0.07002 train_acc: 97.81250 val_loss: 0.54447 val_acc: 88.75000\n",
      "[1, 5000] train_loss: 0.08377 train_acc: 97.18750 val_loss: 0.42721 val_acc: 90.00000\n",
      "[1, 5010] train_loss: 0.10989 train_acc: 97.18750 val_loss: 0.36639 val_acc: 88.75000\n",
      "[1, 5020] train_loss: 0.09601 train_acc: 97.81250 val_loss: 0.14140 val_acc: 97.50000\n",
      "[1, 5030] train_loss: 0.08721 train_acc: 96.87500 val_loss: 0.60449 val_acc: 90.00000\n",
      "[1, 5040] train_loss: 0.12915 train_acc: 96.56250 val_loss: 0.86408 val_acc: 81.25000\n",
      "[1, 5050] train_loss: 0.07466 train_acc: 98.12500 val_loss: 0.98803 val_acc: 86.25000\n",
      "[1, 5060] train_loss: 0.08243 train_acc: 97.18750 val_loss: 0.11258 val_acc: 96.25000\n",
      "[1, 5070] train_loss: 0.15595 train_acc: 96.56250 val_loss: 0.96530 val_acc: 80.00000\n",
      "[1, 5080] train_loss: 0.04686 train_acc: 98.43750 val_loss: 0.79287 val_acc: 87.50000\n",
      "[1, 5090] train_loss: 0.10093 train_acc: 96.87500 val_loss: 0.09856 val_acc: 98.75000\n",
      "[1, 5100] train_loss: 0.07714 train_acc: 97.81250 val_loss: 0.57743 val_acc: 88.75000\n",
      "[1, 5110] train_loss: 0.04509 train_acc: 99.06250 val_loss: 0.31857 val_acc: 93.75000\n",
      "[1, 5120] train_loss: 0.09191 train_acc: 96.25000 val_loss: 0.80968 val_acc: 85.00000\n",
      "[1, 5130] train_loss: 0.08611 train_acc: 97.50000 val_loss: 0.52453 val_acc: 90.00000\n",
      "[1, 5140] train_loss: 0.06630 train_acc: 98.75000 val_loss: 0.42042 val_acc: 92.50000\n",
      "[1, 5150] train_loss: 0.08999 train_acc: 97.81250 val_loss: 0.47888 val_acc: 91.25000\n",
      "[1, 5160] train_loss: 0.10705 train_acc: 96.25000 val_loss: 0.48737 val_acc: 92.50000\n",
      "[1, 5170] train_loss: 0.09456 train_acc: 97.50000 val_loss: 1.09840 val_acc: 88.75000\n",
      "[1, 5180] train_loss: 0.06834 train_acc: 97.81250 val_loss: 0.41415 val_acc: 92.50000\n",
      "[1, 5190] train_loss: 0.09016 train_acc: 98.12500 val_loss: 0.57729 val_acc: 90.00000\n",
      "[1, 5200] train_loss: 0.09603 train_acc: 96.87500 val_loss: 0.66069 val_acc: 86.25000\n",
      "[1, 5210] train_loss: 0.14153 train_acc: 96.56250 val_loss: 0.52594 val_acc: 87.50000\n",
      "[1, 5220] train_loss: 0.07555 train_acc: 97.50000 val_loss: 0.61232 val_acc: 87.50000\n",
      "[1, 5230] train_loss: 0.19487 train_acc: 95.31250 val_loss: 0.73889 val_acc: 90.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5240] train_loss: 0.11351 train_acc: 95.93750 val_loss: 0.82729 val_acc: 85.00000\n",
      "[1, 5250] train_loss: 0.04310 train_acc: 98.43750 val_loss: 0.89069 val_acc: 87.50000\n",
      "[1, 5260] train_loss: 0.09129 train_acc: 97.81250 val_loss: 0.40000 val_acc: 93.75000\n",
      "[1, 5270] train_loss: 0.11739 train_acc: 96.56250 val_loss: 0.19360 val_acc: 93.75000\n",
      "[1, 5280] train_loss: 0.12330 train_acc: 96.56250 val_loss: 0.61112 val_acc: 88.75000\n",
      "[1, 5290] train_loss: 0.07552 train_acc: 96.56250 val_loss: 0.84049 val_acc: 88.75000\n",
      "[1, 5300] train_loss: 0.09418 train_acc: 96.56250 val_loss: 0.44503 val_acc: 90.00000\n",
      "[1, 5310] train_loss: 0.06861 train_acc: 97.81250 val_loss: 0.33838 val_acc: 92.50000\n",
      "[1, 5320] train_loss: 0.14710 train_acc: 96.25000 val_loss: 0.55725 val_acc: 88.75000\n",
      "[1, 5330] train_loss: 0.15791 train_acc: 96.56250 val_loss: 0.76258 val_acc: 86.25000\n",
      "[1, 5340] train_loss: 0.10949 train_acc: 96.87500 val_loss: 0.54826 val_acc: 92.50000\n",
      "[1, 5350] train_loss: 0.06637 train_acc: 98.12500 val_loss: 0.76975 val_acc: 87.50000\n",
      "[1, 5360] train_loss: 0.10543 train_acc: 97.18750 val_loss: 0.40411 val_acc: 95.00000\n",
      "[1, 5370] train_loss: 0.05075 train_acc: 99.06250 val_loss: 0.28333 val_acc: 92.50000\n",
      "[1, 5380] train_loss: 0.07469 train_acc: 97.50000 val_loss: 0.33928 val_acc: 95.00000\n",
      "[1, 5390] train_loss: 0.06378 train_acc: 98.12500 val_loss: 0.41861 val_acc: 92.50000\n",
      "[1, 5400] train_loss: 0.06165 train_acc: 98.43750 val_loss: 0.35503 val_acc: 88.75000\n",
      "[1, 5410] train_loss: 0.08228 train_acc: 97.50000 val_loss: 0.48712 val_acc: 95.00000\n",
      "[1, 5420] train_loss: 0.10286 train_acc: 96.87500 val_loss: 0.55777 val_acc: 91.25000\n",
      "[1, 5430] train_loss: 0.13420 train_acc: 97.18750 val_loss: 0.68496 val_acc: 90.00000\n",
      "[1, 5440] train_loss: 0.10984 train_acc: 97.18750 val_loss: 0.53500 val_acc: 91.25000\n",
      "[1, 5450] train_loss: 0.07217 train_acc: 98.12500 val_loss: 0.58880 val_acc: 88.75000\n",
      "[1, 5460] train_loss: 0.12633 train_acc: 96.25000 val_loss: 1.19683 val_acc: 86.25000\n",
      "[1, 5470] train_loss: 0.08225 train_acc: 97.18750 val_loss: 0.31011 val_acc: 93.75000\n",
      "[1, 5480] train_loss: 0.15449 train_acc: 96.25000 val_loss: 0.52895 val_acc: 92.50000\n",
      "[1, 5490] train_loss: 0.09733 train_acc: 96.87500 val_loss: 0.63395 val_acc: 85.00000\n",
      "[1, 5500] train_loss: 0.03292 train_acc: 98.43750 val_loss: 0.69840 val_acc: 86.25000\n",
      "[1, 5510] train_loss: 0.09145 train_acc: 97.18750 val_loss: 0.63198 val_acc: 90.00000\n",
      "[1, 5520] train_loss: 0.07413 train_acc: 97.50000 val_loss: 0.72728 val_acc: 87.50000\n",
      "[1, 5530] train_loss: 0.08585 train_acc: 97.50000 val_loss: 0.67765 val_acc: 90.00000\n",
      "[1, 5540] train_loss: 0.07600 train_acc: 97.50000 val_loss: 0.74109 val_acc: 88.75000\n",
      "[1, 5550] train_loss: 0.06106 train_acc: 98.43750 val_loss: 0.59524 val_acc: 91.25000\n",
      "[1, 5560] train_loss: 0.11295 train_acc: 96.25000 val_loss: 0.49561 val_acc: 90.00000\n",
      "[1, 5570] train_loss: 0.09878 train_acc: 97.18750 val_loss: 0.63123 val_acc: 88.75000\n",
      "[1, 5580] train_loss: 0.06435 train_acc: 97.81250 val_loss: 0.76942 val_acc: 83.75000\n",
      "[1, 5590] train_loss: 0.12104 train_acc: 97.50000 val_loss: 0.27497 val_acc: 93.75000\n",
      "[1, 5600] train_loss: 0.13831 train_acc: 96.87500 val_loss: 0.74154 val_acc: 81.25000\n",
      "[1, 5610] train_loss: 0.12588 train_acc: 97.18750 val_loss: 0.36139 val_acc: 90.00000\n",
      "[1, 5620] train_loss: 0.13052 train_acc: 95.31250 val_loss: 0.75234 val_acc: 87.50000\n",
      "[1, 5630] train_loss: 0.08615 train_acc: 96.87500 val_loss: 0.24749 val_acc: 91.25000\n",
      "[1, 5640] train_loss: 0.07109 train_acc: 98.43750 val_loss: 0.66806 val_acc: 91.25000\n",
      "[1, 5650] train_loss: 0.08071 train_acc: 97.81250 val_loss: 0.56674 val_acc: 90.00000\n",
      "[1, 5660] train_loss: 0.06442 train_acc: 98.12500 val_loss: 0.56895 val_acc: 87.50000\n",
      "[1, 5670] train_loss: 0.10416 train_acc: 97.18750 val_loss: 0.85564 val_acc: 85.00000\n",
      "[1, 5680] train_loss: 0.09525 train_acc: 97.18750 val_loss: 0.32286 val_acc: 92.50000\n",
      "[1, 5690] train_loss: 0.10212 train_acc: 96.87500 val_loss: 0.18415 val_acc: 93.75000\n",
      "[1, 5700] train_loss: 0.10861 train_acc: 96.56250 val_loss: 0.17824 val_acc: 96.25000\n",
      "[1, 5710] train_loss: 0.08609 train_acc: 97.18750 val_loss: 0.45602 val_acc: 88.75000\n",
      "[1, 5720] train_loss: 0.10400 train_acc: 96.56250 val_loss: 0.72432 val_acc: 88.75000\n",
      "[1, 5730] train_loss: 0.11274 train_acc: 97.18750 val_loss: 0.62475 val_acc: 90.00000\n",
      "[1, 5740] train_loss: 0.10231 train_acc: 97.81250 val_loss: 0.59738 val_acc: 90.00000\n",
      "[1, 5750] train_loss: 0.11554 train_acc: 96.87500 val_loss: 0.52019 val_acc: 92.50000\n",
      "[1, 5760] train_loss: 0.03684 train_acc: 98.75000 val_loss: 0.44526 val_acc: 90.00000\n",
      "[1, 5770] train_loss: 0.07493 train_acc: 97.81250 val_loss: 1.09087 val_acc: 82.50000\n",
      "[1, 5780] train_loss: 0.06968 train_acc: 96.87500 val_loss: 0.60305 val_acc: 92.50000\n",
      "[1, 5790] train_loss: 0.11373 train_acc: 97.81250 val_loss: 0.88428 val_acc: 83.75000\n",
      "[1, 5800] train_loss: 0.16081 train_acc: 95.31250 val_loss: 0.18872 val_acc: 97.50000\n",
      "[1, 5810] train_loss: 0.05588 train_acc: 98.43750 val_loss: 0.42631 val_acc: 93.75000\n",
      "[1, 5820] train_loss: 0.08909 train_acc: 96.56250 val_loss: 0.36322 val_acc: 93.75000\n",
      "[1, 5830] train_loss: 0.07455 train_acc: 97.81250 val_loss: 0.43585 val_acc: 91.25000\n",
      "[1, 5840] train_loss: 0.06354 train_acc: 99.06250 val_loss: 0.73253 val_acc: 87.50000\n",
      "[1, 5850] train_loss: 0.08899 train_acc: 97.50000 val_loss: 0.54457 val_acc: 87.50000\n",
      "[1, 5860] train_loss: 0.04095 train_acc: 98.75000 val_loss: 0.50497 val_acc: 88.75000\n",
      "[1, 5870] train_loss: 0.03605 train_acc: 99.37500 val_loss: 0.33014 val_acc: 92.50000\n",
      "[1, 5880] train_loss: 0.08325 train_acc: 97.50000 val_loss: 0.21428 val_acc: 96.25000\n",
      "[1, 5890] train_loss: 0.05923 train_acc: 97.81250 val_loss: 0.43131 val_acc: 93.75000\n",
      "[1, 5900] train_loss: 0.04788 train_acc: 98.12500 val_loss: 0.71830 val_acc: 86.25000\n",
      "[1, 5910] train_loss: 0.09779 train_acc: 95.31250 val_loss: 0.99629 val_acc: 83.75000\n",
      "[1, 5920] train_loss: 0.05681 train_acc: 99.06250 val_loss: 0.85768 val_acc: 90.00000\n",
      "[1, 5930] train_loss: 0.10045 train_acc: 98.12500 val_loss: 0.70134 val_acc: 86.25000\n",
      "[1, 5940] train_loss: 0.09062 train_acc: 98.43750 val_loss: 0.34493 val_acc: 93.75000\n",
      "[1, 5950] train_loss: 0.03472 train_acc: 99.06250 val_loss: 0.44943 val_acc: 87.50000\n",
      "[1, 5960] train_loss: 0.03182 train_acc: 98.75000 val_loss: 0.51912 val_acc: 88.75000\n",
      "[1, 5970] train_loss: 0.07639 train_acc: 98.12500 val_loss: 0.64892 val_acc: 86.25000\n",
      "[1, 5980] train_loss: 0.09550 train_acc: 96.25000 val_loss: 0.32330 val_acc: 87.50000\n",
      "[1, 5990] train_loss: 0.05561 train_acc: 98.12500 val_loss: 0.56619 val_acc: 91.25000\n",
      "[1, 6000] train_loss: 0.05676 train_acc: 98.75000 val_loss: 0.72042 val_acc: 88.75000\n",
      "[1, 6010] train_loss: 0.04816 train_acc: 98.12500 val_loss: 0.53872 val_acc: 90.00000\n",
      "[1, 6020] train_loss: 0.10045 train_acc: 97.18750 val_loss: 0.49629 val_acc: 88.75000\n",
      "[1, 6030] train_loss: 0.05232 train_acc: 99.06250 val_loss: 0.30911 val_acc: 91.25000\n",
      "[1, 6040] train_loss: 0.03768 train_acc: 98.43750 val_loss: 0.62942 val_acc: 88.75000\n",
      "[1, 6050] train_loss: 0.08545 train_acc: 97.81250 val_loss: 0.76031 val_acc: 80.00000\n",
      "[1, 6060] train_loss: 0.09301 train_acc: 97.81250 val_loss: 0.46482 val_acc: 92.50000\n",
      "[1, 6070] train_loss: 0.03292 train_acc: 99.06250 val_loss: 0.32806 val_acc: 93.75000\n",
      "[1, 6080] train_loss: 0.06621 train_acc: 97.50000 val_loss: 0.42072 val_acc: 88.75000\n",
      "[1, 6090] train_loss: 0.08579 train_acc: 97.81250 val_loss: 0.73563 val_acc: 86.25000\n",
      "[1, 6100] train_loss: 0.04009 train_acc: 99.06250 val_loss: 0.38649 val_acc: 93.75000\n",
      "[1, 6110] train_loss: 0.11223 train_acc: 97.81250 val_loss: 0.65288 val_acc: 88.75000\n",
      "[1, 6120] train_loss: 0.09451 train_acc: 97.50000 val_loss: 0.37260 val_acc: 91.25000\n",
      "[1, 6130] train_loss: 0.10745 train_acc: 97.81250 val_loss: 0.46119 val_acc: 93.75000\n",
      "[1, 6140] train_loss: 0.05268 train_acc: 98.75000 val_loss: 0.16221 val_acc: 95.00000\n",
      "[1, 6150] train_loss: 0.03960 train_acc: 98.43750 val_loss: 0.50001 val_acc: 92.50000\n",
      "[1, 6160] train_loss: 0.09580 train_acc: 97.81250 val_loss: 0.41190 val_acc: 92.50000\n",
      "[1, 6170] train_loss: 0.08404 train_acc: 97.50000 val_loss: 0.32971 val_acc: 91.25000\n",
      "[1, 6180] train_loss: 0.13866 train_acc: 96.56250 val_loss: 0.71135 val_acc: 85.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6190] train_loss: 0.08085 train_acc: 96.56250 val_loss: 0.66873 val_acc: 90.00000\n",
      "[1, 6200] train_loss: 0.12019 train_acc: 96.56250 val_loss: 0.29208 val_acc: 95.00000\n",
      "[1, 6210] train_loss: 0.12263 train_acc: 96.56250 val_loss: 0.67765 val_acc: 88.75000\n",
      "[1, 6220] train_loss: 0.12847 train_acc: 95.31250 val_loss: 0.32679 val_acc: 95.00000\n",
      "[1, 6230] train_loss: 0.06612 train_acc: 97.81250 val_loss: 0.60230 val_acc: 92.50000\n",
      "[1, 6240] train_loss: 0.05298 train_acc: 97.81250 val_loss: 0.92414 val_acc: 90.00000\n",
      "[1, 6250] train_loss: 0.10054 train_acc: 97.50000 val_loss: 0.36427 val_acc: 95.00000\n",
      "[1, 6260] train_loss: 0.08922 train_acc: 97.18750 val_loss: 0.70056 val_acc: 87.50000\n",
      "[1, 6270] train_loss: 0.09811 train_acc: 97.18750 val_loss: 0.67727 val_acc: 88.75000\n",
      "[1, 6280] train_loss: 0.04522 train_acc: 98.43750 val_loss: 0.53196 val_acc: 90.00000\n",
      "[1, 6290] train_loss: 0.07073 train_acc: 97.50000 val_loss: 0.35210 val_acc: 92.50000\n",
      "[1, 6300] train_loss: 0.11887 train_acc: 96.87500 val_loss: 0.60991 val_acc: 92.50000\n",
      "[1, 6310] train_loss: 0.14401 train_acc: 95.62500 val_loss: 1.21294 val_acc: 83.75000\n",
      "[1, 6320] train_loss: 0.06283 train_acc: 97.18750 val_loss: 0.76717 val_acc: 85.00000\n",
      "[1, 6330] train_loss: 0.07545 train_acc: 98.75000 val_loss: 0.19702 val_acc: 93.75000\n",
      "[1, 6340] train_loss: 0.07194 train_acc: 98.12500 val_loss: 0.55715 val_acc: 87.50000\n",
      "[1, 6350] train_loss: 0.06154 train_acc: 97.81250 val_loss: 0.76052 val_acc: 85.00000\n",
      "[1, 6360] train_loss: 0.10890 train_acc: 98.12500 val_loss: 0.76032 val_acc: 90.00000\n",
      "[1, 6370] train_loss: 0.06125 train_acc: 98.12500 val_loss: 0.61158 val_acc: 91.25000\n",
      "[1, 6380] train_loss: 0.06605 train_acc: 98.43750 val_loss: 0.59258 val_acc: 91.25000\n",
      "[1, 6390] train_loss: 0.09206 train_acc: 97.50000 val_loss: 0.57456 val_acc: 88.75000\n",
      "[1, 6400] train_loss: 0.06911 train_acc: 98.12500 val_loss: 0.42795 val_acc: 86.25000\n",
      "[1, 6410] train_loss: 0.06279 train_acc: 97.81250 val_loss: 0.53526 val_acc: 92.50000\n",
      "[1, 6420] train_loss: 0.05099 train_acc: 97.81250 val_loss: 0.52522 val_acc: 90.00000\n",
      "[1, 6430] train_loss: 0.06211 train_acc: 98.12500 val_loss: 0.56694 val_acc: 83.75000\n",
      "[1, 6440] train_loss: 0.07970 train_acc: 97.50000 val_loss: 0.55691 val_acc: 86.25000\n",
      "[1, 6450] train_loss: 0.09736 train_acc: 97.18750 val_loss: 0.88670 val_acc: 85.00000\n",
      "[1, 6460] train_loss: 0.05951 train_acc: 97.81250 val_loss: 0.46899 val_acc: 92.50000\n",
      "[1, 6470] train_loss: 0.10071 train_acc: 97.50000 val_loss: 0.46832 val_acc: 93.75000\n",
      "[1, 6480] train_loss: 0.10925 train_acc: 97.50000 val_loss: 0.17624 val_acc: 95.00000\n",
      "[1, 6490] train_loss: 0.04970 train_acc: 97.81250 val_loss: 0.38402 val_acc: 90.00000\n",
      "[1, 6500] train_loss: 0.09270 train_acc: 97.50000 val_loss: 0.88391 val_acc: 87.50000\n",
      "[1, 6510] train_loss: 0.06394 train_acc: 97.81250 val_loss: 0.70965 val_acc: 90.00000\n",
      "[1, 6520] train_loss: 0.06983 train_acc: 97.81250 val_loss: 0.95592 val_acc: 85.00000\n",
      "[1, 6530] train_loss: 0.05436 train_acc: 96.56250 val_loss: 0.57783 val_acc: 92.50000\n",
      "[1, 6540] train_loss: 0.06615 train_acc: 98.43750 val_loss: 0.67075 val_acc: 93.75000\n",
      "[1, 6550] train_loss: 0.09201 train_acc: 97.18750 val_loss: 0.34659 val_acc: 91.25000\n",
      "[1, 6560] train_loss: 0.11025 train_acc: 96.87500 val_loss: 0.04551 val_acc: 97.50000\n",
      "[1, 6570] train_loss: 0.04041 train_acc: 98.43750 val_loss: 0.63408 val_acc: 88.75000\n",
      "[1, 6580] train_loss: 0.14175 train_acc: 96.25000 val_loss: 0.47083 val_acc: 90.00000\n",
      "[1, 6590] train_loss: 0.12333 train_acc: 96.25000 val_loss: 0.29170 val_acc: 92.50000\n",
      "[1, 6600] train_loss: 0.03852 train_acc: 98.43750 val_loss: 0.76538 val_acc: 85.00000\n",
      "[1, 6610] train_loss: 0.10005 train_acc: 96.87500 val_loss: 0.21903 val_acc: 95.00000\n",
      "[1, 6620] train_loss: 0.07810 train_acc: 97.50000 val_loss: 0.75260 val_acc: 90.00000\n",
      "[1, 6630] train_loss: 0.05649 train_acc: 98.12500 val_loss: 0.46793 val_acc: 92.50000\n",
      "[1, 6640] train_loss: 0.06947 train_acc: 98.12500 val_loss: 0.65850 val_acc: 92.50000\n",
      "[1, 6650] train_loss: 0.14185 train_acc: 95.93750 val_loss: 0.21237 val_acc: 96.25000\n",
      "[1, 6660] train_loss: 0.06278 train_acc: 97.81250 val_loss: 0.25357 val_acc: 93.75000\n",
      "[1, 6670] train_loss: 0.11424 train_acc: 97.50000 val_loss: 0.74724 val_acc: 85.00000\n",
      "[1, 6680] train_loss: 0.07218 train_acc: 97.50000 val_loss: 0.58869 val_acc: 87.50000\n",
      "[1, 6690] train_loss: 0.08661 train_acc: 97.18750 val_loss: 0.56214 val_acc: 91.25000\n",
      "[1, 6700] train_loss: 0.09560 train_acc: 97.18750 val_loss: 1.32845 val_acc: 83.75000\n",
      "[1, 6710] train_loss: 0.05839 train_acc: 98.12500 val_loss: 0.67560 val_acc: 88.75000\n",
      "[1, 6720] train_loss: 0.09443 train_acc: 97.18750 val_loss: 0.41902 val_acc: 95.00000\n",
      "[1, 6730] train_loss: 0.05278 train_acc: 98.43750 val_loss: 0.41155 val_acc: 91.25000\n",
      "[1, 6740] train_loss: 0.03421 train_acc: 99.37500 val_loss: 0.94668 val_acc: 88.75000\n",
      "[1, 6750] train_loss: 0.04692 train_acc: 98.43750 val_loss: 0.79854 val_acc: 85.00000\n",
      "[1, 6760] train_loss: 0.02980 train_acc: 99.06250 val_loss: 0.31504 val_acc: 91.25000\n",
      "[1, 6770] train_loss: 0.03991 train_acc: 99.06250 val_loss: 0.55110 val_acc: 88.75000\n",
      "[1, 6780] train_loss: 0.08208 train_acc: 97.18750 val_loss: 0.51782 val_acc: 92.50000\n",
      "[1, 6790] train_loss: 0.07558 train_acc: 96.87500 val_loss: 0.49155 val_acc: 95.00000\n",
      "[1, 6800] train_loss: 0.11575 train_acc: 97.18750 val_loss: 0.74026 val_acc: 91.25000\n",
      "[1, 6810] train_loss: 0.05154 train_acc: 98.43750 val_loss: 1.04569 val_acc: 83.75000\n",
      "[1, 6820] train_loss: 0.06361 train_acc: 97.81250 val_loss: 0.41799 val_acc: 91.25000\n",
      "[1, 6830] train_loss: 0.15730 train_acc: 95.31250 val_loss: 0.55005 val_acc: 90.00000\n",
      "[1, 6840] train_loss: 0.06035 train_acc: 98.75000 val_loss: 0.41593 val_acc: 90.00000\n",
      "[1, 6850] train_loss: 0.08062 train_acc: 97.50000 val_loss: 0.87004 val_acc: 92.50000\n",
      "[1, 6860] train_loss: 0.06803 train_acc: 98.12500 val_loss: 0.84103 val_acc: 87.50000\n",
      "[1, 6870] train_loss: 0.08803 train_acc: 97.50000 val_loss: 0.36984 val_acc: 90.00000\n",
      "[1, 6880] train_loss: 0.09744 train_acc: 97.18750 val_loss: 0.29863 val_acc: 93.75000\n",
      "[1, 6890] train_loss: 0.11082 train_acc: 96.87500 val_loss: 0.36090 val_acc: 93.75000\n",
      "[1, 6900] train_loss: 0.08823 train_acc: 97.18750 val_loss: 0.53085 val_acc: 88.75000\n",
      "[1, 6910] train_loss: 0.09298 train_acc: 97.50000 val_loss: 0.56216 val_acc: 87.50000\n",
      "[1, 6920] train_loss: 0.06936 train_acc: 97.81250 val_loss: 0.38165 val_acc: 90.00000\n",
      "[1, 6930] train_loss: 0.09150 train_acc: 97.18750 val_loss: 0.58391 val_acc: 90.00000\n",
      "[1, 6940] train_loss: 0.06974 train_acc: 98.43750 val_loss: 0.38492 val_acc: 86.25000\n",
      "[1, 6950] train_loss: 0.09950 train_acc: 98.12500 val_loss: 0.71173 val_acc: 90.00000\n",
      "[1, 6960] train_loss: 0.05510 train_acc: 97.18750 val_loss: 0.55245 val_acc: 91.25000\n",
      "[1, 6970] train_loss: 0.05041 train_acc: 98.43750 val_loss: 0.71905 val_acc: 90.00000\n",
      "[1, 6980] train_loss: 0.10004 train_acc: 98.12500 val_loss: 0.32181 val_acc: 91.25000\n",
      "[1, 6990] train_loss: 0.03569 train_acc: 98.12500 val_loss: 0.46956 val_acc: 90.00000\n",
      "[1, 7000] train_loss: 0.05393 train_acc: 97.81250 val_loss: 0.34353 val_acc: 93.75000\n",
      "[1, 7010] train_loss: 0.06373 train_acc: 98.75000 val_loss: 0.30233 val_acc: 95.00000\n",
      "[1, 7020] train_loss: 0.06776 train_acc: 97.81250 val_loss: 0.45066 val_acc: 92.50000\n",
      "[1, 7030] train_loss: 0.03288 train_acc: 98.75000 val_loss: 0.56095 val_acc: 87.50000\n",
      "[1, 7040] train_loss: 0.10357 train_acc: 97.18750 val_loss: 0.23420 val_acc: 90.00000\n",
      "[1, 7050] train_loss: 0.10284 train_acc: 97.18750 val_loss: 0.20074 val_acc: 95.00000\n",
      "[1, 7060] train_loss: 0.05703 train_acc: 98.43750 val_loss: 0.50821 val_acc: 90.00000\n",
      "[1, 7070] train_loss: 0.17292 train_acc: 96.56250 val_loss: 0.34568 val_acc: 91.25000\n",
      "[1, 7080] train_loss: 0.07981 train_acc: 98.12500 val_loss: 0.50047 val_acc: 87.50000\n",
      "[1, 7090] train_loss: 0.04536 train_acc: 98.43750 val_loss: 0.65497 val_acc: 86.25000\n",
      "[1, 7100] train_loss: 0.09390 train_acc: 97.50000 val_loss: 0.60437 val_acc: 83.75000\n",
      "[1, 7110] train_loss: 0.07833 train_acc: 97.18750 val_loss: 0.39218 val_acc: 93.75000\n",
      "[1, 7120] train_loss: 0.08332 train_acc: 97.81250 val_loss: 0.54254 val_acc: 90.00000\n",
      "[1, 7130] train_loss: 0.06115 train_acc: 98.12500 val_loss: 0.86200 val_acc: 86.25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 7140] train_loss: 0.15932 train_acc: 96.25000 val_loss: 0.76387 val_acc: 87.50000\n",
      "[1, 7150] train_loss: 0.09684 train_acc: 97.18750 val_loss: 0.56516 val_acc: 91.25000\n",
      "[1, 7160] train_loss: 0.13244 train_acc: 96.25000 val_loss: 0.73535 val_acc: 90.00000\n",
      "[1, 7170] train_loss: 0.04005 train_acc: 98.43750 val_loss: 0.85653 val_acc: 86.25000\n",
      "[1, 7180] train_loss: 0.11905 train_acc: 96.87500 val_loss: 0.58561 val_acc: 92.50000\n",
      "[1, 7190] train_loss: 0.07999 train_acc: 97.50000 val_loss: 0.43475 val_acc: 91.25000\n",
      "[1, 7200] train_loss: 0.07756 train_acc: 96.87500 val_loss: 0.75588 val_acc: 90.00000\n",
      "[1, 7210] train_loss: 0.06928 train_acc: 97.81250 val_loss: 0.61279 val_acc: 95.00000\n",
      "[1, 7220] train_loss: 0.05318 train_acc: 97.18750 val_loss: 0.43366 val_acc: 90.00000\n",
      "[1, 7230] train_loss: 0.04148 train_acc: 98.75000 val_loss: 0.88317 val_acc: 85.00000\n",
      "[1, 7240] train_loss: 0.07055 train_acc: 97.50000 val_loss: 0.43346 val_acc: 91.25000\n",
      "[1, 7250] train_loss: 0.05587 train_acc: 97.50000 val_loss: 0.68408 val_acc: 87.50000\n",
      "[1, 7260] train_loss: 0.09544 train_acc: 97.81250 val_loss: 0.29591 val_acc: 93.75000\n",
      "[1, 7270] train_loss: 0.04706 train_acc: 98.12500 val_loss: 0.66162 val_acc: 87.50000\n",
      "[1, 7280] train_loss: 0.13574 train_acc: 96.25000 val_loss: 0.42021 val_acc: 90.00000\n",
      "[1, 7290] train_loss: 0.09750 train_acc: 97.50000 val_loss: 0.46334 val_acc: 92.50000\n",
      "[1, 7300] train_loss: 0.10559 train_acc: 96.87500 val_loss: 0.26765 val_acc: 92.50000\n",
      "[1, 7310] train_loss: 0.08128 train_acc: 98.12500 val_loss: 0.37173 val_acc: 91.25000\n",
      "[1, 7320] train_loss: 0.02809 train_acc: 98.75000 val_loss: 0.36153 val_acc: 96.25000\n",
      "[1, 7330] train_loss: 0.06814 train_acc: 98.12500 val_loss: 0.40805 val_acc: 90.00000\n",
      "[1, 7340] train_loss: 0.10259 train_acc: 97.18750 val_loss: 0.34545 val_acc: 92.50000\n",
      "[1, 7350] train_loss: 0.09823 train_acc: 97.18750 val_loss: 0.21351 val_acc: 93.75000\n",
      "[1, 7360] train_loss: 0.16601 train_acc: 95.62500 val_loss: 0.66736 val_acc: 83.75000\n",
      "[1, 7370] train_loss: 0.09535 train_acc: 96.25000 val_loss: 0.58663 val_acc: 92.50000\n",
      "[1, 7380] train_loss: 0.07276 train_acc: 97.81250 val_loss: 0.55845 val_acc: 86.25000\n",
      "[1, 7390] train_loss: 0.08042 train_acc: 98.75000 val_loss: 0.67018 val_acc: 93.75000\n",
      "[1, 7400] train_loss: 0.10815 train_acc: 95.93750 val_loss: 0.18366 val_acc: 95.00000\n",
      "[1, 7410] train_loss: 0.06188 train_acc: 98.43750 val_loss: 0.59937 val_acc: 90.00000\n",
      "[1, 7420] train_loss: 0.05463 train_acc: 98.43750 val_loss: 0.52233 val_acc: 91.25000\n",
      "[1, 7430] train_loss: 0.08590 train_acc: 96.87500 val_loss: 0.57833 val_acc: 88.75000\n",
      "[1, 7440] train_loss: 0.13595 train_acc: 96.87500 val_loss: 0.59704 val_acc: 88.75000\n",
      "[1, 7450] train_loss: 0.09089 train_acc: 98.12500 val_loss: 0.38855 val_acc: 91.25000\n",
      "[1, 7460] train_loss: 0.07170 train_acc: 98.12500 val_loss: 0.66816 val_acc: 88.75000\n",
      "[1, 7470] train_loss: 0.09708 train_acc: 97.81250 val_loss: 0.91148 val_acc: 86.25000\n",
      "[1, 7480] train_loss: 0.05741 train_acc: 98.43750 val_loss: 0.98193 val_acc: 85.00000\n",
      "[1, 7490] train_loss: 0.12690 train_acc: 95.62500 val_loss: 0.22846 val_acc: 92.50000\n",
      "[1, 7500] train_loss: 0.05635 train_acc: 97.81250 val_loss: 0.48657 val_acc: 91.25000\n",
      "[1, 7510] train_loss: 0.11726 train_acc: 97.18750 val_loss: 0.81325 val_acc: 88.75000\n",
      "[1, 7520] train_loss: 0.09086 train_acc: 97.18750 val_loss: 0.24462 val_acc: 96.25000\n",
      "[1, 7530] train_loss: 0.09666 train_acc: 98.12500 val_loss: 0.67050 val_acc: 91.25000\n",
      "[1, 7540] train_loss: 0.07462 train_acc: 97.81250 val_loss: 0.89957 val_acc: 86.25000\n",
      "[1, 7550] train_loss: 0.08078 train_acc: 97.81250 val_loss: 0.73110 val_acc: 92.50000\n",
      "[1, 7560] train_loss: 0.10434 train_acc: 97.50000 val_loss: 0.35777 val_acc: 87.50000\n",
      "[1, 7570] train_loss: 0.07735 train_acc: 97.81250 val_loss: 0.61519 val_acc: 87.50000\n",
      "[1, 7580] train_loss: 0.10440 train_acc: 97.18750 val_loss: 1.20247 val_acc: 80.00000\n",
      "[1, 7590] train_loss: 0.12345 train_acc: 98.12500 val_loss: 0.34922 val_acc: 93.75000\n",
      "[1, 7600] train_loss: 0.07658 train_acc: 97.50000 val_loss: 0.59435 val_acc: 90.00000\n",
      "[1, 7610] train_loss: 0.06000 train_acc: 98.12500 val_loss: 0.47697 val_acc: 93.75000\n",
      "[1, 7620] train_loss: 0.05033 train_acc: 98.12500 val_loss: 0.50525 val_acc: 87.50000\n",
      "[1, 7630] train_loss: 0.09840 train_acc: 97.81250 val_loss: 0.72474 val_acc: 88.75000\n",
      "[1, 7640] train_loss: 0.08293 train_acc: 97.81250 val_loss: 0.60049 val_acc: 90.00000\n",
      "[1, 7650] train_loss: 0.09635 train_acc: 97.81250 val_loss: 0.42126 val_acc: 88.75000\n",
      "[1, 7660] train_loss: 0.09901 train_acc: 97.18750 val_loss: 0.86339 val_acc: 85.00000\n",
      "[1, 7670] train_loss: 0.04964 train_acc: 97.81250 val_loss: 0.37063 val_acc: 91.25000\n",
      "[1, 7680] train_loss: 0.07888 train_acc: 97.18750 val_loss: 0.74969 val_acc: 87.50000\n",
      "[1, 7690] train_loss: 0.06405 train_acc: 97.81250 val_loss: 0.49957 val_acc: 92.50000\n",
      "[1, 7700] train_loss: 0.11424 train_acc: 96.56250 val_loss: 0.77372 val_acc: 91.25000\n",
      "[1, 7710] train_loss: 0.27076 train_acc: 94.37500 val_loss: 0.43079 val_acc: 91.25000\n",
      "[1, 7720] train_loss: 0.14915 train_acc: 98.12500 val_loss: 0.78272 val_acc: 85.00000\n",
      "[1, 7730] train_loss: 0.06893 train_acc: 97.81250 val_loss: 0.53710 val_acc: 86.25000\n",
      "[1, 7740] train_loss: 0.07738 train_acc: 97.81250 val_loss: 0.83570 val_acc: 85.00000\n",
      "[1, 7750] train_loss: 0.13240 train_acc: 96.25000 val_loss: 0.52046 val_acc: 88.75000\n",
      "[1, 7760] train_loss: 0.12590 train_acc: 96.87500 val_loss: 0.64261 val_acc: 88.75000\n",
      "[1, 7770] train_loss: 0.11583 train_acc: 97.50000 val_loss: 0.33127 val_acc: 92.50000\n",
      "[1, 7780] train_loss: 0.16211 train_acc: 95.93750 val_loss: 0.56004 val_acc: 88.75000\n",
      "[1, 7790] train_loss: 0.09242 train_acc: 96.87500 val_loss: 0.41201 val_acc: 92.50000\n",
      "[1, 7800] train_loss: 0.14220 train_acc: 96.25000 val_loss: 0.71235 val_acc: 88.75000\n",
      "[1, 7810] train_loss: 0.04933 train_acc: 98.43750 val_loss: 0.27408 val_acc: 93.75000\n",
      "[1, 7820] train_loss: 0.10895 train_acc: 96.25000 val_loss: 0.48363 val_acc: 91.25000\n",
      "[1, 7830] train_loss: 0.08777 train_acc: 97.18750 val_loss: 0.65718 val_acc: 87.50000\n",
      "[1, 7840] train_loss: 0.04856 train_acc: 98.43750 val_loss: 0.41776 val_acc: 93.75000\n",
      "[1, 7850] train_loss: 0.11205 train_acc: 97.50000 val_loss: 0.57223 val_acc: 90.00000\n",
      "[1, 7860] train_loss: 0.03757 train_acc: 99.06250 val_loss: 0.70954 val_acc: 86.25000\n",
      "[1, 7870] train_loss: 0.05503 train_acc: 98.12500 val_loss: 0.73524 val_acc: 86.25000\n",
      "[1, 7880] train_loss: 0.18324 train_acc: 95.62500 val_loss: 0.34429 val_acc: 88.75000\n",
      "[1, 7890] train_loss: 0.06599 train_acc: 97.81250 val_loss: 0.70342 val_acc: 90.00000\n",
      "[1, 7900] train_loss: 0.12745 train_acc: 97.18750 val_loss: 0.50643 val_acc: 93.75000\n",
      "[1, 7910] train_loss: 0.09847 train_acc: 97.81250 val_loss: 0.07311 val_acc: 97.50000\n",
      "[1, 7920] train_loss: 0.07597 train_acc: 97.18750 val_loss: 0.61580 val_acc: 88.75000\n",
      "[1, 7930] train_loss: 0.04545 train_acc: 97.81250 val_loss: 0.53074 val_acc: 91.25000\n",
      "[1, 7940] train_loss: 0.09680 train_acc: 96.87500 val_loss: 0.70120 val_acc: 90.00000\n",
      "[1, 7950] train_loss: 0.18160 train_acc: 96.87500 val_loss: 0.55329 val_acc: 86.25000\n",
      "[1, 7960] train_loss: 0.09808 train_acc: 96.25000 val_loss: 0.27685 val_acc: 91.25000\n",
      "[1, 7970] train_loss: 0.07686 train_acc: 98.75000 val_loss: 0.35643 val_acc: 90.00000\n",
      "[1, 7980] train_loss: 0.11671 train_acc: 96.87500 val_loss: 0.71529 val_acc: 88.75000\n",
      "[1, 7990] train_loss: 0.09213 train_acc: 98.12500 val_loss: 0.71653 val_acc: 90.00000\n",
      "[1, 8000] train_loss: 0.10267 train_acc: 97.50000 val_loss: 0.80766 val_acc: 85.00000\n",
      "[1, 8010] train_loss: 0.11820 train_acc: 97.18750 val_loss: 0.45820 val_acc: 92.50000\n",
      "[1, 8020] train_loss: 0.07792 train_acc: 98.12500 val_loss: 0.56233 val_acc: 91.25000\n",
      "[1, 8030] train_loss: 0.12782 train_acc: 95.62500 val_loss: 0.68869 val_acc: 90.00000\n",
      "[1, 8040] train_loss: 0.07967 train_acc: 98.12500 val_loss: 0.58348 val_acc: 92.50000\n",
      "[1, 8050] train_loss: 0.15599 train_acc: 95.93750 val_loss: 0.27013 val_acc: 93.75000\n",
      "[1, 8060] train_loss: 0.07294 train_acc: 97.18750 val_loss: 0.69081 val_acc: 88.75000\n",
      "[1, 8070] train_loss: 0.13726 train_acc: 96.25000 val_loss: 0.84075 val_acc: 81.25000\n",
      "[1, 8080] train_loss: 0.11143 train_acc: 96.56250 val_loss: 0.90636 val_acc: 85.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8090] train_loss: 0.05104 train_acc: 97.18750 val_loss: 0.26435 val_acc: 93.75000\n",
      "[1, 8100] train_loss: 0.09012 train_acc: 97.50000 val_loss: 0.46235 val_acc: 91.25000\n",
      "[1, 8110] train_loss: 0.10163 train_acc: 97.18750 val_loss: 0.81362 val_acc: 87.50000\n",
      "[1, 8120] train_loss: 0.08833 train_acc: 96.87500 val_loss: 0.42883 val_acc: 90.00000\n",
      "[1, 8130] train_loss: 0.05850 train_acc: 98.43750 val_loss: 0.54194 val_acc: 90.00000\n",
      "[1, 8140] train_loss: 0.07185 train_acc: 98.12500 val_loss: 0.79994 val_acc: 86.25000\n",
      "[1, 8150] train_loss: 0.06116 train_acc: 98.12500 val_loss: 0.66128 val_acc: 91.25000\n",
      "[1, 8160] train_loss: 0.06748 train_acc: 97.81250 val_loss: 1.15853 val_acc: 82.50000\n",
      "[1, 8170] train_loss: 0.08440 train_acc: 97.81250 val_loss: 0.43003 val_acc: 92.50000\n",
      "[1, 8180] train_loss: 0.05555 train_acc: 98.43750 val_loss: 0.60345 val_acc: 90.00000\n",
      "[1, 8190] train_loss: 0.06694 train_acc: 98.12500 val_loss: 0.16475 val_acc: 95.00000\n",
      "[1, 8200] train_loss: 0.12599 train_acc: 97.81250 val_loss: 0.65445 val_acc: 88.75000\n",
      "[1, 8210] train_loss: 0.05717 train_acc: 98.12500 val_loss: 0.17555 val_acc: 95.00000\n",
      "[1, 8220] train_loss: 0.12332 train_acc: 97.50000 val_loss: 0.44495 val_acc: 91.25000\n",
      "[1, 8230] train_loss: 0.04229 train_acc: 98.43750 val_loss: 0.70353 val_acc: 90.00000\n",
      "[1, 8240] train_loss: 0.06337 train_acc: 98.43750 val_loss: 0.45116 val_acc: 87.50000\n",
      "[1, 8250] train_loss: 0.10248 train_acc: 97.81250 val_loss: 0.14270 val_acc: 97.50000\n",
      "[1, 8260] train_loss: 0.09677 train_acc: 97.50000 val_loss: 0.34869 val_acc: 95.00000\n",
      "[1, 8270] train_loss: 0.17148 train_acc: 95.93750 val_loss: 0.60269 val_acc: 90.00000\n",
      "[1, 8280] train_loss: 0.03988 train_acc: 98.43750 val_loss: 0.57088 val_acc: 88.75000\n",
      "[1, 8290] train_loss: 0.11469 train_acc: 97.18750 val_loss: 0.44214 val_acc: 91.25000\n",
      "[1, 8300] train_loss: 0.05072 train_acc: 98.75000 val_loss: 0.46701 val_acc: 91.25000\n",
      "[1, 8310] train_loss: 0.05300 train_acc: 98.12500 val_loss: 0.98960 val_acc: 93.75000\n",
      "[1, 8320] train_loss: 0.13109 train_acc: 97.18750 val_loss: 0.70803 val_acc: 90.00000\n",
      "[1, 8330] train_loss: 0.04492 train_acc: 97.50000 val_loss: 0.22520 val_acc: 96.25000\n",
      "[1, 8340] train_loss: 0.05073 train_acc: 97.50000 val_loss: 0.34748 val_acc: 92.50000\n",
      "[1, 8350] train_loss: 0.07198 train_acc: 97.81250 val_loss: 0.97443 val_acc: 87.50000\n",
      "[1, 8360] train_loss: 0.02261 train_acc: 99.06250 val_loss: 0.91988 val_acc: 83.75000\n",
      "[1, 8370] train_loss: 0.07447 train_acc: 97.81250 val_loss: 0.48572 val_acc: 88.75000\n",
      "[1, 8380] train_loss: 0.09804 train_acc: 97.81250 val_loss: 0.66709 val_acc: 90.00000\n",
      "[1, 8390] train_loss: 0.02989 train_acc: 98.43750 val_loss: 0.61761 val_acc: 91.25000\n",
      "[1, 8400] train_loss: 0.03785 train_acc: 98.43750 val_loss: 0.30034 val_acc: 92.50000\n",
      "[1, 8410] train_loss: 0.09025 train_acc: 97.50000 val_loss: 0.39077 val_acc: 92.50000\n",
      "[1, 8420] train_loss: 0.13293 train_acc: 97.50000 val_loss: 0.52795 val_acc: 91.25000\n",
      "[1, 8430] train_loss: 0.08327 train_acc: 97.18750 val_loss: 0.51112 val_acc: 90.00000\n",
      "[1, 8440] train_loss: 0.09853 train_acc: 96.87500 val_loss: 1.00269 val_acc: 88.75000\n",
      "[1, 8450] train_loss: 0.04578 train_acc: 98.75000 val_loss: 0.37365 val_acc: 92.50000\n",
      "[1, 8460] train_loss: 0.11562 train_acc: 98.43750 val_loss: 0.92000 val_acc: 82.50000\n",
      "[1, 8470] train_loss: 0.04161 train_acc: 98.43750 val_loss: 0.55693 val_acc: 92.50000\n",
      "[1, 8480] train_loss: 0.09291 train_acc: 97.50000 val_loss: 0.60670 val_acc: 87.50000\n",
      "[1, 8490] train_loss: 0.04624 train_acc: 98.43750 val_loss: 0.46923 val_acc: 91.25000\n",
      "[1, 8500] train_loss: 0.09071 train_acc: 98.12500 val_loss: 0.30287 val_acc: 92.50000\n",
      "[1, 8510] train_loss: 0.06948 train_acc: 98.43750 val_loss: 0.71983 val_acc: 90.00000\n",
      "[1, 8520] train_loss: 0.07147 train_acc: 97.81250 val_loss: 0.57900 val_acc: 88.75000\n",
      "[1, 8530] train_loss: 0.05185 train_acc: 98.12500 val_loss: 0.21293 val_acc: 95.00000\n",
      "[1, 8540] train_loss: 0.08813 train_acc: 97.50000 val_loss: 0.30049 val_acc: 92.50000\n",
      "[1, 8550] train_loss: 0.06392 train_acc: 98.12500 val_loss: 0.51404 val_acc: 92.50000\n",
      "[1, 8560] train_loss: 0.05843 train_acc: 98.12500 val_loss: 0.43662 val_acc: 95.00000\n",
      "[1, 8570] train_loss: 0.07057 train_acc: 97.81250 val_loss: 0.81002 val_acc: 87.50000\n",
      "[1, 8580] train_loss: 0.04730 train_acc: 98.43750 val_loss: 0.84793 val_acc: 87.50000\n",
      "[1, 8590] train_loss: 0.07745 train_acc: 97.81250 val_loss: 0.40592 val_acc: 88.75000\n",
      "[1, 8600] train_loss: 0.12409 train_acc: 96.56250 val_loss: 0.50550 val_acc: 90.00000\n",
      "[1, 8610] train_loss: 0.07886 train_acc: 99.06250 val_loss: 0.37794 val_acc: 91.25000\n",
      "[1, 8620] train_loss: 0.11474 train_acc: 97.18750 val_loss: 0.38189 val_acc: 92.50000\n",
      "[1, 8630] train_loss: 0.04090 train_acc: 98.75000 val_loss: 0.36597 val_acc: 90.00000\n",
      "[1, 8640] train_loss: 0.10694 train_acc: 97.50000 val_loss: 0.48608 val_acc: 92.50000\n",
      "[1, 8650] train_loss: 0.06910 train_acc: 97.81250 val_loss: 0.71434 val_acc: 88.75000\n",
      "[1, 8660] train_loss: 0.04938 train_acc: 98.43750 val_loss: 0.36310 val_acc: 91.25000\n",
      "[1, 8670] train_loss: 0.06253 train_acc: 97.50000 val_loss: 0.22254 val_acc: 93.75000\n",
      "[1, 8680] train_loss: 0.05624 train_acc: 99.06250 val_loss: 0.70547 val_acc: 90.00000\n",
      "[1, 8690] train_loss: 0.01469 train_acc: 99.37500 val_loss: 0.69339 val_acc: 90.00000\n",
      "[1, 8700] train_loss: 0.08906 train_acc: 97.81250 val_loss: 0.17286 val_acc: 93.75000\n",
      "[1, 8710] train_loss: 0.10265 train_acc: 97.50000 val_loss: 0.48753 val_acc: 90.00000\n",
      "[1, 8720] train_loss: 0.04221 train_acc: 98.12500 val_loss: 1.04008 val_acc: 86.25000\n",
      "[1, 8730] train_loss: 0.03240 train_acc: 99.06250 val_loss: 0.16636 val_acc: 95.00000\n",
      "[1, 8740] train_loss: 0.09609 train_acc: 98.12500 val_loss: 0.66388 val_acc: 87.50000\n",
      "[1, 8750] train_loss: 0.12588 train_acc: 97.50000 val_loss: 0.43311 val_acc: 92.50000\n",
      "[1, 8760] train_loss: 0.04446 train_acc: 97.81250 val_loss: 0.24415 val_acc: 93.75000\n",
      "[1, 8770] train_loss: 0.03530 train_acc: 98.43750 val_loss: 0.57870 val_acc: 93.75000\n",
      "[1, 8780] train_loss: 0.02289 train_acc: 99.37500 val_loss: 0.65822 val_acc: 90.00000\n",
      "[1, 8790] train_loss: 0.12268 train_acc: 97.18750 val_loss: 0.14633 val_acc: 95.00000\n",
      "[1, 8800] train_loss: 0.03561 train_acc: 98.75000 val_loss: 0.63680 val_acc: 90.00000\n",
      "[1, 8810] train_loss: 0.03443 train_acc: 99.06250 val_loss: 0.72670 val_acc: 86.25000\n",
      "[1, 8820] train_loss: 0.10383 train_acc: 97.81250 val_loss: 0.71495 val_acc: 83.75000\n",
      "[1, 8830] train_loss: 0.08500 train_acc: 98.43750 val_loss: 0.44582 val_acc: 92.50000\n",
      "[1, 8840] train_loss: 0.05714 train_acc: 98.12500 val_loss: 0.49287 val_acc: 91.25000\n",
      "[1, 8850] train_loss: 0.05392 train_acc: 98.43750 val_loss: 0.51815 val_acc: 92.50000\n",
      "[1, 8860] train_loss: 0.04463 train_acc: 98.43750 val_loss: 0.24247 val_acc: 92.50000\n",
      "[1, 8870] train_loss: 0.03513 train_acc: 98.12500 val_loss: 0.73662 val_acc: 87.50000\n",
      "[1, 8880] train_loss: 0.00957 train_acc: 99.68750 val_loss: 0.36568 val_acc: 92.50000\n",
      "[1, 8890] train_loss: 0.05532 train_acc: 97.81250 val_loss: 0.51424 val_acc: 95.00000\n",
      "[1, 8900] train_loss: 0.04388 train_acc: 98.75000 val_loss: 0.80616 val_acc: 87.50000\n",
      "[1, 8910] train_loss: 0.06609 train_acc: 99.37500 val_loss: 0.57123 val_acc: 87.50000\n",
      "[1, 8920] train_loss: 0.09306 train_acc: 97.81250 val_loss: 0.49095 val_acc: 91.25000\n",
      "[1, 8930] train_loss: 0.08334 train_acc: 97.50000 val_loss: 0.43709 val_acc: 88.75000\n",
      "[1, 8940] train_loss: 0.04955 train_acc: 98.12500 val_loss: 0.31021 val_acc: 95.00000\n",
      "[1, 8950] train_loss: 0.09362 train_acc: 97.50000 val_loss: 0.89103 val_acc: 88.75000\n",
      "[1, 8960] train_loss: 0.06037 train_acc: 99.06250 val_loss: 0.48921 val_acc: 88.75000\n",
      "[1, 8970] train_loss: 0.04255 train_acc: 98.43750 val_loss: 0.50595 val_acc: 91.25000\n",
      "[1, 8980] train_loss: 0.06323 train_acc: 97.18750 val_loss: 0.45155 val_acc: 92.50000\n",
      "[1, 8990] train_loss: 0.11525 train_acc: 97.81250 val_loss: 0.24992 val_acc: 92.50000\n",
      "[1, 9000] train_loss: 0.08661 train_acc: 97.18750 val_loss: 0.30034 val_acc: 93.75000\n",
      "[1, 9010] train_loss: 0.04334 train_acc: 98.75000 val_loss: 0.57627 val_acc: 88.75000\n",
      "[1, 9020] train_loss: 0.05672 train_acc: 98.12500 val_loss: 0.64794 val_acc: 90.00000\n",
      "[1, 9030] train_loss: 0.04650 train_acc: 97.81250 val_loss: 0.51383 val_acc: 90.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9040] train_loss: 0.06624 train_acc: 97.50000 val_loss: 0.61180 val_acc: 87.50000\n",
      "[1, 9050] train_loss: 0.13126 train_acc: 96.56250 val_loss: 0.52820 val_acc: 91.25000\n",
      "[1, 9060] train_loss: 0.05418 train_acc: 97.50000 val_loss: 0.55368 val_acc: 93.75000\n",
      "[1, 9070] train_loss: 0.06393 train_acc: 98.12500 val_loss: 0.62435 val_acc: 92.50000\n",
      "[1, 9080] train_loss: 0.04212 train_acc: 99.06250 val_loss: 0.61753 val_acc: 88.75000\n",
      "[1, 9090] train_loss: 0.06999 train_acc: 96.87500 val_loss: 0.32923 val_acc: 95.00000\n",
      "[1, 9100] train_loss: 0.05887 train_acc: 98.12500 val_loss: 0.66588 val_acc: 88.75000\n",
      "[1, 9110] train_loss: 0.08105 train_acc: 97.18750 val_loss: 0.30046 val_acc: 95.00000\n",
      "[1, 9120] train_loss: 0.08069 train_acc: 98.12500 val_loss: 0.29381 val_acc: 95.00000\n",
      "[1, 9130] train_loss: 0.07858 train_acc: 97.18750 val_loss: 0.95169 val_acc: 82.50000\n",
      "[1, 9140] train_loss: 0.09405 train_acc: 97.81250 val_loss: 0.73151 val_acc: 88.75000\n",
      "[1, 9150] train_loss: 0.04897 train_acc: 98.43750 val_loss: 0.59276 val_acc: 87.50000\n",
      "[1, 9160] train_loss: 0.04810 train_acc: 97.81250 val_loss: 0.34700 val_acc: 90.00000\n",
      "[1, 9170] train_loss: 0.12405 train_acc: 96.56250 val_loss: 0.40749 val_acc: 88.75000\n",
      "[1, 9180] train_loss: 0.05928 train_acc: 98.43750 val_loss: 0.39746 val_acc: 91.25000\n",
      "[1, 9190] train_loss: 0.02907 train_acc: 99.06250 val_loss: 0.81577 val_acc: 90.00000\n",
      "[1, 9200] train_loss: 0.11985 train_acc: 96.56250 val_loss: 0.24741 val_acc: 93.75000\n",
      "[1, 9210] train_loss: 0.10443 train_acc: 97.18750 val_loss: 0.58268 val_acc: 87.50000\n",
      "[1, 9220] train_loss: 0.12947 train_acc: 96.87500 val_loss: 0.67922 val_acc: 90.00000\n",
      "[1, 9230] train_loss: 0.14567 train_acc: 96.87500 val_loss: 0.34989 val_acc: 92.50000\n",
      "[1, 9240] train_loss: 0.08725 train_acc: 97.50000 val_loss: 0.65443 val_acc: 87.50000\n",
      "[1, 9250] train_loss: 0.13917 train_acc: 96.56250 val_loss: 0.51379 val_acc: 87.50000\n",
      "[1, 9260] train_loss: 0.09778 train_acc: 97.18750 val_loss: 0.60975 val_acc: 86.25000\n",
      "[1, 9270] train_loss: 0.10240 train_acc: 96.87500 val_loss: 0.54753 val_acc: 88.75000\n",
      "[1, 9280] train_loss: 0.08217 train_acc: 98.12500 val_loss: 1.07708 val_acc: 85.00000\n",
      "[1, 9290] train_loss: 0.07281 train_acc: 97.81250 val_loss: 0.50806 val_acc: 90.00000\n",
      "[1, 9300] train_loss: 0.09174 train_acc: 97.18750 val_loss: 0.44667 val_acc: 88.75000\n",
      "[1, 9310] train_loss: 0.04507 train_acc: 98.75000 val_loss: 0.39429 val_acc: 91.25000\n",
      "[1, 9320] train_loss: 0.06007 train_acc: 98.12500 val_loss: 0.72053 val_acc: 88.75000\n",
      "[1, 9330] train_loss: 0.07618 train_acc: 97.81250 val_loss: 0.60953 val_acc: 90.00000\n",
      "[1, 9340] train_loss: 0.02885 train_acc: 99.06250 val_loss: 0.71052 val_acc: 90.00000\n",
      "[1, 9350] train_loss: 0.01956 train_acc: 99.06250 val_loss: 0.54791 val_acc: 88.75000\n",
      "[1, 9360] train_loss: 0.06216 train_acc: 98.12500 val_loss: 0.77620 val_acc: 87.50000\n",
      "[1, 9370] train_loss: 0.07297 train_acc: 98.12500 val_loss: 0.26326 val_acc: 95.00000\n",
      "[1, 9380] train_loss: 0.06026 train_acc: 98.12500 val_loss: 0.76903 val_acc: 87.50000\n",
      "[1, 9390] train_loss: 0.11272 train_acc: 97.50000 val_loss: 0.90030 val_acc: 86.25000\n",
      "[1, 9400] train_loss: 0.09966 train_acc: 97.81250 val_loss: 0.37408 val_acc: 92.50000\n",
      "[1, 9410] train_loss: 0.06183 train_acc: 98.75000 val_loss: 0.53372 val_acc: 91.25000\n",
      "[1, 9420] train_loss: 0.04404 train_acc: 98.75000 val_loss: 0.33289 val_acc: 92.50000\n",
      "[1, 9430] train_loss: 0.06836 train_acc: 98.43750 val_loss: 0.29544 val_acc: 95.00000\n",
      "[1, 9440] train_loss: 0.04635 train_acc: 98.43750 val_loss: 0.89212 val_acc: 86.25000\n",
      "[1, 9450] train_loss: 0.03834 train_acc: 98.75000 val_loss: 0.36917 val_acc: 93.75000\n",
      "[1, 9460] train_loss: 0.04330 train_acc: 98.43750 val_loss: 0.41392 val_acc: 91.25000\n",
      "[1, 9470] train_loss: 0.06063 train_acc: 98.75000 val_loss: 0.48259 val_acc: 91.25000\n",
      "[1, 9480] train_loss: 0.05000 train_acc: 98.75000 val_loss: 0.24272 val_acc: 92.50000\n",
      "[1, 9490] train_loss: 0.06701 train_acc: 98.12500 val_loss: 0.53523 val_acc: 92.50000\n",
      "[1, 9500] train_loss: 0.11309 train_acc: 97.81250 val_loss: 0.39014 val_acc: 90.00000\n",
      "[1, 9510] train_loss: 0.03059 train_acc: 99.37500 val_loss: 0.26062 val_acc: 92.50000\n",
      "[1, 9520] train_loss: 0.07715 train_acc: 98.43750 val_loss: 0.48449 val_acc: 91.25000\n",
      "[1, 9530] train_loss: 0.08333 train_acc: 98.75000 val_loss: 0.72008 val_acc: 90.00000\n",
      "[1, 9540] train_loss: 0.05371 train_acc: 97.81250 val_loss: 0.39738 val_acc: 92.50000\n",
      "[1, 9550] train_loss: 0.05669 train_acc: 97.50000 val_loss: 0.37242 val_acc: 91.25000\n",
      "[1, 9560] train_loss: 0.09480 train_acc: 98.12500 val_loss: 0.47275 val_acc: 91.25000\n",
      "[1, 9570] train_loss: 0.05677 train_acc: 98.43750 val_loss: 0.38630 val_acc: 91.25000\n",
      "[1, 9580] train_loss: 0.06991 train_acc: 97.50000 val_loss: 0.47901 val_acc: 92.50000\n",
      "[1, 9590] train_loss: 0.03732 train_acc: 99.37500 val_loss: 0.22801 val_acc: 95.00000\n",
      "[1, 9600] train_loss: 0.07662 train_acc: 98.12500 val_loss: 0.77017 val_acc: 82.50000\n",
      "[1, 9610] train_loss: 0.08342 train_acc: 97.81250 val_loss: 1.07279 val_acc: 83.75000\n",
      "[1, 9620] train_loss: 0.07166 train_acc: 97.50000 val_loss: 0.32434 val_acc: 93.75000\n",
      "[1, 9630] train_loss: 0.09609 train_acc: 97.18750 val_loss: 0.28511 val_acc: 92.50000\n",
      "[1, 9640] train_loss: 0.13711 train_acc: 96.56250 val_loss: 0.51553 val_acc: 92.50000\n",
      "[1, 9650] train_loss: 0.07606 train_acc: 97.81250 val_loss: 0.17031 val_acc: 97.50000\n",
      "[1, 9660] train_loss: 0.04893 train_acc: 98.75000 val_loss: 0.67498 val_acc: 83.75000\n",
      "[1, 9670] train_loss: 0.08001 train_acc: 97.18750 val_loss: 0.49571 val_acc: 92.50000\n",
      "[1, 9680] train_loss: 0.04687 train_acc: 99.06250 val_loss: 0.44339 val_acc: 91.25000\n",
      "[1, 9690] train_loss: 0.08387 train_acc: 96.56250 val_loss: 0.72631 val_acc: 91.25000\n",
      "[1, 9700] train_loss: 0.11499 train_acc: 97.18750 val_loss: 0.54614 val_acc: 87.50000\n",
      "[1, 9710] train_loss: 0.05811 train_acc: 98.12500 val_loss: 0.36728 val_acc: 90.00000\n",
      "[1, 9720] train_loss: 0.04906 train_acc: 98.43750 val_loss: 0.50840 val_acc: 88.75000\n",
      "[1, 9730] train_loss: 0.09982 train_acc: 97.81250 val_loss: 0.88592 val_acc: 87.50000\n",
      "[1, 9740] train_loss: 0.08441 train_acc: 97.81250 val_loss: 0.70431 val_acc: 87.50000\n",
      "[1, 9750] train_loss: 0.06499 train_acc: 98.12500 val_loss: 0.68663 val_acc: 86.25000\n",
      "[1, 9760] train_loss: 0.07233 train_acc: 97.81250 val_loss: 0.65082 val_acc: 90.00000\n",
      "[1, 9770] train_loss: 0.05565 train_acc: 98.12500 val_loss: 0.26846 val_acc: 92.50000\n",
      "[1, 9780] train_loss: 0.12491 train_acc: 96.87500 val_loss: 0.69617 val_acc: 91.25000\n",
      "[1, 9790] train_loss: 0.08867 train_acc: 98.43750 val_loss: 0.77832 val_acc: 87.50000\n",
      "[1, 9800] train_loss: 0.10219 train_acc: 97.50000 val_loss: 0.78593 val_acc: 87.50000\n",
      "[1, 9810] train_loss: 0.08266 train_acc: 98.12500 val_loss: 0.55264 val_acc: 86.25000\n",
      "[1, 9820] train_loss: 0.07333 train_acc: 97.81250 val_loss: 0.34558 val_acc: 95.00000\n",
      "[1, 9830] train_loss: 0.07358 train_acc: 97.50000 val_loss: 0.46631 val_acc: 93.75000\n",
      "[1, 9840] train_loss: 0.03829 train_acc: 98.43750 val_loss: 0.81598 val_acc: 87.50000\n",
      "[1, 9850] train_loss: 0.11582 train_acc: 96.87500 val_loss: 0.74398 val_acc: 86.25000\n",
      "[1, 9860] train_loss: 0.06510 train_acc: 97.50000 val_loss: 0.40050 val_acc: 91.25000\n",
      "[1, 9870] train_loss: 0.08251 train_acc: 97.81250 val_loss: 0.72144 val_acc: 85.00000\n",
      "[1, 9880] train_loss: 0.07237 train_acc: 98.12500 val_loss: 0.43426 val_acc: 92.50000\n",
      "[1, 9890] train_loss: 0.08363 train_acc: 97.18750 val_loss: 0.44914 val_acc: 92.50000\n",
      "[1, 9900] train_loss: 0.07153 train_acc: 98.12500 val_loss: 0.21385 val_acc: 93.75000\n",
      "[1, 9910] train_loss: 0.08144 train_acc: 97.50000 val_loss: 0.40709 val_acc: 95.00000\n",
      "[1, 9920] train_loss: 0.04113 train_acc: 97.81250 val_loss: 0.15698 val_acc: 96.25000\n",
      "[1, 9930] train_loss: 0.03139 train_acc: 99.37500 val_loss: 0.72472 val_acc: 88.75000\n",
      "[1, 9940] train_loss: 0.05637 train_acc: 99.37500 val_loss: 1.03832 val_acc: 86.25000\n",
      "[1, 9950] train_loss: 0.04221 train_acc: 98.75000 val_loss: 0.22168 val_acc: 95.00000\n",
      "[1, 9960] train_loss: 0.06753 train_acc: 97.18750 val_loss: 0.75126 val_acc: 88.75000\n"
     ]
    }
   ],
   "source": [
    "tr_loss_lis, tr_acc_lis, val_loss_lis, val_acc_lis = trainCNN (model , lossFunc, optimizer , epochs , train_loader , val_loader , device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model weights\n",
    "torch.save(model.state_dict(), 'Saved weights/CNN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the progress\n",
    "with open(\"CNN Acc and Loss.txt\", \"w\") as file:\n",
    "    file.write(str(tr_loss_lis)+'seperator')\n",
    "    file.write(str(tr_acc_lis)+'seperator')\n",
    "    file.write(str(val_loss_lis)+'seperator')\n",
    "    file.write(str(val_acc_lis)+'seperator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tr_loss_lis = np.array(tr_loss_lis[:-4]).reshape(-1 , 10).mean(axis = 1) / 10\n",
    "new_val_loss_lis = np.array(val_loss_lis[:-4]).reshape(-1 , 10).mean(axis = 1) / 10\n",
    "\n",
    "new_tr_acc_lis = 100 * np.array(tr_acc_lis[:-4]).reshape(-1 , 10).mean(axis = 1) /10\n",
    "new_val_acc_lis = 100 * np.array(val_acc_lis[:-4]).reshape(-1 , 10).mean(axis = 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f0e6ef6668>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize= (10,7))\n",
    "plt.grid(True)\n",
    "plt.plot(np.array(new_tr_loss_lis),'b',label='training loss')\n",
    "plt.plot(np.array(new_val_loss_lis),'orange',label='val loss')\n",
    "plt.xlabel('mini_batches through the training')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f0e3f2ceb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize= (10,7))\n",
    "plt.grid(True)\n",
    "plt.plot(new_tr_acc_lis,'b',label='training accuracy')\n",
    "plt.plot(new_val_acc_lis,'orange',label='val accuracy')\n",
    "plt.xlabel('mini_batches through the training')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
